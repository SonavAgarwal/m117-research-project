{
    "sha": "6b710a42e00acca405e085724c89cda016cf7442",
    "node_id": "MDY6Q29tbWl0MjM0MTg1MTc6NmI3MTBhNDJlMDBhY2NhNDA1ZTA4NTcyNGM4OWNkYTAxNmNmNzQ0Mg==",
    "commit": {
        "author": {
            "name": "Vinod Kumar Vavilapalli",
            "email": "vinodkv@apache.org",
            "date": "2015-05-14T23:07:56Z"
        },
        "committer": {
            "name": "Vinod Kumar Vavilapalli",
            "email": "vinodkv@apache.org",
            "date": "2015-05-14T23:07:56Z"
        },
        "message": "Fixing MR intermediate spills. Contributed by Arun Suresh.",
        "tree": {
            "sha": "894b0daf81f596c8d4639229d4a185685560b314",
            "url": "https://api.github.com/repos/apache/hadoop/git/trees/894b0daf81f596c8d4639229d4a185685560b314"
        },
        "url": "https://api.github.com/repos/apache/hadoop/git/commits/6b710a42e00acca405e085724c89cda016cf7442",
        "comment_count": 0,
        "verification": {
            "verified": false,
            "reason": "unsigned",
            "signature": null,
            "payload": null,
            "verified_at": null
        }
    },
    "url": "https://api.github.com/repos/apache/hadoop/commits/6b710a42e00acca405e085724c89cda016cf7442",
    "html_url": "https://github.com/apache/hadoop/commit/6b710a42e00acca405e085724c89cda016cf7442",
    "comments_url": "https://api.github.com/repos/apache/hadoop/commits/6b710a42e00acca405e085724c89cda016cf7442/comments",
    "author": {
        "login": "vinoduec",
        "id": 384796,
        "node_id": "MDQ6VXNlcjM4NDc5Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/384796?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/vinoduec",
        "html_url": "https://github.com/vinoduec",
        "followers_url": "https://api.github.com/users/vinoduec/followers",
        "following_url": "https://api.github.com/users/vinoduec/following{/other_user}",
        "gists_url": "https://api.github.com/users/vinoduec/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/vinoduec/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/vinoduec/subscriptions",
        "organizations_url": "https://api.github.com/users/vinoduec/orgs",
        "repos_url": "https://api.github.com/users/vinoduec/repos",
        "events_url": "https://api.github.com/users/vinoduec/events{/privacy}",
        "received_events_url": "https://api.github.com/users/vinoduec/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
    },
    "committer": {
        "login": "vinoduec",
        "id": 384796,
        "node_id": "MDQ6VXNlcjM4NDc5Ng==",
        "avatar_url": "https://avatars.githubusercontent.com/u/384796?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/vinoduec",
        "html_url": "https://github.com/vinoduec",
        "followers_url": "https://api.github.com/users/vinoduec/followers",
        "following_url": "https://api.github.com/users/vinoduec/following{/other_user}",
        "gists_url": "https://api.github.com/users/vinoduec/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/vinoduec/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/vinoduec/subscriptions",
        "organizations_url": "https://api.github.com/users/vinoduec/orgs",
        "repos_url": "https://api.github.com/users/vinoduec/repos",
        "events_url": "https://api.github.com/users/vinoduec/events{/privacy}",
        "received_events_url": "https://api.github.com/users/vinoduec/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
    },
    "parents": [
        {
            "sha": "53fe4eff09fdaeed75a8cad3a26156bf963a8d37",
            "url": "https://api.github.com/repos/apache/hadoop/commits/53fe4eff09fdaeed75a8cad3a26156bf963a8d37",
            "html_url": "https://github.com/apache/hadoop/commit/53fe4eff09fdaeed75a8cad3a26156bf963a8d37"
        }
    ],
    "stats": {
        "total": 197,
        "additions": 156,
        "deletions": 41
    },
    "files": [
        {
            "sha": "9d8b4a5abd2d19e3e79e708ab57dffd07f59bbd2",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FLocalContainerLauncher.java",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FLocalContainerLauncher.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FLocalContainerLauncher.java?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -83,6 +83,7 @@ public class LocalContainerLauncher extends AbstractService implements\n   private final ClassLoader jobClassLoader;\n   private ExecutorService taskRunner;\n   private Thread eventHandler;\n+  private byte[] encryptedSpillKey = new byte[] {0};\n   private BlockingQueue<ContainerLauncherEvent> eventQueue =\n       new LinkedBlockingQueue<ContainerLauncherEvent>();\n \n@@ -176,6 +177,11 @@ public void handle(ContainerLauncherEvent event) {\n     }\n   }\n \n+  public void setEncryptedSpillKey(byte[] encryptedSpillKey) {\n+    if (encryptedSpillKey != null) {\n+      this.encryptedSpillKey = encryptedSpillKey;\n+    }\n+  }\n \n   /*\n    * Uber-AM lifecycle/ordering (\"normal\" case):\n@@ -382,6 +388,10 @@ private void runSubtask(org.apache.hadoop.mapred.Task task,\n         // map to handle)\n         conf.setBoolean(\"mapreduce.task.uberized\", true);\n \n+        // Check and handle Encrypted spill key\n+        task.setEncryptedSpillKey(encryptedSpillKey);\n+        YarnChild.setEncryptedSpillKeyIfRequired(task);\n+\n         // META-FIXME: do we want the extra sanity-checking (doneWithMaps,\n         // etc.), or just assume/hope the state machine(s) and uber-AM work\n         // as expected?"
        },
        {
            "sha": "49a00c50a00984dc762f2638052a5526fe99ee99",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
            "status": "modified",
            "additions": 14,
            "deletions": 3,
            "changes": 17,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTaskAttemptListenerImpl.java",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTaskAttemptListenerImpl.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTaskAttemptListenerImpl.java?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -84,20 +84,30 @@ public class TaskAttemptListenerImpl extends CompositeService\n     jvmIDToActiveAttemptMap\n       = new ConcurrentHashMap<WrappedJvmID, org.apache.hadoop.mapred.Task>();\n   private Set<WrappedJvmID> launchedJVMs = Collections\n-      .newSetFromMap(new ConcurrentHashMap<WrappedJvmID, Boolean>()); \n-  \n+      .newSetFromMap(new ConcurrentHashMap<WrappedJvmID, Boolean>());\n+\n   private JobTokenSecretManager jobTokenSecretManager = null;\n   private AMPreemptionPolicy preemptionPolicy;\n-  \n+  private byte[] encryptedSpillKey;\n+\n   public TaskAttemptListenerImpl(AppContext context,\n       JobTokenSecretManager jobTokenSecretManager,\n       RMHeartbeatHandler rmHeartbeatHandler,\n       AMPreemptionPolicy preemptionPolicy) {\n+    this(context, jobTokenSecretManager, rmHeartbeatHandler,\n+            preemptionPolicy, null);\n+  }\n+\n+  public TaskAttemptListenerImpl(AppContext context,\n+      JobTokenSecretManager jobTokenSecretManager,\n+      RMHeartbeatHandler rmHeartbeatHandler,\n+      AMPreemptionPolicy preemptionPolicy, byte[] secretShuffleKey) {\n     super(TaskAttemptListenerImpl.class.getName());\n     this.context = context;\n     this.jobTokenSecretManager = jobTokenSecretManager;\n     this.rmHeartbeatHandler = rmHeartbeatHandler;\n     this.preemptionPolicy = preemptionPolicy;\n+    this.encryptedSpillKey = secretShuffleKey;\n   }\n \n   @Override\n@@ -484,6 +494,7 @@ public JvmTask getTask(JvmContext context) throws IOException {\n             jvmIDToActiveAttemptMap.remove(wJvmID);\n         launchedJVMs.remove(wJvmID);\n         LOG.info(\"JVM with ID: \" + jvmId + \" given task: \" + task.getTaskID());\n+        task.setEncryptedSpillKey(encryptedSpillKey);\n         jvmTask = new JvmTask(task, false);\n       }\n     }"
        },
        {
            "sha": "ea9733c3ef65de8fb2ef0e5192410c7769a15e0c",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/YarnChild.java",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FYarnChild.java",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FYarnChild.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FYarnChild.java?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -165,6 +165,7 @@ public TaskUmbilicalProtocol run() throws Exception {\n         @Override\n         public Object run() throws Exception {\n           // use job-specified working directory\n+          setEncryptedSpillKeyIfRequired(taskFinal);\n           FileSystem.get(job).setWorkingDirectory(job.getWorkingDirectory());\n           taskFinal.run(job, umbilical); // run the task\n           return null;\n@@ -223,6 +224,23 @@ public Object run() throws Exception {\n     }\n   }\n \n+  /**\n+   * Utility method to check if the Encrypted Spill Key needs to be set into the\n+   * user credentials of the user running the Map / Reduce Task\n+   * @param task The Map / Reduce task to set the Encrypted Spill information in\n+   * @throws Exception\n+   */\n+  public static void setEncryptedSpillKeyIfRequired(Task task) throws\n+          Exception {\n+    if ((task != null) && (task.getEncryptedSpillKey() != null) && (task\n+            .getEncryptedSpillKey().length > 1)) {\n+      Credentials creds =\n+              UserGroupInformation.getCurrentUser().getCredentials();\n+      TokenCache.setEncryptedSpillKey(task.getEncryptedSpillKey(), creds);\n+      UserGroupInformation.getCurrentUser().addCredentials(creds);\n+    }\n+  }\n+\n   /**\n    * Configure mapred-local dirs. This config is used by the task for finding\n    * out an output directory."
        },
        {
            "sha": "752b30ccf3cc7b754d651d647d207efb57a34f39",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java",
            "status": "modified",
            "additions": 23,
            "deletions": 1,
            "changes": 24,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fv2%2Fapp%2FMRAppMaster.java",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fv2%2Fapp%2FMRAppMaster.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-app%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fv2%2Fapp%2FMRAppMaster.java?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -21,6 +21,7 @@\n import java.io.IOException;\n import java.lang.reflect.Constructor;\n import java.lang.reflect.InvocationTargetException;\n+import java.security.NoSuchAlgorithmException;\n import java.security.PrivilegedExceptionAction;\n import java.util.ArrayList;\n import java.util.HashMap;\n@@ -47,6 +48,7 @@\n import org.apache.hadoop.mapred.TaskAttemptListenerImpl;\n import org.apache.hadoop.mapred.TaskLog;\n import org.apache.hadoop.mapred.TaskUmbilicalProtocol;\n+import org.apache.hadoop.mapreduce.CryptoUtils;\n import org.apache.hadoop.mapreduce.JobContext;\n import org.apache.hadoop.mapreduce.MRJobConfig;\n import org.apache.hadoop.mapreduce.OutputCommitter;\n@@ -148,6 +150,8 @@\n \n import com.google.common.annotations.VisibleForTesting;\n \n+import javax.crypto.KeyGenerator;\n+\n /**\n  * The Map-Reduce Application Master.\n  * The state machine is encapsulated in the implementation of Job interface.\n@@ -175,6 +179,7 @@ public class MRAppMaster extends CompositeService {\n    * Priority of the MRAppMaster shutdown hook.\n    */\n   public static final int SHUTDOWN_HOOK_PRIORITY = 30;\n+  public static final String INTERMEDIATE_DATA_ENCRYPTION_ALGO = \"HmacSHA1\";\n \n   private Clock clock;\n   private final long startTime;\n@@ -206,6 +211,7 @@ public class MRAppMaster extends CompositeService {\n   private JobHistoryEventHandler jobHistoryEventHandler;\n   private SpeculatorEventDispatcher speculatorEventDispatcher;\n   private AMPreemptionPolicy preemptionPolicy;\n+  private byte[] encryptedSpillKey;\n \n   // After a task attempt completes from TaskUmbilicalProtocol's point of view,\n   // it will be transitioned to finishing state.\n@@ -704,8 +710,22 @@ protected void initJobCredentialsAndUGI(Configuration conf) {\n     try {\n       this.currentUser = UserGroupInformation.getCurrentUser();\n       this.jobCredentials = ((JobConf)conf).getCredentials();\n+      if (CryptoUtils.isEncryptedSpillEnabled(conf)) {\n+        int keyLen = conf.getInt(\n+                MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS,\n+                MRJobConfig\n+                        .DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS);\n+        KeyGenerator keyGen =\n+                KeyGenerator.getInstance(INTERMEDIATE_DATA_ENCRYPTION_ALGO);\n+        keyGen.init(keyLen);\n+        encryptedSpillKey = keyGen.generateKey().getEncoded();\n+      } else {\n+        encryptedSpillKey = new byte[] {0};\n+      }\n     } catch (IOException e) {\n       throw new YarnRuntimeException(e);\n+    } catch (NoSuchAlgorithmException e) {\n+      throw new YarnRuntimeException(e);\n     }\n   }\n \n@@ -762,7 +782,7 @@ protected TaskAttemptListener createTaskAttemptListener(AppContext context,\n       AMPreemptionPolicy preemptionPolicy) {\n     TaskAttemptListener lis =\n         new TaskAttemptListenerImpl(context, jobTokenSecretManager,\n-            getRMHeartbeatHandler(), preemptionPolicy);\n+            getRMHeartbeatHandler(), preemptionPolicy, encryptedSpillKey);\n     return lis;\n   }\n \n@@ -929,6 +949,8 @@ protected void serviceStart() throws Exception {\n       if (job.isUber()) {\n         this.containerLauncher = new LocalContainerLauncher(context,\n             (TaskUmbilicalProtocol) taskAttemptListener, jobClassLoader);\n+        ((LocalContainerLauncher) this.containerLauncher)\n+                .setEncryptedSpillKey(encryptedSpillKey);\n       } else {\n         this.containerLauncher = new ContainerLauncherImpl(context);\n       }"
        },
        {
            "sha": "c07d5178e5b98989af3386ecdb7ac142c35e12d8",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTask.java",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTask.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTask.java?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -149,6 +149,8 @@ static synchronized String getOutputName(int partition) {\n   private String user;                            // user running the job\n   private TaskAttemptID taskId;                   // unique, includes job id\n   private int partition;                          // id within job\n+  private byte[] encryptedSpillKey = new byte[] {0};  // Key Used to encrypt\n+  // intermediate spills\n   TaskStatus taskStatus;                          // current status of the task\n   protected JobStatus.State jobRunStateForCleanup;\n   protected boolean jobCleanup = false;\n@@ -262,6 +264,24 @@ public void setJobTokenSecret(SecretKey tokenSecret) {\n     this.tokenSecret = tokenSecret;\n   }\n \n+  /**\n+   * Get Encrypted spill key\n+   * @return encrypted spill key\n+   */\n+  public byte[] getEncryptedSpillKey() {\n+    return encryptedSpillKey;\n+  }\n+\n+  /**\n+   * Set Encrypted spill key\n+   * @param encryptedSpillKey key\n+   */\n+  public void setEncryptedSpillKey(byte[] encryptedSpillKey) {\n+    if (encryptedSpillKey != null) {\n+      this.encryptedSpillKey = encryptedSpillKey;\n+    }\n+  }\n+\n   /**\n    * Get the job token secret\n    * @return the token secret\n@@ -492,6 +512,8 @@ public void write(DataOutput out) throws IOException {\n     out.writeBoolean(writeSkipRecs);\n     out.writeBoolean(taskCleanup);\n     Text.writeString(out, user);\n+    out.writeInt(encryptedSpillKey.length);\n+    out.write(encryptedSpillKey);\n     extraData.write(out);\n   }\n   \n@@ -517,6 +539,9 @@ public void readFields(DataInput in) throws IOException {\n       setPhase(TaskStatus.Phase.CLEANUP);\n     }\n     user = StringInterner.weakIntern(Text.readString(in));\n+    int len = in.readInt();\n+    encryptedSpillKey = new byte[len];\n+    in.readFully(encryptedSpillKey);\n     extraData.readFields(in);\n   }\n "
        },
        {
            "sha": "c4130b1b07f753b7297c4bde68536d343b225382",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/CryptoUtils.java",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2FCryptoUtils.java",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2FCryptoUtils.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2FCryptoUtils.java?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -34,7 +34,6 @@\n import org.apache.hadoop.fs.crypto.CryptoFSDataInputStream;\n import org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream;\n import org.apache.hadoop.io.IOUtils;\n-import org.apache.hadoop.mapreduce.MRJobConfig;\n import org.apache.hadoop.mapreduce.security.TokenCache;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.util.LimitInputStream;\n@@ -50,7 +49,7 @@ public class CryptoUtils {\n \n   private static final Log LOG = LogFactory.getLog(CryptoUtils.class);\n \n-  public static boolean isShuffleEncrypted(Configuration conf) {\n+  public static boolean isEncryptedSpillEnabled(Configuration conf) {\n     return conf.getBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA,\n         MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA);\n   }\n@@ -64,7 +63,7 @@ public static boolean isShuffleEncrypted(Configuration conf) {\n    */\n   public static byte[] createIV(Configuration conf) throws IOException {\n     CryptoCodec cryptoCodec = CryptoCodec.getInstance(conf);\n-    if (isShuffleEncrypted(conf)) {\n+    if (isEncryptedSpillEnabled(conf)) {\n       byte[] iv = new byte[cryptoCodec.getCipherSuite().getAlgorithmBlockSize()];\n       cryptoCodec.generateSecureRandom(iv);\n       return iv;\n@@ -75,13 +74,13 @@ public static byte[] createIV(Configuration conf) throws IOException {\n \n   public static int cryptoPadding(Configuration conf) {\n     // Sizeof(IV) + long(start-offset)\n-    return isShuffleEncrypted(conf) ? CryptoCodec.getInstance(conf)\n+    return isEncryptedSpillEnabled(conf) ? CryptoCodec.getInstance(conf)\n         .getCipherSuite().getAlgorithmBlockSize() + 8 : 0;\n   }\n \n   private static byte[] getEncryptionKey() throws IOException {\n-    return TokenCache.getShuffleSecretKey(UserGroupInformation.getCurrentUser()\n-        .getCredentials());\n+    return TokenCache.getEncryptedSpillKey(UserGroupInformation.getCurrentUser()\n+            .getCredentials());\n   }\n \n   private static int getBufferSize(Configuration conf) {\n@@ -102,7 +101,7 @@ private static int getBufferSize(Configuration conf) {\n    */\n   public static FSDataOutputStream wrapIfNecessary(Configuration conf,\n       FSDataOutputStream out) throws IOException {\n-    if (isShuffleEncrypted(conf)) {\n+    if (isEncryptedSpillEnabled(conf)) {\n       out.write(ByteBuffer.allocate(8).putLong(out.getPos()).array());\n       byte[] iv = createIV(conf);\n       out.write(iv);\n@@ -137,7 +136,7 @@ public static FSDataOutputStream wrapIfNecessary(Configuration conf,\n    */\n   public static InputStream wrapIfNecessary(Configuration conf, InputStream in,\n       long length) throws IOException {\n-    if (isShuffleEncrypted(conf)) {\n+    if (isEncryptedSpillEnabled(conf)) {\n       int bufferSize = getBufferSize(conf);\n       if (length > -1) {\n         in = new LimitInputStream(in, length);\n@@ -174,7 +173,7 @@ public static InputStream wrapIfNecessary(Configuration conf, InputStream in,\n    */\n   public static FSDataInputStream wrapIfNecessary(Configuration conf,\n       FSDataInputStream in) throws IOException {\n-    if (isShuffleEncrypted(conf)) {\n+    if (isEncryptedSpillEnabled(conf)) {\n       CryptoCodec cryptoCodec = CryptoCodec.getInstance(conf);\n       int bufferSize = getBufferSize(conf);\n       // Not going to be used... but still has to be read..."
        },
        {
            "sha": "a458e2c3685328ae2715845530e259e06f61e89e",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2FJobSubmitter.java",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2FJobSubmitter.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2FJobSubmitter.java?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -18,12 +18,10 @@\n package org.apache.hadoop.mapreduce;\n \n import java.io.File;\n-import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.net.InetAddress;\n import java.net.URI;\n import java.net.URISyntaxException;\n-import java.net.UnknownHostException;\n import java.security.NoSuchAlgorithmException;\n import java.util.ArrayList;\n import java.util.Arrays;\n@@ -42,7 +40,6 @@\n import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileContext;\n import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.io.Text;\n@@ -52,7 +49,6 @@\n import static org.apache.hadoop.mapred.QueueManager.toFullPropertyName;\n \n import org.apache.hadoop.mapreduce.counters.Limits;\n-import org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager;\n import org.apache.hadoop.mapreduce.filecache.DistributedCache;\n import org.apache.hadoop.mapreduce.protocol.ClientProtocol;\n import org.apache.hadoop.mapreduce.security.TokenCache;\n@@ -176,20 +172,20 @@ JobStatus submitJobInternal(Job job, Cluster cluster)\n       if (TokenCache.getShuffleSecretKey(job.getCredentials()) == null) {\n         KeyGenerator keyGen;\n         try {\n-         \n-          int keyLen = CryptoUtils.isShuffleEncrypted(conf) \n-              ? conf.getInt(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS, \n-                  MRJobConfig.DEFAULT_MR_ENCRYPTED_INTERMEDIATE_DATA_KEY_SIZE_BITS)\n-              : SHUFFLE_KEY_LENGTH;\n           keyGen = KeyGenerator.getInstance(SHUFFLE_KEYGEN_ALGORITHM);\n-          keyGen.init(keyLen);\n+          keyGen.init(SHUFFLE_KEY_LENGTH);\n         } catch (NoSuchAlgorithmException e) {\n           throw new IOException(\"Error generating shuffle secret key\", e);\n         }\n         SecretKey shuffleKey = keyGen.generateKey();\n         TokenCache.setShuffleSecretKey(shuffleKey.getEncoded(),\n             job.getCredentials());\n       }\n+      if (CryptoUtils.isEncryptedSpillEnabled(conf)) {\n+        conf.setInt(MRJobConfig.MR_AM_MAX_ATTEMPTS, 1);\n+        LOG.warn(\"Max job attempts set to 1 since encrypted intermediate\" +\n+                \"data spill is enabled\");\n+      }\n \n       copyAndConfigureFiles(job, submitJobDir);\n "
        },
        {
            "sha": "78f6c1675e5bca2e2cf77b8c04d74c34cad45b3b",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/security/TokenCache.java",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fsecurity%2FTokenCache.java",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fsecurity%2FTokenCache.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Fsecurity%2FTokenCache.java?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -176,6 +176,7 @@ private static void mergeBinaryTokens(Credentials creds, Configuration conf) {\n   public static final String JOB_TOKENS_FILENAME = \"mapreduce.job.jobTokenFile\";\n   private static final Text JOB_TOKEN = new Text(\"JobToken\");\n   private static final Text SHUFFLE_TOKEN = new Text(\"MapReduceShuffleToken\");\n+  private static final Text ENC_SPILL_KEY = new Text(\"MapReduceEncryptedSpillKey\");\n   \n   /**\n    * load job token from a file\n@@ -244,6 +245,15 @@ public static byte[] getShuffleSecretKey(Credentials credentials) {\n     return getSecretKey(credentials, SHUFFLE_TOKEN);\n   }\n \n+  @InterfaceAudience.Private\n+  public static void setEncryptedSpillKey(byte[] key, Credentials credentials) {\n+    credentials.addSecretKey(ENC_SPILL_KEY, key);\n+  }\n+\n+  @InterfaceAudience.Private\n+  public static byte[] getEncryptedSpillKey(Credentials credentials) {\n+    return getSecretKey(credentials, ENC_SPILL_KEY);\n+  }\n   /**\n    * @deprecated Use {@link Credentials#getToken(org.apache.hadoop.io.Text)}\n    * instead, this method is included for compatibility against Hadoop-1"
        },
        {
            "sha": "de2382cf92f9f239a0e51e63690eb16a063586fe",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/LocalFetcher.java",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Ftask%2Freduce%2FLocalFetcher.java",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Ftask%2Freduce%2FLocalFetcher.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fmain%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Ftask%2Freduce%2FLocalFetcher.java?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -127,6 +127,9 @@ private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n     long compressedLength = ir.partLength;\n     long decompressedLength = ir.rawLength;\n \n+    compressedLength -= CryptoUtils.cryptoPadding(job);\n+    decompressedLength -= CryptoUtils.cryptoPadding(job);\n+\n     // Get the location for the map output - either in-memory or on-disk\n     MapOutput<K, V> mapOutput = merger.reserve(mapTaskId, decompressedLength,\n         id);\n@@ -150,8 +153,7 @@ private boolean copyMapOutput(TaskAttemptID mapTaskId) throws IOException {\n     inStream = CryptoUtils.wrapIfNecessary(job, inStream);\n \n     try {\n-      inStream.seek(ir.startOffset);\n-\n+      inStream.seek(ir.startOffset + CryptoUtils.cryptoPadding(job));\n       mapOutput.shuffle(LOCALHOST, inStream, compressedLength, decompressedLength, metrics, reporter);\n     } finally {\n       try {"
        },
        {
            "sha": "c23be7a9929075b7d0644467cc1c170ff296efb8",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/site/markdown/EncryptedShuffle.md",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fsite%2Fmarkdown%2FEncryptedShuffle.md",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fsite%2Fmarkdown%2FEncryptedShuffle.md",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Fsite%2Fmarkdown%2FEncryptedShuffle.md?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -253,3 +253,11 @@ You can do this on a per-job basis, or by means of a cluster-wide setting in the\n To set this property in NodeManager, set it in the `yarn-env.sh` file:\n \n       YARN_NODEMANAGER_OPTS=\"-Djavax.net.debug=all\"\n+\n+Encrypted Intermediate Data Spill files\n+---------------------------------------\n+\n+This capability allows encryption of the intermediate files generated during the merge and shuffle phases.\n+It can be enabled by setting the `mapreduce.job.encrypted-intermediate-data` job property to `true`.\n+\n+**NOTE:** Currently, enabling encrypted intermediate data spills would restrict the number of attempts of the job to 1.\n\\ No newline at end of file"
        },
        {
            "sha": "a6b19646cf1bf7252663741ff104105c0ed029f3",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/task/reduce/TestMerger.java",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Ftask%2Freduce%2FTestMerger.java",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Ftask%2Freduce%2FTestMerger.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-core%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapreduce%2Ftask%2Freduce%2FTestMerger.java?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -87,7 +87,7 @@ public void testEncryptedMerger() throws Throwable {\n     jobConf.setBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA, true);\n     conf.setBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA, true);\n     Credentials credentials = UserGroupInformation.getCurrentUser().getCredentials();\n-    TokenCache.setShuffleSecretKey(new byte[16], credentials);\n+    TokenCache.setEncryptedSpillKey(new byte[16], credentials);\n     UserGroupInformation.getCurrentUser().addCredentials(credentials);\n     testInMemoryAndOnDiskMerger();\n   }"
        },
        {
            "sha": "28b22956732b6efcdde461d558f24e553cb51898",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMRIntermediateDataEncryption.java",
            "status": "modified",
            "additions": 21,
            "deletions": 9,
            "changes": 30,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTestMRIntermediateDataEncryption.java",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTestMRIntermediateDataEncryption.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTestMRIntermediateDataEncryption.java?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -52,24 +52,31 @@ public class TestMRIntermediateDataEncryption {\n \n   @Test\n   public void testSingleReducer() throws Exception {\n-    doEncryptionTest(3, 1, 2);\n+    doEncryptionTest(3, 1, 2, false);\n+  }\n+\n+  @Test\n+  public void testUberMode() throws Exception {\n+    doEncryptionTest(3, 1, 2, true);\n   }\n \n   @Test\n   public void testMultipleMapsPerNode() throws Exception {\n-    doEncryptionTest(8, 1, 2);\n+    doEncryptionTest(8, 1, 2, false);\n   }\n \n   @Test\n   public void testMultipleReducers() throws Exception {\n-    doEncryptionTest(2, 4, 2);\n+    doEncryptionTest(2, 4, 2, false);\n   }\n \n-  public void doEncryptionTest(int numMappers, int numReducers, int numNodes) throws Exception {\n-    doEncryptionTest(numMappers, numReducers, numNodes, 1000);\n+  public void doEncryptionTest(int numMappers, int numReducers, int numNodes,\n+                               boolean isUber) throws Exception {\n+    doEncryptionTest(numMappers, numReducers, numNodes, 1000, isUber);\n   }\n \n-  public void doEncryptionTest(int numMappers, int numReducers, int numNodes, int numLines) throws Exception {\n+  public void doEncryptionTest(int numMappers, int numReducers, int numNodes,\n+                               int numLines, boolean isUber) throws Exception {\n     MiniDFSCluster dfsCluster = null;\n     MiniMRClientCluster mrCluster = null;\n     FileSystem fileSystem = null;\n@@ -85,7 +92,8 @@ public void doEncryptionTest(int numMappers, int numReducers, int numNodes, int\n       // Generate input.\n       createInput(fileSystem, numMappers, numLines);\n       // Run the test.\n-      runMergeTest(new JobConf(mrCluster.getConfig()), fileSystem, numMappers, numReducers, numLines);\n+      runMergeTest(new JobConf(mrCluster.getConfig()), fileSystem,\n+              numMappers, numReducers, numLines, isUber);\n     } finally {\n       if (dfsCluster != null) {\n         dfsCluster.shutdown();\n@@ -111,8 +119,9 @@ private void createInput(FileSystem fs, int numMappers, int numLines) throws Exc\n     }\n   }\n \n-  private void runMergeTest(JobConf job, FileSystem fileSystem, int numMappers, int numReducers, int numLines)\n-    throws Exception {\n+  private void runMergeTest(JobConf job, FileSystem fileSystem, int\n+          numMappers, int numReducers, int numLines, boolean isUber)\n+          throws Exception {\n     fileSystem.delete(OUTPUT, true);\n     job.setJobName(\"Test\");\n     JobClient client = new JobClient(job);\n@@ -133,6 +142,9 @@ private void runMergeTest(JobConf job, FileSystem fileSystem, int numMappers, in\n     job.setInt(\"mapreduce.map.maxattempts\", 1);\n     job.setInt(\"mapreduce.reduce.maxattempts\", 1);\n     job.setInt(\"mapred.test.num_lines\", numLines);\n+    if (isUber) {\n+      job.setBoolean(\"mapreduce.job.ubertask.enable\", true);\n+    }\n     job.setBoolean(MRJobConfig.MR_ENCRYPTED_INTERMEDIATE_DATA, true);\n     try {\n       submittedJob = client.submitJob(job);"
        },
        {
            "sha": "db6348ba44070724ef54c62714aec2b378de0122",
            "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestMapProgress.java",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/apache/hadoop/blob/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTestMapProgress.java",
            "raw_url": "https://github.com/apache/hadoop/raw/6b710a42e00acca405e085724c89cda016cf7442/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTestMapProgress.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project%2Fhadoop-mapreduce-client%2Fhadoop-mapreduce-client-jobclient%2Fsrc%2Ftest%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmapred%2FTestMapProgress.java?ref=6b710a42e00acca405e085724c89cda016cf7442",
            "patch": "@@ -119,12 +119,14 @@ public AMFeedback statusUpdate(TaskAttemptID taskId, TaskStatus taskStatus)\n     throws IOException, InterruptedException {\n       StringBuffer buf = new StringBuffer(\"Task \");\n       buf.append(taskId);\n-      buf.append(\" making progress to \");\n-      buf.append(taskStatus.getProgress());\n-      String state = taskStatus.getStateString();\n-      if (state != null) {\n-        buf.append(\" and state of \");\n-        buf.append(state);\n+      if (taskStatus != null) {\n+        buf.append(\" making progress to \");\n+        buf.append(taskStatus.getProgress());\n+        String state = taskStatus.getStateString();\n+        if (state != null) {\n+          buf.append(\" and state of \");\n+          buf.append(state);\n+        }\n       }\n       LOG.info(buf.toString());\n       // ignore phase"
        }
    ]
}