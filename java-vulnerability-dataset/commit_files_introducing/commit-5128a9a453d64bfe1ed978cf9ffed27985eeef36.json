{
    "sha": "5128a9a453d64bfe1ed978cf9ffed27985eeef36",
    "node_id": "MDY6Q29tbWl0MjM0MTg1MTc6NTEyOGE5YTQ1M2Q2NGJmZTFlZDk3OGNmOWZmZWQyNzk4NWVlZWYzNg==",
    "commit": {
        "author": {
            "name": "Owen O'Malley",
            "email": "omalley@apache.org",
            "date": "2009-05-19T04:20:40Z"
        },
        "committer": {
            "name": "Owen O'Malley",
            "email": "omalley@apache.org",
            "date": "2009-05-19T04:20:40Z"
        },
        "message": "HADOOP-4687 Moving src directories on branch\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/core/branches/HADOOP-4687/core@776174 13f79535-47bb-0310-9956-ffa450edef68",
        "tree": {
            "sha": "af2790564b533f7d92b01cdabacdcd5bb59faf93",
            "url": "https://api.github.com/repos/apache/hadoop/git/trees/af2790564b533f7d92b01cdabacdcd5bb59faf93"
        },
        "url": "https://api.github.com/repos/apache/hadoop/git/commits/5128a9a453d64bfe1ed978cf9ffed27985eeef36",
        "comment_count": 0,
        "verification": {
            "verified": false,
            "reason": "unsigned",
            "signature": null,
            "payload": null,
            "verified_at": null
        }
    },
    "url": "https://api.github.com/repos/apache/hadoop/commits/5128a9a453d64bfe1ed978cf9ffed27985eeef36",
    "html_url": "https://github.com/apache/hadoop/commit/5128a9a453d64bfe1ed978cf9ffed27985eeef36",
    "comments_url": "https://api.github.com/repos/apache/hadoop/commits/5128a9a453d64bfe1ed978cf9ffed27985eeef36/comments",
    "author": {
        "login": "omalley",
        "id": 206536,
        "node_id": "MDQ6VXNlcjIwNjUzNg==",
        "avatar_url": "https://avatars.githubusercontent.com/u/206536?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/omalley",
        "html_url": "https://github.com/omalley",
        "followers_url": "https://api.github.com/users/omalley/followers",
        "following_url": "https://api.github.com/users/omalley/following{/other_user}",
        "gists_url": "https://api.github.com/users/omalley/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/omalley/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/omalley/subscriptions",
        "organizations_url": "https://api.github.com/users/omalley/orgs",
        "repos_url": "https://api.github.com/users/omalley/repos",
        "events_url": "https://api.github.com/users/omalley/events{/privacy}",
        "received_events_url": "https://api.github.com/users/omalley/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
    },
    "committer": {
        "login": "omalley",
        "id": 206536,
        "node_id": "MDQ6VXNlcjIwNjUzNg==",
        "avatar_url": "https://avatars.githubusercontent.com/u/206536?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/omalley",
        "html_url": "https://github.com/omalley",
        "followers_url": "https://api.github.com/users/omalley/followers",
        "following_url": "https://api.github.com/users/omalley/following{/other_user}",
        "gists_url": "https://api.github.com/users/omalley/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/omalley/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/omalley/subscriptions",
        "organizations_url": "https://api.github.com/users/omalley/orgs",
        "repos_url": "https://api.github.com/users/omalley/repos",
        "events_url": "https://api.github.com/users/omalley/events{/privacy}",
        "received_events_url": "https://api.github.com/users/omalley/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
    },
    "parents": [],
    "stats": {
        "total": 67543,
        "additions": 67543,
        "deletions": 0
    },
    "files": [
        {
            "sha": "b56dda4235b99467bee87a6b6091a0a947530a78",
            "filename": "src/java/core-default.xml",
            "status": "added",
            "additions": 444,
            "deletions": 0,
            "changes": 444,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Fcore-default.xml",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Fcore-default.xml",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Fcore-default.xml?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,444 @@\n+<?xml version=\"1.0\"?>\n+<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n+\n+<!-- Do not modify this file directly.  Instead, copy entries that you -->\n+<!-- wish to modify from this file into core-site.xml and change them -->\n+<!-- there.  If core-site.xml does not already exist, create it.      -->\n+\n+<configuration>\n+\n+<!--- global properties -->\n+\n+<property>\n+  <name>hadoop.tmp.dir</name>\n+  <value>/tmp/hadoop-${user.name}</value>\n+  <description>A base for other temporary directories.</description>\n+</property>\n+\n+<property>\n+  <name>hadoop.native.lib</name>\n+  <value>true</value>\n+  <description>Should native hadoop libraries, if present, be used.</description>\n+</property>\n+\n+<property>\n+  <name>hadoop.http.filter.initializers</name>\n+  <value></value>\n+  <description>A comma separated list of class names. Each class in the list \n+  must extend org.apache.hadoop.http.FilterInitializer. The corresponding \n+  Filter will be initialized. Then, the Filter will be applied to all user \n+  facing jsp and servlet web pages.  The ordering of the list defines the \n+  ordering of the filters.</description>\n+</property>\n+\n+<property>\n+  <name>hadoop.security.authorization</name>\n+  <value>false</value>\n+  <description>Is service-level authorization enabled?</description>\n+</property>\n+\n+<!--- logging properties -->\n+\n+<property>\n+  <name>hadoop.logfile.size</name>\n+  <value>10000000</value>\n+  <description>The max size of each log file</description>\n+</property>\n+\n+<property>\n+  <name>hadoop.logfile.count</name>\n+  <value>10</value>\n+  <description>The max number of log files</description>\n+</property>\n+\n+<!-- i/o properties -->\n+<property>\n+  <name>io.file.buffer.size</name>\n+  <value>4096</value>\n+  <description>The size of buffer for use in sequence files.\n+  The size of this buffer should probably be a multiple of hardware\n+  page size (4096 on Intel x86), and it determines how much data is\n+  buffered during read and write operations.</description>\n+</property>\n+  \n+<property>\n+  <name>io.bytes.per.checksum</name>\n+  <value>512</value>\n+  <description>The number of bytes per checksum.  Must not be larger than\n+  io.file.buffer.size.</description>\n+</property>\n+\n+<property>\n+  <name>io.skip.checksum.errors</name>\n+  <value>false</value>\n+  <description>If true, when a checksum error is encountered while\n+  reading a sequence file, entries are skipped, instead of throwing an\n+  exception.</description>\n+</property>\n+\n+<property>\n+  <name>io.compression.codecs</name>\n+  <value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec</value>\n+  <description>A list of the compression codec classes that can be used \n+               for compression/decompression.</description>\n+</property>\n+\n+<property>\n+  <name>io.serializations</name>\n+  <value>org.apache.hadoop.io.serializer.WritableSerialization</value>\n+  <description>A list of serialization classes that can be used for\n+  obtaining serializers and deserializers.</description>\n+</property>\n+\n+<property>\n+  <name>io.seqfile.local.dir</name>\n+  <value>${hadoop.tmp.dir}/io/local</value>\n+  <description>The local directory where sequence file stores intermediate\n+  data files during merge.  May be a comma-separated list of\n+  directories on different devices in order to spread disk i/o.\n+  Directories that do not exist are ignored.\n+  </description>\n+</property>\n+\n+<!-- file system properties -->\n+\n+<property>\n+  <name>fs.default.name</name>\n+  <value>file:///</value>\n+  <description>The name of the default file system.  A URI whose\n+  scheme and authority determine the FileSystem implementation.  The\n+  uri's scheme determines the config property (fs.SCHEME.impl) naming\n+  the FileSystem implementation class.  The uri's authority is used to\n+  determine the host, port, etc. for a filesystem.</description>\n+</property>\n+\n+<property>\n+  <name>fs.trash.interval</name>\n+  <value>0</value>\n+  <description>Number of minutes between trash checkpoints.\n+  If zero, the trash feature is disabled.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>fs.file.impl</name>\n+  <value>org.apache.hadoop.fs.LocalFileSystem</value>\n+  <description>The FileSystem for file: uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.hdfs.impl</name>\n+  <value>org.apache.hadoop.hdfs.DistributedFileSystem</value>\n+  <description>The FileSystem for hdfs: uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3.impl</name>\n+  <value>org.apache.hadoop.fs.s3.S3FileSystem</value>\n+  <description>The FileSystem for s3: uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3n.impl</name>\n+  <value>org.apache.hadoop.fs.s3native.NativeS3FileSystem</value>\n+  <description>The FileSystem for s3n: (Native S3) uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.kfs.impl</name>\n+  <value>org.apache.hadoop.fs.kfs.KosmosFileSystem</value>\n+  <description>The FileSystem for kfs: uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.hftp.impl</name>\n+  <value>org.apache.hadoop.hdfs.HftpFileSystem</value>\n+</property>\n+\n+<property>\n+  <name>fs.hsftp.impl</name>\n+  <value>org.apache.hadoop.hdfs.HsftpFileSystem</value>\n+</property>\n+\n+<property>\n+  <name>fs.ftp.impl</name>\n+  <value>org.apache.hadoop.fs.ftp.FTPFileSystem</value>\n+  <description>The FileSystem for ftp: uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.ramfs.impl</name>\n+  <value>org.apache.hadoop.fs.InMemoryFileSystem</value>\n+  <description>The FileSystem for ramfs: uris.</description>\n+</property>\n+\n+<property>\n+  <name>fs.har.impl</name>\n+  <value>org.apache.hadoop.fs.HarFileSystem</value>\n+  <description>The filesystem for Hadoop archives. </description>\n+</property>\n+\n+<property>\n+  <name>fs.checkpoint.dir</name>\n+  <value>${hadoop.tmp.dir}/dfs/namesecondary</value>\n+  <description>Determines where on the local filesystem the DFS secondary\n+      name node should store the temporary images to merge.\n+      If this is a comma-delimited list of directories then the image is\n+      replicated in all of the directories for redundancy.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>fs.checkpoint.edits.dir</name>\n+  <value>${fs.checkpoint.dir}</value>\n+  <description>Determines where on the local filesystem the DFS secondary\n+      name node should store the temporary edits to merge.\n+      If this is a comma-delimited list of directoires then teh edits is\n+      replicated in all of the directoires for redundancy.\n+      Default value is same as fs.checkpoint.dir\n+  </description>\n+</property>\n+\n+<property>\n+  <name>fs.checkpoint.period</name>\n+  <value>3600</value>\n+  <description>The number of seconds between two periodic checkpoints.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>fs.checkpoint.size</name>\n+  <value>67108864</value>\n+  <description>The size of the current edit log (in bytes) that triggers\n+       a periodic checkpoint even if the fs.checkpoint.period hasn't expired.\n+  </description>\n+</property>\n+\n+\n+\n+<property>\n+  <name>fs.s3.block.size</name>\n+  <value>67108864</value>\n+  <description>Block size to use when writing files to S3.</description>\n+</property>\n+\n+<property>\n+  <name>fs.s3.buffer.dir</name>\n+  <value>${hadoop.tmp.dir}/s3</value>\n+  <description>Determines where on the local filesystem the S3 filesystem\n+  should store files before sending them to S3\n+  (or after retrieving them from S3).\n+  </description>\n+</property>\n+\n+<property>\n+  <name>fs.s3.maxRetries</name>\n+  <value>4</value>\n+  <description>The maximum number of retries for reading or writing files to S3, \n+  before we signal failure to the application.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>fs.s3.sleepTimeSeconds</name>\n+  <value>10</value>\n+  <description>The number of seconds to sleep between each S3 retry.\n+  </description>\n+</property>\n+\n+\n+<property>\n+  <name>local.cache.size</name>\n+  <value>10737418240</value>\n+  <description>The limit on the size of cache you want to keep, set by default\n+  to 10GB. This will act as a soft limit on the cache directory for out of band data.\n+  </description>\n+</property>\n+            \n+<property>\n+  <name>io.seqfile.compress.blocksize</name>\n+  <value>1000000</value>\n+  <description>The minimum block size for compression in block compressed \n+          SequenceFiles.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>io.seqfile.lazydecompress</name>\n+  <value>true</value>\n+  <description>Should values of block-compressed SequenceFiles be decompressed\n+          only when necessary.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>io.seqfile.sorter.recordlimit</name>\n+  <value>1000000</value>\n+  <description>The limit on number of records to be kept in memory in a spill \n+          in SequenceFiles.Sorter\n+  </description>\n+</property>\n+\n+ <property>\n+  <name>io.mapfile.bloom.size</name>\n+  <value>1048576</value>\n+  <description>The size of BloomFilter-s used in BloomMapFile. Each time this many\n+  keys is appended the next BloomFilter will be created (inside a DynamicBloomFilter).\n+  Larger values minimize the number of filters, which slightly increases the performance,\n+  but may waste too much space if the total number of keys is usually much smaller\n+  than this number.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>io.mapfile.bloom.error.rate</name>\n+  <value>0.005</value>\n+  <description>The rate of false positives in BloomFilter-s used in BloomMapFile.\n+  As this value decreases, the size of BloomFilter-s increases exponentially. This\n+  value is the probability of encountering false positives (default is 0.5%).\n+  </description>\n+</property>\n+\n+<property>\n+  <name>hadoop.util.hash.type</name>\n+  <value>murmur</value>\n+  <description>The default implementation of Hash. Currently this can take one of the\n+  two values: 'murmur' to select MurmurHash and 'jenkins' to select JenkinsHash.\n+  </description>\n+</property>\n+\n+\n+<!-- ipc properties -->\n+\n+<property>\n+  <name>ipc.client.idlethreshold</name>\n+  <value>4000</value>\n+  <description>Defines the threshold number of connections after which\n+               connections will be inspected for idleness.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>ipc.client.kill.max</name>\n+  <value>10</value>\n+  <description>Defines the maximum number of clients to disconnect in one go.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>ipc.client.connection.maxidletime</name>\n+  <value>10000</value>\n+  <description>The maximum time in msec after which a client will bring down the\n+               connection to the server.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>ipc.client.connect.max.retries</name>\n+  <value>10</value>\n+  <description>Indicates the number of retries a client will make to establish\n+               a server connection.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>ipc.server.listen.queue.size</name>\n+  <value>128</value>\n+  <description>Indicates the length of the listen queue for servers accepting\n+               client connections.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>ipc.server.tcpnodelay</name>\n+  <value>false</value>\n+  <description>Turn on/off Nagle's algorithm for the TCP socket connection on \n+  the server. Setting to true disables the algorithm and may decrease latency\n+  with a cost of more/smaller packets. \n+  </description>\n+</property>\n+\n+<property>\n+  <name>ipc.client.tcpnodelay</name>\n+  <value>false</value>\n+  <description>Turn on/off Nagle's algorithm for the TCP socket connection on \n+  the client. Setting to true disables the algorithm and may decrease latency\n+  with a cost of more/smaller packets. \n+  </description>\n+</property>\n+\n+\n+<!-- Web Interface Configuration -->\n+\n+<property>\n+  <name>webinterface.private.actions</name>\n+  <value>false</value>\n+  <description> If set to true, the web interfaces of JT and NN may contain \n+                actions, such as kill job, delete file, etc., that should \n+                not be exposed to public. Enable this option if the interfaces \n+                are only reachable by those who have the right authorization.\n+  </description>\n+</property>\n+\n+<!-- Proxy Configuration -->\n+\n+<property>\n+  <name>hadoop.rpc.socket.factory.class.default</name>\n+  <value>org.apache.hadoop.net.StandardSocketFactory</value>\n+  <description> Default SocketFactory to use. This parameter is expected to be\n+    formatted as \"package.FactoryClassName\".\n+  </description>\n+</property>\n+\n+<property>\n+  <name>hadoop.rpc.socket.factory.class.ClientProtocol</name>\n+  <value></value>\n+  <description> SocketFactory to use to connect to a DFS. If null or empty, use\n+    hadoop.rpc.socket.class.default. This socket factory is also used by\n+    DFSClient to create sockets to DataNodes.\n+  </description>\n+</property>\n+\n+\n+\n+<property>\n+  <name>hadoop.socks.server</name>\n+  <value></value>\n+  <description> Address (host:port) of the SOCKS server to be used by the\n+    SocksSocketFactory.\n+  </description>\n+</property>\n+\n+<!-- Rack Configuration -->\n+\n+<property>\n+  <name>topology.node.switch.mapping.impl</name>\n+  <value>org.apache.hadoop.net.ScriptBasedMapping</value>\n+  <description> The default implementation of the DNSToSwitchMapping. It\n+    invokes a script specified in topology.script.file.name to resolve\n+    node names. If the value for topology.script.file.name is not set, the\n+    default value of DEFAULT_RACK is returned for all node names.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>topology.script.file.name</name>\n+  <value></value>\n+  <description> The script name that should be invoked to resolve DNS names to\n+    NetworkTopology names. Example: the script would take host.foo.bar as an\n+    argument, and return /rack1 as the output.\n+  </description>\n+</property>\n+\n+<property>\n+  <name>topology.script.number.args</name>\n+  <value>100</value>\n+  <description> The max number of args that the script configured with \n+    topology.script.file.name should be run with. Each arg is an\n+    IP address.\n+  </description>\n+</property>\n+\n+\n+\n+</configuration>"
        },
        {
            "sha": "324003a839b16bc60d8b0b08843d0483b73337c6",
            "filename": "src/java/org/apache/hadoop/HadoopVersionAnnotation.java",
            "status": "added",
            "additions": 69,
            "deletions": 0,
            "changes": 69,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2FHadoopVersionAnnotation.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2FHadoopVersionAnnotation.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2FHadoopVersionAnnotation.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop;\n+\n+import java.lang.annotation.*;\n+\n+/**\n+ * A package attribute that captures the version of Hadoop that was compiled.\n+ */\n+@Retention(RetentionPolicy.RUNTIME)\n+@Target(ElementType.PACKAGE)\n+public @interface HadoopVersionAnnotation {\n+ \n+  /**\n+   * Get the Hadoop version\n+   * @return the version string \"0.6.3-dev\"\n+   */\n+  String version();\n+  \n+  /**\n+   * Get the username that compiled Hadoop.\n+   */\n+  String user();\n+  \n+  /**\n+   * Get the date when Hadoop was compiled.\n+   * @return the date in unix 'date' format\n+   */\n+  String date();\n+    \n+  /**\n+   * Get the url for the subversion repository.\n+   */\n+  String url();\n+  \n+  /**\n+   * Get the subversion revision.\n+   * @return the revision number as a string (eg. \"451451\")\n+   */\n+  String revision();\n+\n+  /**\n+   * Get the branch from which this was compiled.\n+   * @return The branch name, e.g. \"trunk\" or \"branches/branch-0.20\"\n+   */\n+  String branch();\n+\n+  /**\n+   * Get a checksum of the source files from which\n+   * Hadoop was compiled.\n+   * @return a string that uniquely identifies the source\n+   **/\n+  String srcChecksum();    \n+}"
        },
        {
            "sha": "f4637f0e82b4cd51c4bdd06f9aa9948387fd2cdb",
            "filename": "src/java/org/apache/hadoop/conf/Configurable.java",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fconf%2FConfigurable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fconf%2FConfigurable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fconf%2FConfigurable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,29 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.conf;\n+\n+/** Something that may be configured with a {@link Configuration}. */\n+public interface Configurable {\n+\n+  /** Set the configuration to be used by this object. */\n+  void setConf(Configuration conf);\n+\n+  /** Return the configuration used by this object. */\n+  Configuration getConf();\n+}"
        },
        {
            "sha": "e1381f3bb62e90905c68149a9e5a82198056962e",
            "filename": "src/java/org/apache/hadoop/conf/Configuration.java",
            "status": "added",
            "additions": 1326,
            "deletions": 0,
            "changes": 1326,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fconf%2FConfiguration.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fconf%2FConfiguration.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fconf%2FConfiguration.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,1326 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.conf;\n+\n+import java.io.BufferedInputStream;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.InputStreamReader;\n+import java.io.OutputStream;\n+import java.io.Reader;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Enumeration;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.ListIterator;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.StringTokenizer;\n+import java.util.WeakHashMap;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import javax.xml.parsers.DocumentBuilder;\n+import javax.xml.parsers.DocumentBuilderFactory;\n+import javax.xml.parsers.ParserConfigurationException;\n+import javax.xml.transform.Transformer;\n+import javax.xml.transform.TransformerFactory;\n+import javax.xml.transform.dom.DOMSource;\n+import javax.xml.transform.stream.StreamResult;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.io.WritableUtils;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.StringUtils;\n+import org.w3c.dom.DOMException;\n+import org.w3c.dom.Document;\n+import org.w3c.dom.Element;\n+import org.w3c.dom.Node;\n+import org.w3c.dom.NodeList;\n+import org.w3c.dom.Text;\n+import org.xml.sax.SAXException;\n+\n+/** \n+ * Provides access to configuration parameters.\n+ *\n+ * <h4 id=\"Resources\">Resources</h4>\n+ *\n+ * <p>Configurations are specified by resources. A resource contains a set of\n+ * name/value pairs as XML data. Each resource is named by either a \n+ * <code>String</code> or by a {@link Path}. If named by a <code>String</code>, \n+ * then the classpath is examined for a file with that name.  If named by a \n+ * <code>Path</code>, then the local filesystem is examined directly, without \n+ * referring to the classpath.\n+ *\n+ * <p>Unless explicitly turned off, Hadoop by default specifies two \n+ * resources, loaded in-order from the classpath: <ol>\n+ * <li><tt><a href=\"{@docRoot}/../core-default.html\">core-default.xml</a>\n+ * </tt>: Read-only defaults for hadoop.</li>\n+ * <li><tt>core-site.xml</tt>: Site-specific configuration for a given hadoop\n+ * installation.</li>\n+ * </ol>\n+ * Applications may add additional resources, which are loaded\n+ * subsequent to these resources in the order they are added.\n+ * \n+ * <h4 id=\"FinalParams\">Final Parameters</h4>\n+ *\n+ * <p>Configuration parameters may be declared <i>final</i>. \n+ * Once a resource declares a value final, no subsequently-loaded \n+ * resource can alter that value.  \n+ * For example, one might define a final parameter with:\n+ * <tt><pre>\n+ *  &lt;property&gt;\n+ *    &lt;name&gt;dfs.client.buffer.dir&lt;/name&gt;\n+ *    &lt;value&gt;/tmp/hadoop/dfs/client&lt;/value&gt;\n+ *    <b>&lt;final&gt;true&lt;/final&gt;</b>\n+ *  &lt;/property&gt;</pre></tt>\n+ *\n+ * Administrators typically define parameters as final in \n+ * <tt>core-site.xml</tt> for values that user applications may not alter.\n+ *\n+ * <h4 id=\"VariableExpansion\">Variable Expansion</h4>\n+ *\n+ * <p>Value strings are first processed for <i>variable expansion</i>. The\n+ * available properties are:<ol>\n+ * <li>Other properties defined in this Configuration; and, if a name is\n+ * undefined here,</li>\n+ * <li>Properties in {@link System#getProperties()}.</li>\n+ * </ol>\n+ *\n+ * <p>For example, if a configuration resource contains the following property\n+ * definitions: \n+ * <tt><pre>\n+ *  &lt;property&gt;\n+ *    &lt;name&gt;basedir&lt;/name&gt;\n+ *    &lt;value&gt;/user/${<i>user.name</i>}&lt;/value&gt;\n+ *  &lt;/property&gt;\n+ *  \n+ *  &lt;property&gt;\n+ *    &lt;name&gt;tempdir&lt;/name&gt;\n+ *    &lt;value&gt;${<i>basedir</i>}/tmp&lt;/value&gt;\n+ *  &lt;/property&gt;</pre></tt>\n+ *\n+ * When <tt>conf.get(\"tempdir\")</tt> is called, then <tt>${<i>basedir</i>}</tt>\n+ * will be resolved to another property in this Configuration, while\n+ * <tt>${<i>user.name</i>}</tt> would then ordinarily be resolved to the value\n+ * of the System property with that name.\n+ */\n+public class Configuration implements Iterable<Map.Entry<String,String>>,\n+                                      Writable {\n+  private static final Log LOG =\n+    LogFactory.getLog(Configuration.class);\n+\n+  private boolean quietmode = true;\n+  \n+  /**\n+   * List of configuration resources.\n+   */\n+  private ArrayList<Object> resources = new ArrayList<Object>();\n+\n+  /**\n+   * List of configuration parameters marked <b>final</b>. \n+   */\n+  private Set<String> finalParameters = new HashSet<String>();\n+  \n+  private boolean loadDefaults = true;\n+  \n+  /**\n+   * Configurtion objects\n+   */\n+  private static final WeakHashMap<Configuration,Object> REGISTRY = \n+    new WeakHashMap<Configuration,Object>();\n+  \n+  /**\n+   * List of default Resources. Resources are loaded in the order of the list \n+   * entries\n+   */\n+  private static final ArrayList<String> defaultResources = \n+    new ArrayList<String>();\n+  \n+  static{\n+    //print deprecation warning if hadoop-site.xml is found in classpath\n+    ClassLoader cL = Thread.currentThread().getContextClassLoader();\n+    if (cL == null) {\n+      cL = Configuration.class.getClassLoader();\n+    }\n+    if(cL.getResource(\"hadoop-site.xml\")!=null) {\n+      LOG.warn(\"DEPRECATED: hadoop-site.xml found in the classpath. \" +\n+          \"Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, \"\n+          + \"mapred-site.xml and hdfs-site.xml to override properties of \" +\n+          \"core-default.xml, mapred-default.xml and hdfs-default.xml \" +\n+          \"respectively\");\n+    }\n+    addDefaultResource(\"core-default.xml\");\n+    addDefaultResource(\"core-site.xml\");\n+  }\n+  \n+  private Properties properties;\n+  private Properties overlay;\n+  private ClassLoader classLoader;\n+  {\n+    classLoader = Thread.currentThread().getContextClassLoader();\n+    if (classLoader == null) {\n+      classLoader = Configuration.class.getClassLoader();\n+    }\n+  }\n+  \n+  /** A new configuration. */\n+  public Configuration() {\n+    this(true);\n+  }\n+\n+  /** A new configuration where the behavior of reading from the default \n+   * resources can be turned off.\n+   * \n+   * If the parameter {@code loadDefaults} is false, the new instance\n+   * will not load resources from the default files. \n+   * @param loadDefaults specifies whether to load from the default files\n+   */\n+  public Configuration(boolean loadDefaults) {\n+    this.loadDefaults = loadDefaults;\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(StringUtils.stringifyException(new IOException(\"config()\")));\n+    }\n+    synchronized(Configuration.class) {\n+      REGISTRY.put(this, null);\n+    }\n+  }\n+  \n+  /** \n+   * A new configuration with the same settings cloned from another.\n+   * \n+   * @param other the configuration from which to clone settings.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  public Configuration(Configuration other) {\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(StringUtils.stringifyException\n+                (new IOException(\"config(config)\")));\n+    }\n+   \n+   this.resources = (ArrayList)other.resources.clone();\n+   synchronized(other) {\n+     if (other.properties != null) {\n+       this.properties = (Properties)other.properties.clone();\n+     }\n+\n+     if (other.overlay!=null) {\n+       this.overlay = (Properties)other.overlay.clone();\n+     }\n+   }\n+   \n+    this.finalParameters = new HashSet<String>(other.finalParameters);\n+    synchronized(Configuration.class) {\n+      REGISTRY.put(this, null);\n+    }\n+  }\n+  \n+  /**\n+   * Add a default resource. Resources are loaded in the order of the resources \n+   * added.\n+   * @param name file name. File should be present in the classpath.\n+   */\n+  public static synchronized void addDefaultResource(String name) {\n+    if(!defaultResources.contains(name)) {\n+      defaultResources.add(name);\n+      for(Configuration conf : REGISTRY.keySet()) {\n+        if(conf.loadDefaults) {\n+          conf.reloadConfiguration();\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Add a configuration resource. \n+   * \n+   * The properties of this resource will override properties of previously \n+   * added resources, unless they were marked <a href=\"#Final\">final</a>. \n+   * \n+   * @param name resource to be added, the classpath is examined for a file \n+   *             with that name.\n+   */\n+  public void addResource(String name) {\n+    addResourceObject(name);\n+  }\n+\n+  /**\n+   * Add a configuration resource. \n+   * \n+   * The properties of this resource will override properties of previously \n+   * added resources, unless they were marked <a href=\"#Final\">final</a>. \n+   * \n+   * @param url url of the resource to be added, the local filesystem is \n+   *            examined directly to find the resource, without referring to \n+   *            the classpath.\n+   */\n+  public void addResource(URL url) {\n+    addResourceObject(url);\n+  }\n+\n+  /**\n+   * Add a configuration resource. \n+   * \n+   * The properties of this resource will override properties of previously \n+   * added resources, unless they were marked <a href=\"#Final\">final</a>. \n+   * \n+   * @param file file-path of resource to be added, the local filesystem is\n+   *             examined directly to find the resource, without referring to \n+   *             the classpath.\n+   */\n+  public void addResource(Path file) {\n+    addResourceObject(file);\n+  }\n+\n+  /**\n+   * Add a configuration resource. \n+   * \n+   * The properties of this resource will override properties of previously \n+   * added resources, unless they were marked <a href=\"#Final\">final</a>. \n+   * \n+   * @param in InputStream to deserialize the object from. \n+   */\n+  public void addResource(InputStream in) {\n+    addResourceObject(in);\n+  }\n+  \n+  \n+  /**\n+   * Reload configuration from previously added resources.\n+   *\n+   * This method will clear all the configuration read from the added \n+   * resources, and final parameters. This will make the resources to \n+   * be read again before accessing the values. Values that are added\n+   * via set methods will overlay values read from the resources.\n+   */\n+  public synchronized void reloadConfiguration() {\n+    properties = null;                            // trigger reload\n+    finalParameters.clear();                      // clear site-limits\n+  }\n+  \n+  private synchronized void addResourceObject(Object resource) {\n+    resources.add(resource);                      // add to resources\n+    reloadConfiguration();\n+  }\n+  \n+  private static Pattern varPat = Pattern.compile(\"\\\\$\\\\{[^\\\\}\\\\$\\u0020]+\\\\}\");\n+  private static int MAX_SUBST = 20;\n+\n+  private String substituteVars(String expr) {\n+    if (expr == null) {\n+      return null;\n+    }\n+    Matcher match = varPat.matcher(\"\");\n+    String eval = expr;\n+    for(int s=0; s<MAX_SUBST; s++) {\n+      match.reset(eval);\n+      if (!match.find()) {\n+        return eval;\n+      }\n+      String var = match.group();\n+      var = var.substring(2, var.length()-1); // remove ${ .. }\n+      String val = null;\n+      try {\n+        val = System.getProperty(var);\n+      } catch(SecurityException se) {\n+        LOG.warn(\"Unexpected SecurityException in Configuration\", se);\n+      }\n+      if (val == null) {\n+        val = getRaw(var);\n+      }\n+      if (val == null) {\n+        return eval; // return literal ${var}: var is unbound\n+      }\n+      // substitute\n+      eval = eval.substring(0, match.start())+val+eval.substring(match.end());\n+    }\n+    throw new IllegalStateException(\"Variable substitution depth too large: \" \n+                                    + MAX_SUBST + \" \" + expr);\n+  }\n+  \n+  /**\n+   * Get the value of the <code>name</code> property, <code>null</code> if\n+   * no such property exists.\n+   * \n+   * Values are processed for <a href=\"#VariableExpansion\">variable expansion</a> \n+   * before being returned. \n+   * \n+   * @param name the property name.\n+   * @return the value of the <code>name</code> property, \n+   *         or null if no such property exists.\n+   */\n+  public String get(String name) {\n+    return substituteVars(getProps().getProperty(name));\n+  }\n+\n+  /**\n+   * Get the value of the <code>name</code> property, without doing\n+   * <a href=\"#VariableExpansion\">variable expansion</a>.\n+   * \n+   * @param name the property name.\n+   * @return the value of the <code>name</code> property, \n+   *         or null if no such property exists.\n+   */\n+  public String getRaw(String name) {\n+    return getProps().getProperty(name);\n+  }\n+\n+  /** \n+   * Set the <code>value</code> of the <code>name</code> property.\n+   * \n+   * @param name property name.\n+   * @param value property value.\n+   */\n+  public void set(String name, String value) {\n+    getOverlay().setProperty(name, value);\n+    getProps().setProperty(name, value);\n+  }\n+  \n+  /**\n+   * Sets a property if it is currently unset.\n+   * @param name the property name\n+   * @param value the new value\n+   */\n+  public void setIfUnset(String name, String value) {\n+    if (get(name) == null) {\n+      set(name, value);\n+    }\n+  }\n+  \n+  private synchronized Properties getOverlay() {\n+    if (overlay==null){\n+      overlay=new Properties();\n+    }\n+    return overlay;\n+  }\n+\n+  /** \n+   * Get the value of the <code>name</code> property. If no such property \n+   * exists, then <code>defaultValue</code> is returned.\n+   * \n+   * @param name property name.\n+   * @param defaultValue default value.\n+   * @return property value, or <code>defaultValue</code> if the property \n+   *         doesn't exist.                    \n+   */\n+  public String get(String name, String defaultValue) {\n+    return substituteVars(getProps().getProperty(name, defaultValue));\n+  }\n+    \n+  /** \n+   * Get the value of the <code>name</code> property as an <code>int</code>.\n+   *   \n+   * If no such property exists, or if the specified value is not a valid\n+   * <code>int</code>, then <code>defaultValue</code> is returned.\n+   * \n+   * @param name property name.\n+   * @param defaultValue default value.\n+   * @return property value as an <code>int</code>, \n+   *         or <code>defaultValue</code>. \n+   */\n+  public int getInt(String name, int defaultValue) {\n+    String valueString = get(name);\n+    if (valueString == null)\n+      return defaultValue;\n+    try {\n+      String hexString = getHexDigits(valueString);\n+      if (hexString != null) {\n+        return Integer.parseInt(hexString, 16);\n+      }\n+      return Integer.parseInt(valueString);\n+    } catch (NumberFormatException e) {\n+      return defaultValue;\n+    }\n+  }\n+\n+  /** \n+   * Set the value of the <code>name</code> property to an <code>int</code>.\n+   * \n+   * @param name property name.\n+   * @param value <code>int</code> value of the property.\n+   */\n+  public void setInt(String name, int value) {\n+    set(name, Integer.toString(value));\n+  }\n+\n+\n+  /** \n+   * Get the value of the <code>name</code> property as a <code>long</code>.  \n+   * If no such property is specified, or if the specified value is not a valid\n+   * <code>long</code>, then <code>defaultValue</code> is returned.\n+   * \n+   * @param name property name.\n+   * @param defaultValue default value.\n+   * @return property value as a <code>long</code>, \n+   *         or <code>defaultValue</code>. \n+   */\n+  public long getLong(String name, long defaultValue) {\n+    String valueString = get(name);\n+    if (valueString == null)\n+      return defaultValue;\n+    try {\n+      String hexString = getHexDigits(valueString);\n+      if (hexString != null) {\n+        return Long.parseLong(hexString, 16);\n+      }\n+      return Long.parseLong(valueString);\n+    } catch (NumberFormatException e) {\n+      return defaultValue;\n+    }\n+  }\n+\n+  private String getHexDigits(String value) {\n+    boolean negative = false;\n+    String str = value;\n+    String hexString = null;\n+    if (value.startsWith(\"-\")) {\n+      negative = true;\n+      str = value.substring(1);\n+    }\n+    if (str.startsWith(\"0x\") || str.startsWith(\"0X\")) {\n+      hexString = str.substring(2);\n+      if (negative) {\n+        hexString = \"-\" + hexString;\n+      }\n+      return hexString;\n+    }\n+    return null;\n+  }\n+  \n+  /** \n+   * Set the value of the <code>name</code> property to a <code>long</code>.\n+   * \n+   * @param name property name.\n+   * @param value <code>long</code> value of the property.\n+   */\n+  public void setLong(String name, long value) {\n+    set(name, Long.toString(value));\n+  }\n+\n+  /** \n+   * Get the value of the <code>name</code> property as a <code>float</code>.  \n+   * If no such property is specified, or if the specified value is not a valid\n+   * <code>float</code>, then <code>defaultValue</code> is returned.\n+   * \n+   * @param name property name.\n+   * @param defaultValue default value.\n+   * @return property value as a <code>float</code>, \n+   *         or <code>defaultValue</code>. \n+   */\n+  public float getFloat(String name, float defaultValue) {\n+    String valueString = get(name);\n+    if (valueString == null)\n+      return defaultValue;\n+    try {\n+      return Float.parseFloat(valueString);\n+    } catch (NumberFormatException e) {\n+      return defaultValue;\n+    }\n+  }\n+  /**\n+   * Set the value of the <code>name</code> property to a <code>float</code>.\n+   * \n+   * @param name property name.\n+   * @param value property value.\n+   */\n+  public void setFloat(String name, float value) {\n+    set(name,Float.toString(value));\n+  }\n+ \n+  /** \n+   * Get the value of the <code>name</code> property as a <code>boolean</code>.  \n+   * If no such property is specified, or if the specified value is not a valid\n+   * <code>boolean</code>, then <code>defaultValue</code> is returned.\n+   * \n+   * @param name property name.\n+   * @param defaultValue default value.\n+   * @return property value as a <code>boolean</code>, \n+   *         or <code>defaultValue</code>. \n+   */\n+  public boolean getBoolean(String name, boolean defaultValue) {\n+    String valueString = get(name);\n+    if (\"true\".equals(valueString))\n+      return true;\n+    else if (\"false\".equals(valueString))\n+      return false;\n+    else return defaultValue;\n+  }\n+\n+  /** \n+   * Set the value of the <code>name</code> property to a <code>boolean</code>.\n+   * \n+   * @param name property name.\n+   * @param value <code>boolean</code> value of the property.\n+   */\n+  public void setBoolean(String name, boolean value) {\n+    set(name, Boolean.toString(value));\n+  }\n+\n+  /**\n+   * Set the given property, if it is currently unset.\n+   * @param name property name\n+   * @param value new value\n+   */\n+  public void setBooleanIfUnset(String name, boolean value) {\n+    setIfUnset(name, Boolean.toString(value));\n+  }\n+\n+  /**\n+   * A class that represents a set of positive integer ranges. It parses \n+   * strings of the form: \"2-3,5,7-\" where ranges are separated by comma and \n+   * the lower/upper bounds are separated by dash. Either the lower or upper \n+   * bound may be omitted meaning all values up to or over. So the string \n+   * above means 2, 3, 5, and 7, 8, 9, ...\n+   */\n+  public static class IntegerRanges {\n+    private static class Range {\n+      int start;\n+      int end;\n+    }\n+\n+    List<Range> ranges = new ArrayList<Range>();\n+    \n+    public IntegerRanges() {\n+    }\n+    \n+    public IntegerRanges(String newValue) {\n+      StringTokenizer itr = new StringTokenizer(newValue, \",\");\n+      while (itr.hasMoreTokens()) {\n+        String rng = itr.nextToken().trim();\n+        String[] parts = rng.split(\"-\", 3);\n+        if (parts.length < 1 || parts.length > 2) {\n+          throw new IllegalArgumentException(\"integer range badly formed: \" + \n+                                             rng);\n+        }\n+        Range r = new Range();\n+        r.start = convertToInt(parts[0], 0);\n+        if (parts.length == 2) {\n+          r.end = convertToInt(parts[1], Integer.MAX_VALUE);\n+        } else {\n+          r.end = r.start;\n+        }\n+        if (r.start > r.end) {\n+          throw new IllegalArgumentException(\"IntegerRange from \" + r.start + \n+                                             \" to \" + r.end + \" is invalid\");\n+        }\n+        ranges.add(r);\n+      }\n+    }\n+\n+    /**\n+     * Convert a string to an int treating empty strings as the default value.\n+     * @param value the string value\n+     * @param defaultValue the value for if the string is empty\n+     * @return the desired integer\n+     */\n+    private static int convertToInt(String value, int defaultValue) {\n+      String trim = value.trim();\n+      if (trim.length() == 0) {\n+        return defaultValue;\n+      }\n+      return Integer.parseInt(trim);\n+    }\n+\n+    /**\n+     * Is the given value in the set of ranges\n+     * @param value the value to check\n+     * @return is the value in the ranges?\n+     */\n+    public boolean isIncluded(int value) {\n+      for(Range r: ranges) {\n+        if (r.start <= value && value <= r.end) {\n+          return true;\n+        }\n+      }\n+      return false;\n+    }\n+    \n+    @Override\n+    public String toString() {\n+      StringBuffer result = new StringBuffer();\n+      boolean first = true;\n+      for(Range r: ranges) {\n+        if (first) {\n+          first = false;\n+        } else {\n+          result.append(',');\n+        }\n+        result.append(r.start);\n+        result.append('-');\n+        result.append(r.end);\n+      }\n+      return result.toString();\n+    }\n+  }\n+\n+  /**\n+   * Parse the given attribute as a set of integer ranges\n+   * @param name the attribute name\n+   * @param defaultValue the default value if it is not set\n+   * @return a new set of ranges from the configured value\n+   */\n+  public IntegerRanges getRange(String name, String defaultValue) {\n+    return new IntegerRanges(get(name, defaultValue));\n+  }\n+\n+  /** \n+   * Get the comma delimited values of the <code>name</code> property as \n+   * a collection of <code>String</code>s.  \n+   * If no such property is specified then empty collection is returned.\n+   * <p>\n+   * This is an optimized version of {@link #getStrings(String)}\n+   * \n+   * @param name property name.\n+   * @return property value as a collection of <code>String</code>s. \n+   */\n+  public Collection<String> getStringCollection(String name) {\n+    String valueString = get(name);\n+    return StringUtils.getStringCollection(valueString);\n+  }\n+\n+  /** \n+   * Get the comma delimited values of the <code>name</code> property as \n+   * an array of <code>String</code>s.  \n+   * If no such property is specified then <code>null</code> is returned.\n+   * \n+   * @param name property name.\n+   * @return property value as an array of <code>String</code>s, \n+   *         or <code>null</code>. \n+   */\n+  public String[] getStrings(String name) {\n+    String valueString = get(name);\n+    return StringUtils.getStrings(valueString);\n+  }\n+\n+  /** \n+   * Get the comma delimited values of the <code>name</code> property as \n+   * an array of <code>String</code>s.  \n+   * If no such property is specified then default value is returned.\n+   * \n+   * @param name property name.\n+   * @param defaultValue The default value\n+   * @return property value as an array of <code>String</code>s, \n+   *         or default value. \n+   */\n+  public String[] getStrings(String name, String... defaultValue) {\n+    String valueString = get(name);\n+    if (valueString == null) {\n+      return defaultValue;\n+    } else {\n+      return StringUtils.getStrings(valueString);\n+    }\n+  }\n+\n+  /** \n+   * Set the array of string values for the <code>name</code> property as \n+   * as comma delimited values.  \n+   * \n+   * @param name property name.\n+   * @param values The values\n+   */\n+  public void setStrings(String name, String... values) {\n+    set(name, StringUtils.arrayToString(values));\n+  }\n+ \n+  /**\n+   * Load a class by name.\n+   * \n+   * @param name the class name.\n+   * @return the class object.\n+   * @throws ClassNotFoundException if the class is not found.\n+   */\n+  public Class<?> getClassByName(String name) throws ClassNotFoundException {\n+    return Class.forName(name, true, classLoader);\n+  }\n+\n+  /** \n+   * Get the value of the <code>name</code> property\n+   * as an array of <code>Class</code>.\n+   * The value of the property specifies a list of comma separated class names.  \n+   * If no such property is specified, then <code>defaultValue</code> is \n+   * returned.\n+   * \n+   * @param name the property name.\n+   * @param defaultValue default value.\n+   * @return property value as a <code>Class[]</code>, \n+   *         or <code>defaultValue</code>. \n+   */\n+  public Class<?>[] getClasses(String name, Class<?> ... defaultValue) {\n+    String[] classnames = getStrings(name);\n+    if (classnames == null)\n+      return defaultValue;\n+    try {\n+      Class<?>[] classes = new Class<?>[classnames.length];\n+      for(int i = 0; i < classnames.length; i++) {\n+        classes[i] = getClassByName(classnames[i]);\n+      }\n+      return classes;\n+    } catch (ClassNotFoundException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  /** \n+   * Get the value of the <code>name</code> property as a <code>Class</code>.  \n+   * If no such property is specified, then <code>defaultValue</code> is \n+   * returned.\n+   * \n+   * @param name the class name.\n+   * @param defaultValue default value.\n+   * @return property value as a <code>Class</code>, \n+   *         or <code>defaultValue</code>. \n+   */\n+  public Class<?> getClass(String name, Class<?> defaultValue) {\n+    String valueString = get(name);\n+    if (valueString == null)\n+      return defaultValue;\n+    try {\n+      return getClassByName(valueString);\n+    } catch (ClassNotFoundException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  /** \n+   * Get the value of the <code>name</code> property as a <code>Class</code>\n+   * implementing the interface specified by <code>xface</code>.\n+   *   \n+   * If no such property is specified, then <code>defaultValue</code> is \n+   * returned.\n+   * \n+   * An exception is thrown if the returned class does not implement the named\n+   * interface. \n+   * \n+   * @param name the class name.\n+   * @param defaultValue default value.\n+   * @param xface the interface implemented by the named class.\n+   * @return property value as a <code>Class</code>, \n+   *         or <code>defaultValue</code>.\n+   */\n+  public <U> Class<? extends U> getClass(String name, \n+                                         Class<? extends U> defaultValue, \n+                                         Class<U> xface) {\n+    try {\n+      Class<?> theClass = getClass(name, defaultValue);\n+      if (theClass != null && !xface.isAssignableFrom(theClass))\n+        throw new RuntimeException(theClass+\" not \"+xface.getName());\n+      else if (theClass != null)\n+        return theClass.asSubclass(xface);\n+      else\n+        return null;\n+    } catch (Exception e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  /**\n+   * Get the value of the <code>name</code> property as a <code>List</code>\n+   * of objects implementing the interface specified by <code>xface</code>.\n+   * \n+   * An exception is thrown if any of the classes does not exist, or if it does\n+   * not implement the named interface.\n+   * \n+   * @param name the property name.\n+   * @param xface the interface implemented by the classes named by\n+   *        <code>name</code>.\n+   * @return a <code>List</code> of objects implementing <code>xface</code>.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  public <U> List<U> getInstances(String name, Class<U> xface) {\n+    List<U> ret = new ArrayList<U>();\n+    Class<?>[] classes = getClasses(name);\n+    for (Class<?> cl: classes) {\n+      if (!xface.isAssignableFrom(cl)) {\n+        throw new RuntimeException(cl + \" does not implement \" + xface);\n+      }\n+      ret.add((U)ReflectionUtils.newInstance(cl, this));\n+    }\n+    return ret;\n+  }\n+\n+  /** \n+   * Set the value of the <code>name</code> property to the name of a \n+   * <code>theClass</code> implementing the given interface <code>xface</code>.\n+   * \n+   * An exception is thrown if <code>theClass</code> does not implement the \n+   * interface <code>xface</code>. \n+   * \n+   * @param name property name.\n+   * @param theClass property value.\n+   * @param xface the interface implemented by the named class.\n+   */\n+  public void setClass(String name, Class<?> theClass, Class<?> xface) {\n+    if (!xface.isAssignableFrom(theClass))\n+      throw new RuntimeException(theClass+\" not \"+xface.getName());\n+    set(name, theClass.getName());\n+  }\n+\n+  /** \n+   * Get a local file under a directory named by <i>dirsProp</i> with\n+   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,\n+   * then one is chosen based on <i>path</i>'s hash code.  If the selected\n+   * directory does not exist, an attempt is made to create it.\n+   * \n+   * @param dirsProp directory in which to locate the file.\n+   * @param path file-path.\n+   * @return local file under the directory with the given path.\n+   */\n+  public Path getLocalPath(String dirsProp, String path)\n+    throws IOException {\n+    String[] dirs = getStrings(dirsProp);\n+    int hashCode = path.hashCode();\n+    FileSystem fs = FileSystem.getLocal(this);\n+    for (int i = 0; i < dirs.length; i++) {  // try each local dir\n+      int index = (hashCode+i & Integer.MAX_VALUE) % dirs.length;\n+      Path file = new Path(dirs[index], path);\n+      Path dir = file.getParent();\n+      if (fs.mkdirs(dir) || fs.exists(dir)) {\n+        return file;\n+      }\n+    }\n+    LOG.warn(\"Could not make \" + path + \n+             \" in local directories from \" + dirsProp);\n+    for(int i=0; i < dirs.length; i++) {\n+      int index = (hashCode+i & Integer.MAX_VALUE) % dirs.length;\n+      LOG.warn(dirsProp + \"[\" + index + \"]=\" + dirs[index]);\n+    }\n+    throw new IOException(\"No valid local directories in property: \"+dirsProp);\n+  }\n+\n+  /** \n+   * Get a local file name under a directory named in <i>dirsProp</i> with\n+   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,\n+   * then one is chosen based on <i>path</i>'s hash code.  If the selected\n+   * directory does not exist, an attempt is made to create it.\n+   * \n+   * @param dirsProp directory in which to locate the file.\n+   * @param path file-path.\n+   * @return local file under the directory with the given path.\n+   */\n+  public File getFile(String dirsProp, String path)\n+    throws IOException {\n+    String[] dirs = getStrings(dirsProp);\n+    int hashCode = path.hashCode();\n+    for (int i = 0; i < dirs.length; i++) {  // try each local dir\n+      int index = (hashCode+i & Integer.MAX_VALUE) % dirs.length;\n+      File file = new File(dirs[index], path);\n+      File dir = file.getParentFile();\n+      if (dir.exists() || dir.mkdirs()) {\n+        return file;\n+      }\n+    }\n+    throw new IOException(\"No valid local directories in property: \"+dirsProp);\n+  }\n+\n+  /** \n+   * Get the {@link URL} for the named resource.\n+   * \n+   * @param name resource name.\n+   * @return the url for the named resource.\n+   */\n+  public URL getResource(String name) {\n+    return classLoader.getResource(name);\n+  }\n+  \n+  /** \n+   * Get an input stream attached to the configuration resource with the\n+   * given <code>name</code>.\n+   * \n+   * @param name configuration resource name.\n+   * @return an input stream attached to the resource.\n+   */\n+  public InputStream getConfResourceAsInputStream(String name) {\n+    try {\n+      URL url= getResource(name);\n+\n+      if (url == null) {\n+        LOG.info(name + \" not found\");\n+        return null;\n+      } else {\n+        LOG.info(\"found resource \" + name + \" at \" + url);\n+      }\n+\n+      return url.openStream();\n+    } catch (Exception e) {\n+      return null;\n+    }\n+  }\n+\n+  /** \n+   * Get a {@link Reader} attached to the configuration resource with the\n+   * given <code>name</code>.\n+   * \n+   * @param name configuration resource name.\n+   * @return a reader attached to the resource.\n+   */\n+  public Reader getConfResourceAsReader(String name) {\n+    try {\n+      URL url= getResource(name);\n+\n+      if (url == null) {\n+        LOG.info(name + \" not found\");\n+        return null;\n+      } else {\n+        LOG.info(\"found resource \" + name + \" at \" + url);\n+      }\n+\n+      return new InputStreamReader(url.openStream());\n+    } catch (Exception e) {\n+      return null;\n+    }\n+  }\n+\n+  protected synchronized Properties getProps() {\n+    if (properties == null) {\n+      properties = new Properties();\n+      loadResources(properties, resources, quietmode);\n+      if (overlay!= null)\n+        properties.putAll(overlay);\n+    }\n+    return properties;\n+  }\n+\n+  /**\n+   * Return the number of keys in the configuration.\n+   *\n+   * @return number of keys in the configuration.\n+   */\n+  public int size() {\n+    return getProps().size();\n+  }\n+\n+  /**\n+   * Clears all keys from the configuration.\n+   */\n+  public void clear() {\n+    getProps().clear();\n+    getOverlay().clear();\n+  }\n+\n+  /**\n+   * Get an {@link Iterator} to go through the list of <code>String</code> \n+   * key-value pairs in the configuration.\n+   * \n+   * @return an iterator over the entries.\n+   */\n+  public Iterator<Map.Entry<String, String>> iterator() {\n+    // Get a copy of just the string to string pairs. After the old object\n+    // methods that allow non-strings to be put into configurations are removed,\n+    // we could replace properties with a Map<String,String> and get rid of this\n+    // code.\n+    Map<String,String> result = new HashMap<String,String>();\n+    for(Map.Entry<Object,Object> item: getProps().entrySet()) {\n+      if (item.getKey() instanceof String && \n+          item.getValue() instanceof String) {\n+        result.put((String) item.getKey(), (String) item.getValue());\n+      }\n+    }\n+    return result.entrySet().iterator();\n+  }\n+\n+  private void loadResources(Properties properties,\n+                             ArrayList resources,\n+                             boolean quiet) {\n+    if(loadDefaults) {\n+      for (String resource : defaultResources) {\n+        loadResource(properties, resource, quiet);\n+      }\n+    \n+      //support the hadoop-site.xml as a deprecated case\n+      if(getResource(\"hadoop-site.xml\")!=null) {\n+        loadResource(properties, \"hadoop-site.xml\", quiet);\n+      }\n+    }\n+    \n+    for (Object resource : resources) {\n+      loadResource(properties, resource, quiet);\n+    }\n+  }\n+\n+  private void loadResource(Properties properties, Object name, boolean quiet) {\n+    try {\n+      DocumentBuilderFactory docBuilderFactory \n+        = DocumentBuilderFactory.newInstance();\n+      //ignore all comments inside the xml file\n+      docBuilderFactory.setIgnoringComments(true);\n+\n+      //allow includes in the xml file\n+      docBuilderFactory.setNamespaceAware(true);\n+      try {\n+          docBuilderFactory.setXIncludeAware(true);\n+      } catch (UnsupportedOperationException e) {\n+        LOG.error(\"Failed to set setXIncludeAware(true) for parser \"\n+                + docBuilderFactory\n+                + \":\" + e,\n+                e);\n+      }\n+      DocumentBuilder builder = docBuilderFactory.newDocumentBuilder();\n+      Document doc = null;\n+      Element root = null;\n+\n+      if (name instanceof URL) {                  // an URL resource\n+        URL url = (URL)name;\n+        if (url != null) {\n+          if (!quiet) {\n+            LOG.info(\"parsing \" + url);\n+          }\n+          doc = builder.parse(url.toString());\n+        }\n+      } else if (name instanceof String) {        // a CLASSPATH resource\n+        URL url = getResource((String)name);\n+        if (url != null) {\n+          if (!quiet) {\n+            LOG.info(\"parsing \" + url);\n+          }\n+          doc = builder.parse(url.toString());\n+        }\n+      } else if (name instanceof Path) {          // a file resource\n+        // Can't use FileSystem API or we get an infinite loop\n+        // since FileSystem uses Configuration API.  Use java.io.File instead.\n+        File file = new File(((Path)name).toUri().getPath())\n+          .getAbsoluteFile();\n+        if (file.exists()) {\n+          if (!quiet) {\n+            LOG.info(\"parsing \" + file);\n+          }\n+          InputStream in = new BufferedInputStream(new FileInputStream(file));\n+          try {\n+            doc = builder.parse(in);\n+          } finally {\n+            in.close();\n+          }\n+        }\n+      } else if (name instanceof InputStream) {\n+        try {\n+          doc = builder.parse((InputStream)name);\n+        } finally {\n+          ((InputStream)name).close();\n+        }\n+      } else if (name instanceof Element) {\n+        root = (Element)name;\n+      }\n+\n+      if (doc == null && root == null) {\n+        if (quiet)\n+          return;\n+        throw new RuntimeException(name + \" not found\");\n+      }\n+\n+      if (root == null) {\n+        root = doc.getDocumentElement();\n+      }\n+      if (!\"configuration\".equals(root.getTagName()))\n+        LOG.fatal(\"bad conf file: top-level element not <configuration>\");\n+      NodeList props = root.getChildNodes();\n+      for (int i = 0; i < props.getLength(); i++) {\n+        Node propNode = props.item(i);\n+        if (!(propNode instanceof Element))\n+          continue;\n+        Element prop = (Element)propNode;\n+        if (\"configuration\".equals(prop.getTagName())) {\n+          loadResource(properties, prop, quiet);\n+          continue;\n+        }\n+        if (!\"property\".equals(prop.getTagName()))\n+          LOG.warn(\"bad conf file: element not <property>\");\n+        NodeList fields = prop.getChildNodes();\n+        String attr = null;\n+        String value = null;\n+        boolean finalParameter = false;\n+        for (int j = 0; j < fields.getLength(); j++) {\n+          Node fieldNode = fields.item(j);\n+          if (!(fieldNode instanceof Element))\n+            continue;\n+          Element field = (Element)fieldNode;\n+          if (\"name\".equals(field.getTagName()) && field.hasChildNodes())\n+            attr = ((Text)field.getFirstChild()).getData().trim();\n+          if (\"value\".equals(field.getTagName()) && field.hasChildNodes())\n+            value = ((Text)field.getFirstChild()).getData();\n+          if (\"final\".equals(field.getTagName()) && field.hasChildNodes())\n+            finalParameter = \"true\".equals(((Text)field.getFirstChild()).getData());\n+        }\n+        \n+        // Ignore this parameter if it has already been marked as 'final'\n+        if (attr != null && value != null) {\n+          if (!finalParameters.contains(attr)) {\n+            properties.setProperty(attr, value);\n+            if (finalParameter)\n+              finalParameters.add(attr);\n+          } else {\n+            LOG.warn(name+\":a attempt to override final parameter: \"+attr\n+                     +\";  Ignoring.\");\n+          }\n+        }\n+      }\n+        \n+    } catch (IOException e) {\n+      LOG.fatal(\"error parsing conf file: \" + e);\n+      throw new RuntimeException(e);\n+    } catch (DOMException e) {\n+      LOG.fatal(\"error parsing conf file: \" + e);\n+      throw new RuntimeException(e);\n+    } catch (SAXException e) {\n+      LOG.fatal(\"error parsing conf file: \" + e);\n+      throw new RuntimeException(e);\n+    } catch (ParserConfigurationException e) {\n+      LOG.fatal(\"error parsing conf file: \" + e);\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  /** \n+   * Write out the non-default properties in this configuration to the give\n+   * {@link OutputStream}.\n+   * \n+   * @param out the output stream to write to.\n+   */\n+  public void writeXml(OutputStream out) throws IOException {\n+    Properties properties = getProps();\n+    try {\n+      Document doc =\n+        DocumentBuilderFactory.newInstance().newDocumentBuilder().newDocument();\n+      Element conf = doc.createElement(\"configuration\");\n+      doc.appendChild(conf);\n+      conf.appendChild(doc.createTextNode(\"\\n\"));\n+      for (Enumeration e = properties.keys(); e.hasMoreElements();) {\n+        String name = (String)e.nextElement();\n+        Object object = properties.get(name);\n+        String value = null;\n+        if (object instanceof String) {\n+          value = (String) object;\n+        }else {\n+          continue;\n+        }\n+        Element propNode = doc.createElement(\"property\");\n+        conf.appendChild(propNode);\n+      \n+        Element nameNode = doc.createElement(\"name\");\n+        nameNode.appendChild(doc.createTextNode(name));\n+        propNode.appendChild(nameNode);\n+      \n+        Element valueNode = doc.createElement(\"value\");\n+        valueNode.appendChild(doc.createTextNode(value));\n+        propNode.appendChild(valueNode);\n+\n+        conf.appendChild(doc.createTextNode(\"\\n\"));\n+      }\n+    \n+      DOMSource source = new DOMSource(doc);\n+      StreamResult result = new StreamResult(out);\n+      TransformerFactory transFactory = TransformerFactory.newInstance();\n+      Transformer transformer = transFactory.newTransformer();\n+      transformer.transform(source, result);\n+    } catch (Exception e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  /**\n+   * Get the {@link ClassLoader} for this job.\n+   * \n+   * @return the correct class loader.\n+   */\n+  public ClassLoader getClassLoader() {\n+    return classLoader;\n+  }\n+  \n+  /**\n+   * Set the class loader that will be used to load the various objects.\n+   * \n+   * @param classLoader the new class loader.\n+   */\n+  public void setClassLoader(ClassLoader classLoader) {\n+    this.classLoader = classLoader;\n+  }\n+  \n+  @Override\n+  public String toString() {\n+    StringBuffer sb = new StringBuffer();\n+    sb.append(\"Configuration: \");\n+    if(loadDefaults) {\n+      toString(defaultResources, sb);\n+      if(resources.size()>0) {\n+        sb.append(\", \");\n+      }\n+    }\n+    toString(resources, sb);\n+    return sb.toString();\n+  }\n+\n+  private void toString(ArrayList resources, StringBuffer sb) {\n+    ListIterator i = resources.listIterator();\n+    while (i.hasNext()) {\n+      if (i.nextIndex() != 0) {\n+        sb.append(\", \");\n+      }\n+      sb.append(i.next());\n+    }\n+  }\n+\n+  /** \n+   * Set the quietness-mode. \n+   * \n+   * In the quiet-mode, error and informational messages might not be logged.\n+   * \n+   * @param quietmode <code>true</code> to set quiet-mode on, <code>false</code>\n+   *              to turn it off.\n+   */\n+  public synchronized void setQuietMode(boolean quietmode) {\n+    this.quietmode = quietmode;\n+  }\n+\n+  /** For debugging.  List non-default properties to the terminal and exit. */\n+  public static void main(String[] args) throws Exception {\n+    new Configuration().writeXml(System.out);\n+  }\n+\n+  @Override\n+  public void readFields(DataInput in) throws IOException {\n+    clear();\n+    int size = WritableUtils.readVInt(in);\n+    for(int i=0; i < size; ++i) {\n+      set(org.apache.hadoop.io.Text.readString(in), \n+          org.apache.hadoop.io.Text.readString(in));\n+    }\n+  }\n+\n+  //@Override\n+  public void write(DataOutput out) throws IOException {\n+    Properties props = getProps();\n+    WritableUtils.writeVInt(out, props.size());\n+    for(Map.Entry<Object, Object> item: props.entrySet()) {\n+      org.apache.hadoop.io.Text.writeString(out, (String) item.getKey());\n+      org.apache.hadoop.io.Text.writeString(out, (String) item.getValue());\n+    }\n+  }\n+\n+}"
        },
        {
            "sha": "cd5604e981d765393e6c58c5a3f13e96e2e54484",
            "filename": "src/java/org/apache/hadoop/conf/Configured.java",
            "status": "added",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fconf%2FConfigured.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fconf%2FConfigured.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fconf%2FConfigured.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,46 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.conf;\n+\n+/** Base class for things that may be configured with a {@link Configuration}. */\n+public class Configured implements Configurable {\n+\n+  private Configuration conf;\n+\n+  /** Construct a Configured. */\n+  public Configured() {\n+    this(null);\n+  }\n+  \n+  /** Construct a Configured. */\n+  public Configured(Configuration conf) {\n+    setConf(conf);\n+  }\n+\n+  // inherit javadoc\n+  public void setConf(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  // inherit javadoc\n+  public Configuration getConf() {\n+    return conf;\n+  }\n+\n+}"
        },
        {
            "sha": "0be80bed5f9b01025867a93836ea0ae8cd70b05e",
            "filename": "src/java/org/apache/hadoop/conf/package.html",
            "status": "added",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fconf%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fconf%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fconf%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,23 @@\n+<html>\n+\n+<!--\n+   Licensed to the Apache Software Foundation (ASF) under one or more\n+   contributor license agreements.  See the NOTICE file distributed with\n+   this work for additional information regarding copyright ownership.\n+   The ASF licenses this file to You under the Apache License, Version 2.0\n+   (the \"License\"); you may not use this file except in compliance with\n+   the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+-->\n+\n+<body>\n+Configuration of system parameters.\n+</body>\n+</html>"
        },
        {
            "sha": "9d4a8f9a4262e738804aae3071fb0eddd79ef09c",
            "filename": "src/java/org/apache/hadoop/filecache/DistributedCache.java",
            "status": "added",
            "additions": 879,
            "deletions": 0,
            "changes": 879,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffilecache%2FDistributedCache.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffilecache%2FDistributedCache.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffilecache%2FDistributedCache.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,879 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.filecache;\n+\n+import org.apache.commons.logging.*;\n+import java.io.*;\n+import java.util.*;\n+import org.apache.hadoop.conf.*;\n+import org.apache.hadoop.util.*;\n+import org.apache.hadoop.fs.*;\n+\n+import java.net.URI;\n+\n+/**\n+ * Distribute application-specific large, read-only files efficiently.\n+ * \n+ * <p><code>DistributedCache</code> is a facility provided by the Map-Reduce\n+ * framework to cache files (text, archives, jars etc.) needed by applications.\n+ * </p>\n+ * \n+ * <p>Applications specify the files, via urls (hdfs:// or http://) to be cached \n+ * via the {@link org.apache.hadoop.mapred.JobConf}.\n+ * The <code>DistributedCache</code> assumes that the\n+ * files specified via hdfs:// urls are already present on the \n+ * {@link FileSystem} at the path specified by the url.</p>\n+ * \n+ * <p>The framework will copy the necessary files on to the slave node before \n+ * any tasks for the job are executed on that node. Its efficiency stems from \n+ * the fact that the files are only copied once per job and the ability to \n+ * cache archives which are un-archived on the slaves.</p> \n+ *\n+ * <p><code>DistributedCache</code> can be used to distribute simple, read-only\n+ * data/text files and/or more complex types such as archives, jars etc. \n+ * Archives (zip, tar and tgz/tar.gz files) are un-archived at the slave nodes. \n+ * Jars may be optionally added to the classpath of the tasks, a rudimentary \n+ * software distribution mechanism.  Files have execution permissions.\n+ * Optionally users can also direct it to symlink the distributed cache file(s)\n+ * into the working directory of the task.</p>\n+ * \n+ * <p><code>DistributedCache</code> tracks modification timestamps of the cache \n+ * files. Clearly the cache files should not be modified by the application \n+ * or externally while the job is executing.</p>\n+ * \n+ * <p>Here is an illustrative example on how to use the \n+ * <code>DistributedCache</code>:</p>\n+ * <p><blockquote><pre>\n+ *     // Setting up the cache for the application\n+ *     \n+ *     1. Copy the requisite files to the <code>FileSystem</code>:\n+ *     \n+ *     $ bin/hadoop fs -copyFromLocal lookup.dat /myapp/lookup.dat  \n+ *     $ bin/hadoop fs -copyFromLocal map.zip /myapp/map.zip  \n+ *     $ bin/hadoop fs -copyFromLocal mylib.jar /myapp/mylib.jar\n+ *     $ bin/hadoop fs -copyFromLocal mytar.tar /myapp/mytar.tar\n+ *     $ bin/hadoop fs -copyFromLocal mytgz.tgz /myapp/mytgz.tgz\n+ *     $ bin/hadoop fs -copyFromLocal mytargz.tar.gz /myapp/mytargz.tar.gz\n+ *     \n+ *     2. Setup the application's <code>JobConf</code>:\n+ *     \n+ *     JobConf job = new JobConf();\n+ *     DistributedCache.addCacheFile(new URI(\"/myapp/lookup.dat#lookup.dat\"), \n+ *                                   job);\n+ *     DistributedCache.addCacheArchive(new URI(\"/myapp/map.zip\", job);\n+ *     DistributedCache.addFileToClassPath(new Path(\"/myapp/mylib.jar\"), job);\n+ *     DistributedCache.addCacheArchive(new URI(\"/myapp/mytar.tar\", job);\n+ *     DistributedCache.addCacheArchive(new URI(\"/myapp/mytgz.tgz\", job);\n+ *     DistributedCache.addCacheArchive(new URI(\"/myapp/mytargz.tar.gz\", job);\n+ *     \n+ *     3. Use the cached files in the {@link org.apache.hadoop.mapred.Mapper}\n+ *     or {@link org.apache.hadoop.mapred.Reducer}:\n+ *     \n+ *     public static class MapClass extends MapReduceBase  \n+ *     implements Mapper&lt;K, V, K, V&gt; {\n+ *     \n+ *       private Path[] localArchives;\n+ *       private Path[] localFiles;\n+ *       \n+ *       public void configure(JobConf job) {\n+ *         // Get the cached archives/files\n+ *         localArchives = DistributedCache.getLocalCacheArchives(job);\n+ *         localFiles = DistributedCache.getLocalCacheFiles(job);\n+ *       }\n+ *       \n+ *       public void map(K key, V value, \n+ *                       OutputCollector&lt;K, V&gt; output, Reporter reporter) \n+ *       throws IOException {\n+ *         // Use data from the cached archives/files here\n+ *         // ...\n+ *         // ...\n+ *         output.collect(k, v);\n+ *       }\n+ *     }\n+ *     \n+ * </pre></blockquote></p>\n+ * \n+ * @see org.apache.hadoop.mapred.JobConf\n+ * @see org.apache.hadoop.mapred.JobClient\n+ */\n+public class DistributedCache {\n+  // cacheID to cacheStatus mapping\n+  private static TreeMap<String, CacheStatus> cachedArchives = new TreeMap<String, CacheStatus>();\n+  \n+  private static TreeMap<Path, Long> baseDirSize = new TreeMap<Path, Long>();\n+  \n+  // default total cache size\n+  private static final long DEFAULT_CACHE_SIZE = 10737418240L;\n+\n+  private static final Log LOG =\n+    LogFactory.getLog(DistributedCache.class);\n+  \n+  /**\n+   * Get the locally cached file or archive; it could either be \n+   * previously cached (and valid) or copy it from the {@link FileSystem} now.\n+   * \n+   * @param cache the cache to be localized, this should be specified as \n+   * new URI(hdfs://hostname:port/absolute_path_to_file#LINKNAME). If no schema \n+   * or hostname:port is provided the file is assumed to be in the filesystem\n+   * being used in the Configuration\n+   * @param conf The Confguration file which contains the filesystem\n+   * @param baseDir The base cache Dir where you wnat to localize the files/archives\n+   * @param fileStatus The file status on the dfs.\n+   * @param isArchive if the cache is an archive or a file. In case it is an\n+   *  archive with a .zip or .jar or .tar or .tgz or .tar.gz extension it will\n+   *  be unzipped/unjarred/untarred automatically \n+   *  and the directory where the archive is unzipped/unjarred/untarred is\n+   *  returned as the Path.\n+   *  In case of a file, the path to the file is returned\n+   * @param confFileStamp this is the hdfs file modification timestamp to verify that the \n+   * file to be cached hasn't changed since the job started\n+   * @param currentWorkDir this is the directory where you would want to create symlinks \n+   * for the locally cached files/archives\n+   * @return the path to directory where the archives are unjarred in case of archives,\n+   * the path to the file where the file is copied locally \n+   * @throws IOException\n+   */\n+  public static Path getLocalCache(URI cache, Configuration conf, \n+                                   Path baseDir, FileStatus fileStatus,\n+                                   boolean isArchive, long confFileStamp,\n+                                   Path currentWorkDir) \n+  throws IOException {\n+    return getLocalCache(cache, conf, baseDir, fileStatus, isArchive, \n+        confFileStamp, currentWorkDir, true);\n+  }\n+  /**\n+   * Get the locally cached file or archive; it could either be \n+   * previously cached (and valid) or copy it from the {@link FileSystem} now.\n+   * \n+   * @param cache the cache to be localized, this should be specified as \n+   * new URI(hdfs://hostname:port/absolute_path_to_file#LINKNAME). If no schema \n+   * or hostname:port is provided the file is assumed to be in the filesystem\n+   * being used in the Configuration\n+   * @param conf The Confguration file which contains the filesystem\n+   * @param baseDir The base cache Dir where you wnat to localize the files/archives\n+   * @param fileStatus The file status on the dfs.\n+   * @param isArchive if the cache is an archive or a file. In case it is an\n+   *  archive with a .zip or .jar or .tar or .tgz or .tar.gz extension it will\n+   *  be unzipped/unjarred/untarred automatically \n+   *  and the directory where the archive is unzipped/unjarred/untarred is\n+   *  returned as the Path.\n+   *  In case of a file, the path to the file is returned\n+   * @param confFileStamp this is the hdfs file modification timestamp to verify that the \n+   * file to be cached hasn't changed since the job started\n+   * @param currentWorkDir this is the directory where you would want to create symlinks \n+   * for the locally cached files/archives\n+   * @param honorSymLinkConf if this is false, then the symlinks are not\n+   * created even if conf says so (this is required for an optimization in task\n+   * launches\n+   * @return the path to directory where the archives are unjarred in case of archives,\n+   * the path to the file where the file is copied locally \n+   * @throws IOException\n+   */\n+  public static Path getLocalCache(URI cache, Configuration conf, \n+      Path baseDir, FileStatus fileStatus,\n+      boolean isArchive, long confFileStamp,\n+      Path currentWorkDir, boolean honorSymLinkConf) \n+  throws IOException {\n+    String cacheId = makeRelative(cache, conf);\n+    CacheStatus lcacheStatus;\n+    Path localizedPath;\n+    synchronized (cachedArchives) {\n+      lcacheStatus = cachedArchives.get(cacheId);\n+      if (lcacheStatus == null) {\n+        // was never localized\n+        lcacheStatus = new CacheStatus(baseDir, new Path(baseDir, new Path(cacheId)));\n+        cachedArchives.put(cacheId, lcacheStatus);\n+      }\n+\n+      synchronized (lcacheStatus) {\n+        localizedPath = localizeCache(conf, cache, confFileStamp, lcacheStatus, \n+            fileStatus, isArchive, currentWorkDir, honorSymLinkConf);\n+        lcacheStatus.refcount++;\n+      }\n+    }\n+\n+    // try deleting stuff if you can\n+    long size = 0;\n+    synchronized (baseDirSize) {\n+      Long get = baseDirSize.get(baseDir);\n+      if ( get != null ) {\n+    \tsize = get.longValue();\n+      }\n+    }\n+    // setting the cache size to a default of 10GB\n+    long allowedSize = conf.getLong(\"local.cache.size\", DEFAULT_CACHE_SIZE);\n+    if (allowedSize < size) {\n+      // try some cache deletions\n+      deleteCache(conf);\n+    }\n+    return localizedPath;\n+  }\n+\n+  \n+  /**\n+   * Get the locally cached file or archive; it could either be \n+   * previously cached (and valid) or copy it from the {@link FileSystem} now.\n+   * \n+   * @param cache the cache to be localized, this should be specified as \n+   * new URI(hdfs://hostname:port/absolute_path_to_file#LINKNAME). If no schema \n+   * or hostname:port is provided the file is assumed to be in the filesystem\n+   * being used in the Configuration\n+   * @param conf The Confguration file which contains the filesystem\n+   * @param baseDir The base cache Dir where you wnat to localize the files/archives\n+   * @param isArchive if the cache is an archive or a file. In case it is an \n+   *  archive with a .zip or .jar or .tar or .tgz or .tar.gz extension it will \n+   *  be unzipped/unjarred/untarred automatically \n+   *  and the directory where the archive is unzipped/unjarred/untarred \n+   *  is returned as the Path.\n+   *  In case of a file, the path to the file is returned\n+   * @param confFileStamp this is the hdfs file modification timestamp to verify that the \n+   * file to be cached hasn't changed since the job started\n+   * @param currentWorkDir this is the directory where you would want to create symlinks \n+   * for the locally cached files/archives\n+   * @return the path to directory where the archives are unjarred in case of archives,\n+   * the path to the file where the file is copied locally \n+   * @throws IOException\n+\n+   */\n+  public static Path getLocalCache(URI cache, Configuration conf, \n+                                   Path baseDir, boolean isArchive,\n+                                   long confFileStamp, Path currentWorkDir) \n+  throws IOException {\n+    return getLocalCache(cache, conf, \n+                         baseDir, null, isArchive,\n+                         confFileStamp, currentWorkDir);\n+  }\n+  \n+  /**\n+   * This is the opposite of getlocalcache. When you are done with\n+   * using the cache, you need to release the cache\n+   * @param cache The cache URI to be released\n+   * @param conf configuration which contains the filesystem the cache \n+   * is contained in.\n+   * @throws IOException\n+   */\n+  public static void releaseCache(URI cache, Configuration conf)\n+    throws IOException {\n+    String cacheId = makeRelative(cache, conf);\n+    synchronized (cachedArchives) {\n+      CacheStatus lcacheStatus = cachedArchives.get(cacheId);\n+      if (lcacheStatus == null)\n+        return;\n+      synchronized (lcacheStatus) {\n+        lcacheStatus.refcount--;\n+      }\n+    }\n+  }\n+  \n+  // To delete the caches which have a refcount of zero\n+  \n+  private static void deleteCache(Configuration conf) throws IOException {\n+    // try deleting cache Status with refcount of zero\n+    synchronized (cachedArchives) {\n+      for (Iterator it = cachedArchives.keySet().iterator(); it.hasNext();) {\n+        String cacheId = (String) it.next();\n+        CacheStatus lcacheStatus = cachedArchives.get(cacheId);\n+        synchronized (lcacheStatus) {\n+          if (lcacheStatus.refcount == 0) {\n+            // delete this cache entry\n+            FileSystem.getLocal(conf).delete(lcacheStatus.localLoadPath, true);\n+            synchronized (baseDirSize) {\n+              Long dirSize = baseDirSize.get(lcacheStatus.baseDir);\n+              if ( dirSize != null ) {\n+            \tdirSize -= lcacheStatus.size;\n+            \tbaseDirSize.put(lcacheStatus.baseDir, dirSize);\n+              }\n+            }\n+            it.remove();\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  /*\n+   * Returns the relative path of the dir this cache will be localized in\n+   * relative path that this cache will be localized in. For\n+   * hdfs://hostname:port/absolute_path -- the relative path is\n+   * hostname/absolute path -- if it is just /absolute_path -- then the\n+   * relative path is hostname of DFS this mapred cluster is running\n+   * on/absolute_path\n+   */\n+  public static String makeRelative(URI cache, Configuration conf)\n+    throws IOException {\n+    String host = cache.getHost();\n+    if (host == null) {\n+      host = cache.getScheme();\n+    }\n+    if (host == null) {\n+      URI defaultUri = FileSystem.get(conf).getUri();\n+      host = defaultUri.getHost();\n+      if (host == null) {\n+        host = defaultUri.getScheme();\n+      }\n+    }\n+    String path = host + cache.getPath();\n+    path = path.replace(\":/\",\"/\");                // remove windows device colon\n+    return path;\n+  }\n+\n+  private static Path cacheFilePath(Path p) {\n+    return new Path(p, p.getName());\n+  }\n+\n+  // the method which actually copies the caches locally and unjars/unzips them\n+  // and does chmod for the files\n+  private static Path localizeCache(Configuration conf, \n+                                    URI cache, long confFileStamp,\n+                                    CacheStatus cacheStatus,\n+                                    FileStatus fileStatus,\n+                                    boolean isArchive, \n+                                    Path currentWorkDir,boolean honorSymLinkConf) \n+  throws IOException {\n+    boolean doSymlink = honorSymLinkConf && getSymlink(conf);\n+    if(cache.getFragment() == null) {\n+    \tdoSymlink = false;\n+    }\n+    FileSystem fs = getFileSystem(cache, conf);\n+    String link = currentWorkDir.toString() + Path.SEPARATOR + cache.getFragment();\n+    File flink = new File(link);\n+    if (ifExistsAndFresh(conf, fs, cache, confFileStamp,\n+                           cacheStatus, fileStatus)) {\n+      if (isArchive) {\n+        if (doSymlink){\n+          if (!flink.exists())\n+            FileUtil.symLink(cacheStatus.localLoadPath.toString(), \n+                             link);\n+        }\n+        return cacheStatus.localLoadPath;\n+      }\n+      else {\n+        if (doSymlink){\n+          if (!flink.exists())\n+            FileUtil.symLink(cacheFilePath(cacheStatus.localLoadPath).toString(), \n+                             link);\n+        }\n+        return cacheFilePath(cacheStatus.localLoadPath);\n+      }\n+    } else {\n+      // remove the old archive\n+      // if the old archive cannot be removed since it is being used by another\n+      // job\n+      // return null\n+      if (cacheStatus.refcount > 1 && (cacheStatus.currentStatus == true))\n+        throw new IOException(\"Cache \" + cacheStatus.localLoadPath.toString()\n+                              + \" is in use and cannot be refreshed\");\n+      \n+      FileSystem localFs = FileSystem.getLocal(conf);\n+      localFs.delete(cacheStatus.localLoadPath, true);\n+      synchronized (baseDirSize) {\n+    \tLong dirSize = baseDirSize.get(cacheStatus.baseDir);\n+    \tif ( dirSize != null ) {\n+    \t  dirSize -= cacheStatus.size;\n+    \t  baseDirSize.put(cacheStatus.baseDir, dirSize);\n+    \t}\n+      }\n+      Path parchive = new Path(cacheStatus.localLoadPath,\n+                               new Path(cacheStatus.localLoadPath.getName()));\n+      \n+      if (!localFs.mkdirs(cacheStatus.localLoadPath)) {\n+        throw new IOException(\"Mkdirs failed to create directory \" + \n+                              cacheStatus.localLoadPath.toString());\n+      }\n+\n+      String cacheId = cache.getPath();\n+      fs.copyToLocalFile(new Path(cacheId), parchive);\n+      if (isArchive) {\n+        String tmpArchive = parchive.toString().toLowerCase();\n+        File srcFile = new File(parchive.toString());\n+        File destDir = new File(parchive.getParent().toString());\n+        if (tmpArchive.endsWith(\".jar\")) {\n+          RunJar.unJar(srcFile, destDir);\n+        } else if (tmpArchive.endsWith(\".zip\")) {\n+          FileUtil.unZip(srcFile, destDir);\n+        } else if (isTarFile(tmpArchive)) {\n+          FileUtil.unTar(srcFile, destDir);\n+        }\n+        // else will not do anyhting\n+        // and copy the file into the dir as it is\n+      }\n+      \n+      long cacheSize = FileUtil.getDU(new File(parchive.getParent().toString()));\n+      cacheStatus.size = cacheSize;\n+      synchronized (baseDirSize) {\n+      \tLong dirSize = baseDirSize.get(cacheStatus.baseDir);\n+      \tif( dirSize == null ) {\n+      \t  dirSize = Long.valueOf(cacheSize);\n+      \t} else {\n+      \t  dirSize += cacheSize;\n+      \t}\n+      \tbaseDirSize.put(cacheStatus.baseDir, dirSize);\n+      }\n+      \n+      // do chmod here \n+      try {\n+        //Setting recursive permission to grant everyone read and execute\n+        FileUtil.chmod(cacheStatus.baseDir.toString(), \"ugo+rx\",true);\n+      } catch(InterruptedException e) {\n+    \tLOG.warn(\"Exception in chmod\" + e.toString());\n+      }\n+\n+      // update cacheStatus to reflect the newly cached file\n+      cacheStatus.currentStatus = true;\n+      cacheStatus.mtime = getTimestamp(conf, cache);\n+    }\n+    \n+    if (isArchive){\n+      if (doSymlink){\n+        if (!flink.exists())\n+          FileUtil.symLink(cacheStatus.localLoadPath.toString(), \n+                           link);\n+      }\n+      return cacheStatus.localLoadPath;\n+    }\n+    else {\n+      if (doSymlink){\n+        if (!flink.exists())\n+          FileUtil.symLink(cacheFilePath(cacheStatus.localLoadPath).toString(), \n+                           link);\n+      }\n+      return cacheFilePath(cacheStatus.localLoadPath);\n+    }\n+  }\n+\n+  private static boolean isTarFile(String filename) {\n+    return (filename.endsWith(\".tgz\") || filename.endsWith(\".tar.gz\") ||\n+           filename.endsWith(\".tar\"));\n+  }\n+  \n+  // Checks if the cache has already been localized and is fresh\n+  private static boolean ifExistsAndFresh(Configuration conf, FileSystem fs, \n+                                          URI cache, long confFileStamp, \n+                                          CacheStatus lcacheStatus,\n+                                          FileStatus fileStatus) \n+  throws IOException {\n+    // check for existence of the cache\n+    if (lcacheStatus.currentStatus == false) {\n+      return false;\n+    } else {\n+      long dfsFileStamp;\n+      if (fileStatus != null) {\n+        dfsFileStamp = fileStatus.getModificationTime();\n+      } else {\n+        dfsFileStamp = getTimestamp(conf, cache);\n+      }\n+\n+      // ensure that the file on hdfs hasn't been modified since the job started \n+      if (dfsFileStamp != confFileStamp) {\n+        LOG.fatal(\"File: \" + cache + \" has changed on HDFS since job started\");\n+        throw new IOException(\"File: \" + cache + \n+                              \" has changed on HDFS since job started\");\n+      }\n+      \n+      if (dfsFileStamp != lcacheStatus.mtime) {\n+        // needs refreshing\n+        return false;\n+      }\n+    }\n+    \n+    return true;\n+  }\n+\n+  /**\n+   * Returns mtime of a given cache file on hdfs.\n+   * @param conf configuration\n+   * @param cache cache file \n+   * @return mtime of a given cache file on hdfs\n+   * @throws IOException\n+   */\n+  public static long getTimestamp(Configuration conf, URI cache)\n+    throws IOException {\n+    FileSystem fileSystem = FileSystem.get(cache, conf);\n+    Path filePath = new Path(cache.getPath());\n+\n+    return fileSystem.getFileStatus(filePath).getModificationTime();\n+  }\n+  \n+  /**\n+   * This method create symlinks for all files in a given dir in another directory\n+   * @param conf the configuration\n+   * @param jobCacheDir the target directory for creating symlinks\n+   * @param workDir the directory in which the symlinks are created\n+   * @throws IOException\n+   */\n+  public static void createAllSymlink(Configuration conf, File jobCacheDir, File workDir)\n+    throws IOException{\n+    if ((jobCacheDir == null || !jobCacheDir.isDirectory()) ||\n+           workDir == null || (!workDir.isDirectory())) {\n+      return;\n+    }\n+    boolean createSymlink = getSymlink(conf);\n+    if (createSymlink){\n+      File[] list = jobCacheDir.listFiles();\n+      for (int i=0; i < list.length; i++){\n+        FileUtil.symLink(list[i].getAbsolutePath(),\n+                         new File(workDir, list[i].getName()).toString());\n+      }\n+    }  \n+  }\n+  \n+  private static FileSystem getFileSystem(URI cache, Configuration conf)\n+    throws IOException {\n+    if (\"hdfs\".equals(cache.getScheme()))\n+      return FileSystem.get(cache, conf);\n+    else\n+      return FileSystem.get(conf);\n+  }\n+\n+  /**\n+   * Set the configuration with the given set of archives\n+   * @param archives The list of archives that need to be localized\n+   * @param conf Configuration which will be changed\n+   */\n+  public static void setCacheArchives(URI[] archives, Configuration conf) {\n+    String sarchives = StringUtils.uriToString(archives);\n+    conf.set(\"mapred.cache.archives\", sarchives);\n+  }\n+\n+  /**\n+   * Set the configuration with the given set of files\n+   * @param files The list of files that need to be localized\n+   * @param conf Configuration which will be changed\n+   */\n+  public static void setCacheFiles(URI[] files, Configuration conf) {\n+    String sfiles = StringUtils.uriToString(files);\n+    conf.set(\"mapred.cache.files\", sfiles);\n+  }\n+\n+  /**\n+   * Get cache archives set in the Configuration\n+   * @param conf The configuration which contains the archives\n+   * @return A URI array of the caches set in the Configuration\n+   * @throws IOException\n+   */\n+  public static URI[] getCacheArchives(Configuration conf) throws IOException {\n+    return StringUtils.stringToURI(conf.getStrings(\"mapred.cache.archives\"));\n+  }\n+\n+  /**\n+   * Get cache files set in the Configuration\n+   * @param conf The configuration which contains the files\n+   * @return A URI array of the files set in the Configuration\n+   * @throws IOException\n+   */\n+\n+  public static URI[] getCacheFiles(Configuration conf) throws IOException {\n+    return StringUtils.stringToURI(conf.getStrings(\"mapred.cache.files\"));\n+  }\n+\n+  /**\n+   * Return the path array of the localized caches\n+   * @param conf Configuration that contains the localized archives\n+   * @return A path array of localized caches\n+   * @throws IOException\n+   */\n+  public static Path[] getLocalCacheArchives(Configuration conf)\n+    throws IOException {\n+    return StringUtils.stringToPath(conf\n+                                    .getStrings(\"mapred.cache.localArchives\"));\n+  }\n+\n+  /**\n+   * Return the path array of the localized files\n+   * @param conf Configuration that contains the localized files\n+   * @return A path array of localized files\n+   * @throws IOException\n+   */\n+  public static Path[] getLocalCacheFiles(Configuration conf)\n+    throws IOException {\n+    return StringUtils.stringToPath(conf.getStrings(\"mapred.cache.localFiles\"));\n+  }\n+\n+  /**\n+   * Get the timestamps of the archives\n+   * @param conf The configuration which stored the timestamps\n+   * @return a string array of timestamps \n+   * @throws IOException\n+   */\n+  public static String[] getArchiveTimestamps(Configuration conf) {\n+    return conf.getStrings(\"mapred.cache.archives.timestamps\");\n+  }\n+\n+\n+  /**\n+   * Get the timestamps of the files\n+   * @param conf The configuration which stored the timestamps\n+   * @return a string array of timestamps \n+   * @throws IOException\n+   */\n+  public static String[] getFileTimestamps(Configuration conf) {\n+    return conf.getStrings(\"mapred.cache.files.timestamps\");\n+  }\n+\n+  /**\n+   * This is to check the timestamp of the archives to be localized\n+   * @param conf Configuration which stores the timestamp's\n+   * @param timestamps comma separated list of timestamps of archives.\n+   * The order should be the same as the order in which the archives are added.\n+   */\n+  public static void setArchiveTimestamps(Configuration conf, String timestamps) {\n+    conf.set(\"mapred.cache.archives.timestamps\", timestamps);\n+  }\n+\n+  /**\n+   * This is to check the timestamp of the files to be localized\n+   * @param conf Configuration which stores the timestamp's\n+   * @param timestamps comma separated list of timestamps of files.\n+   * The order should be the same as the order in which the files are added.\n+   */\n+  public static void setFileTimestamps(Configuration conf, String timestamps) {\n+    conf.set(\"mapred.cache.files.timestamps\", timestamps);\n+  }\n+  \n+  /**\n+   * Set the conf to contain the location for localized archives \n+   * @param conf The conf to modify to contain the localized caches\n+   * @param str a comma separated list of local archives\n+   */\n+  public static void setLocalArchives(Configuration conf, String str) {\n+    conf.set(\"mapred.cache.localArchives\", str);\n+  }\n+\n+  /**\n+   * Set the conf to contain the location for localized files \n+   * @param conf The conf to modify to contain the localized caches\n+   * @param str a comma separated list of local files\n+   */\n+  public static void setLocalFiles(Configuration conf, String str) {\n+    conf.set(\"mapred.cache.localFiles\", str);\n+  }\n+\n+  /**\n+   * Add a archives to be localized to the conf\n+   * @param uri The uri of the cache to be localized\n+   * @param conf Configuration to add the cache to\n+   */\n+  public static void addCacheArchive(URI uri, Configuration conf) {\n+    String archives = conf.get(\"mapred.cache.archives\");\n+    conf.set(\"mapred.cache.archives\", archives == null ? uri.toString()\n+             : archives + \",\" + uri.toString());\n+  }\n+  \n+  /**\n+   * Add a file to be localized to the conf\n+   * @param uri The uri of the cache to be localized\n+   * @param conf Configuration to add the cache to\n+   */\n+  public static void addCacheFile(URI uri, Configuration conf) {\n+    String files = conf.get(\"mapred.cache.files\");\n+    conf.set(\"mapred.cache.files\", files == null ? uri.toString() : files + \",\"\n+             + uri.toString());\n+  }\n+\n+  /**\n+   * Add an file path to the current set of classpath entries It adds the file\n+   * to cache as well.\n+   * \n+   * @param file Path of the file to be added\n+   * @param conf Configuration that contains the classpath setting\n+   */\n+  public static void addFileToClassPath(Path file, Configuration conf)\n+    throws IOException {\n+    String classpath = conf.get(\"mapred.job.classpath.files\");\n+    conf.set(\"mapred.job.classpath.files\", classpath == null ? file.toString()\n+             : classpath + System.getProperty(\"path.separator\") + file.toString());\n+    FileSystem fs = FileSystem.get(conf);\n+    URI uri = fs.makeQualified(file).toUri();\n+\n+    addCacheFile(uri, conf);\n+  }\n+\n+  /**\n+   * Get the file entries in classpath as an array of Path\n+   * \n+   * @param conf Configuration that contains the classpath setting\n+   */\n+  public static Path[] getFileClassPaths(Configuration conf) {\n+    String classpath = conf.get(\"mapred.job.classpath.files\");\n+    if (classpath == null)\n+      return null;\n+    ArrayList list = Collections.list(new StringTokenizer(classpath, System\n+                                                          .getProperty(\"path.separator\")));\n+    Path[] paths = new Path[list.size()];\n+    for (int i = 0; i < list.size(); i++) {\n+      paths[i] = new Path((String) list.get(i));\n+    }\n+    return paths;\n+  }\n+\n+  /**\n+   * Add an archive path to the current set of classpath entries. It adds the\n+   * archive to cache as well.\n+   * \n+   * @param archive Path of the archive to be added\n+   * @param conf Configuration that contains the classpath setting\n+   */\n+  public static void addArchiveToClassPath(Path archive, Configuration conf)\n+    throws IOException {\n+    String classpath = conf.get(\"mapred.job.classpath.archives\");\n+    conf.set(\"mapred.job.classpath.archives\", classpath == null ? archive\n+             .toString() : classpath + System.getProperty(\"path.separator\")\n+             + archive.toString());\n+    FileSystem fs = FileSystem.get(conf);\n+    URI uri = fs.makeQualified(archive).toUri();\n+\n+    addCacheArchive(uri, conf);\n+  }\n+\n+  /**\n+   * Get the archive entries in classpath as an array of Path\n+   * \n+   * @param conf Configuration that contains the classpath setting\n+   */\n+  public static Path[] getArchiveClassPaths(Configuration conf) {\n+    String classpath = conf.get(\"mapred.job.classpath.archives\");\n+    if (classpath == null)\n+      return null;\n+    ArrayList list = Collections.list(new StringTokenizer(classpath, System\n+                                                          .getProperty(\"path.separator\")));\n+    Path[] paths = new Path[list.size()];\n+    for (int i = 0; i < list.size(); i++) {\n+      paths[i] = new Path((String) list.get(i));\n+    }\n+    return paths;\n+  }\n+\n+  /**\n+   * This method allows you to create symlinks in the current working directory\n+   * of the task to all the cache files/archives\n+   * @param conf the jobconf \n+   */\n+  public static void createSymlink(Configuration conf){\n+    conf.set(\"mapred.create.symlink\", \"yes\");\n+  }\n+  \n+  /**\n+   * This method checks to see if symlinks are to be create for the \n+   * localized cache files in the current working directory \n+   * @param conf the jobconf\n+   * @return true if symlinks are to be created- else return false\n+   */\n+  public static boolean getSymlink(Configuration conf){\n+    String result = conf.get(\"mapred.create.symlink\");\n+    if (\"yes\".equals(result)){\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  /**\n+   * This method checks if there is a conflict in the fragment names \n+   * of the uris. Also makes sure that each uri has a fragment. It \n+   * is only to be called if you want to create symlinks for \n+   * the various archives and files.\n+   * @param uriFiles The uri array of urifiles\n+   * @param uriArchives the uri array of uri archives\n+   */\n+  public static boolean checkURIs(URI[]  uriFiles, URI[] uriArchives){\n+    if ((uriFiles == null) && (uriArchives == null)){\n+      return true;\n+    }\n+    if (uriFiles != null){\n+      for (int i = 0; i < uriFiles.length; i++){\n+        String frag1 = uriFiles[i].getFragment();\n+        if (frag1 == null)\n+          return false;\n+        for (int j=i+1; j < uriFiles.length; j++){\n+          String frag2 = uriFiles[j].getFragment();\n+          if (frag2 == null)\n+            return false;\n+          if (frag1.equalsIgnoreCase(frag2))\n+            return false;\n+        }\n+        if (uriArchives != null){\n+          for (int j = 0; j < uriArchives.length; j++){\n+            String frag2 = uriArchives[j].getFragment();\n+            if (frag2 == null){\n+              return false;\n+            }\n+            if (frag1.equalsIgnoreCase(frag2))\n+              return false;\n+            for (int k=j+1; k < uriArchives.length; k++){\n+              String frag3 = uriArchives[k].getFragment();\n+              if (frag3 == null)\n+                return false;\n+              if (frag2.equalsIgnoreCase(frag3))\n+                return false;\n+            }\n+          }\n+        }\n+      }\n+    }\n+    return true;\n+  }\n+\n+  private static class CacheStatus {\n+    // false, not loaded yet, true is loaded\n+    boolean currentStatus;\n+\n+    // the local load path of this cache\n+    Path localLoadPath;\n+    \n+    //the base dir where the cache lies\n+    Path baseDir;\n+    \n+    //the size of this cache\n+    long size;\n+\n+    // number of instances using this cache\n+    int refcount;\n+\n+    // the cache-file modification time\n+    long mtime;\n+\n+    public CacheStatus(Path baseDir, Path localLoadPath) {\n+      super();\n+      this.currentStatus = false;\n+      this.localLoadPath = localLoadPath;\n+      this.refcount = 0;\n+      this.mtime = -1;\n+      this.baseDir = baseDir;\n+      this.size = 0;\n+    }\n+  }\n+\n+  /**\n+   * Clear the entire contents of the cache and delete the backing files. This\n+   * should only be used when the server is reinitializing, because the users\n+   * are going to lose their files.\n+   */\n+  public static void purgeCache(Configuration conf) throws IOException {\n+    synchronized (cachedArchives) {\n+      FileSystem localFs = FileSystem.getLocal(conf);\n+      for (Map.Entry<String,CacheStatus> f: cachedArchives.entrySet()) {\n+        try {\n+          localFs.delete(f.getValue().localLoadPath, true);\n+        } catch (IOException ie) {\n+          LOG.debug(\"Error cleaning up cache\", ie);\n+        }\n+      }\n+      cachedArchives.clear();\n+    }\n+  }\n+}"
        },
        {
            "sha": "8fb24a2fb303e3002f2dfecf5db6af129e7b1397",
            "filename": "src/java/org/apache/hadoop/fs/BlockLocation.java",
            "status": "added",
            "additions": 241,
            "deletions": 0,
            "changes": 241,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FBlockLocation.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FBlockLocation.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FBlockLocation.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,241 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import org.apache.hadoop.io.*;\n+\n+import java.io.*;\n+\n+/*\n+ * A BlockLocation lists hosts, offset and length\n+ * of block. \n+ * \n+ */\n+public class BlockLocation implements Writable {\n+\n+  static {               // register a ctor\n+    WritableFactories.setFactory\n+      (BlockLocation.class,\n+       new WritableFactory() {\n+         public Writable newInstance() { return new BlockLocation(); }\n+       });\n+  }\n+\n+  private String[] hosts; //hostnames of datanodes\n+  private String[] names; //hostname:portNumber of datanodes\n+  private String[] topologyPaths; // full path name in network topology\n+  private long offset;  //offset of the of the block in the file\n+  private long length;\n+\n+  /**\n+   * Default Constructor\n+   */\n+  public BlockLocation() {\n+    this(new String[0], new String[0],  0L, 0L);\n+  }\n+\n+  /**\n+   * Constructor with host, name, offset and length\n+   */\n+  public BlockLocation(String[] names, String[] hosts, long offset, \n+                       long length) {\n+    if (names == null) {\n+      this.names = new String[0];\n+    } else {\n+      this.names = names;\n+    }\n+    if (hosts == null) {\n+      this.hosts = new String[0];\n+    } else {\n+      this.hosts = hosts;\n+    }\n+    this.offset = offset;\n+    this.length = length;\n+    this.topologyPaths = new String[0];\n+  }\n+\n+  /**\n+   * Constructor with host, name, network topology, offset and length\n+   */\n+  public BlockLocation(String[] names, String[] hosts, String[] topologyPaths,\n+                       long offset, long length) {\n+    this(names, hosts, offset, length);\n+    if (topologyPaths == null) {\n+      this.topologyPaths = new String[0];\n+    } else {\n+      this.topologyPaths = topologyPaths;\n+    }\n+  }\n+\n+  /**\n+   * Get the list of hosts (hostname) hosting this block\n+   */\n+  public String[] getHosts() throws IOException {\n+    if ((hosts == null) || (hosts.length == 0)) {\n+      return new String[0];\n+    } else {\n+      return hosts;\n+    }\n+  }\n+\n+  /**\n+   * Get the list of names (hostname:port) hosting this block\n+   */\n+  public String[] getNames() throws IOException {\n+    if ((names == null) || (names.length == 0)) {\n+      return new String[0];\n+    } else {\n+      return this.names;\n+    }\n+  }\n+\n+  /**\n+   * Get the list of network topology paths for each of the hosts.\n+   * The last component of the path is the host.\n+   */\n+  public String[] getTopologyPaths() throws IOException {\n+    if ((topologyPaths == null) || (topologyPaths.length == 0)) {\n+      return new String[0];\n+    } else {\n+      return this.topologyPaths;\n+    }\n+  }\n+  \n+  /**\n+   * Get the start offset of file associated with this block\n+   */\n+  public long getOffset() {\n+    return offset;\n+  }\n+  \n+  /**\n+   * Get the length of the block\n+   */\n+  public long getLength() {\n+    return length;\n+  }\n+  \n+  /**\n+   * Set the start offset of file associated with this block\n+   */\n+  public void setOffset(long offset) {\n+    this.offset = offset;\n+  }\n+\n+  /**\n+   * Set the length of block\n+   */\n+  public void setLength(long length) {\n+    this.length = length;\n+  }\n+\n+  /**\n+   * Set the hosts hosting this block\n+   */\n+  public void setHosts(String[] hosts) throws IOException {\n+    if (hosts == null) {\n+      this.hosts = new String[0];\n+    } else {\n+      this.hosts = hosts;\n+    }\n+  }\n+\n+  /**\n+   * Set the names (host:port) hosting this block\n+   */\n+  public void setNames(String[] names) throws IOException {\n+    if (names == null) {\n+      this.names = new String[0];\n+    } else {\n+      this.names = names;\n+    }\n+  }\n+\n+  /**\n+   * Set the network topology paths of the hosts\n+   */\n+  public void setTopologyPaths(String[] topologyPaths) throws IOException {\n+    if (topologyPaths == null) {\n+      this.topologyPaths = new String[0];\n+    } else {\n+      this.topologyPaths = topologyPaths;\n+    }\n+  }\n+\n+  /**\n+   * Implement write of Writable\n+   */\n+  public void write(DataOutput out) throws IOException {\n+    out.writeLong(offset);\n+    out.writeLong(length);\n+    out.writeInt(names.length);\n+    for (int i=0; i < names.length; i++) {\n+      Text name = new Text(names[i]);\n+      name.write(out);\n+    }\n+    out.writeInt(hosts.length);\n+    for (int i=0; i < hosts.length; i++) {\n+      Text host = new Text(hosts[i]);\n+      host.write(out);\n+    }\n+    out.writeInt(topologyPaths.length);\n+    for (int i=0; i < topologyPaths.length; i++) {\n+      Text host = new Text(topologyPaths[i]);\n+      host.write(out);\n+    }\n+  }\n+  \n+  /**\n+   * Implement readFields of Writable\n+   */\n+  public void readFields(DataInput in) throws IOException {\n+    this.offset = in.readLong();\n+    this.length = in.readLong();\n+    int numNames = in.readInt();\n+    this.names = new String[numNames];\n+    for (int i = 0; i < numNames; i++) {\n+      Text name = new Text();\n+      name.readFields(in);\n+      names[i] = name.toString();\n+    }\n+    int numHosts = in.readInt();\n+    for (int i = 0; i < numHosts; i++) {\n+      Text host = new Text();\n+      host.readFields(in);\n+      hosts[i] = host.toString();\n+    }\n+    int numTops = in.readInt();\n+    Text path = new Text();\n+    for (int i = 0; i < numTops; i++) {\n+      path.readFields(in);\n+      topologyPaths[i] = path.toString();\n+    }\n+  }\n+  \n+  public String toString() {\n+    StringBuilder result = new StringBuilder();\n+    result.append(offset);\n+    result.append(',');\n+    result.append(length);\n+    for(String h: hosts) {\n+      result.append(',');\n+      result.append(h);\n+    }\n+    return result.toString();\n+  }\n+}"
        },
        {
            "sha": "f682d969e403d65680b7168829d586e456554390",
            "filename": "src/java/org/apache/hadoop/fs/BufferedFSInputStream.java",
            "status": "added",
            "additions": 96,
            "deletions": 0,
            "changes": 96,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FBufferedFSInputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FBufferedFSInputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FBufferedFSInputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,96 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.BufferedInputStream;\n+import java.io.IOException;\n+\n+\n+/**\n+ * A class optimizes reading from FSInputStream by bufferring\n+ */\n+\n+\n+public class BufferedFSInputStream extends BufferedInputStream\n+implements Seekable, PositionedReadable {\n+  /**\n+   * Creates a <code>BufferedFSInputStream</code>\n+   * with the specified buffer size,\n+   * and saves its  argument, the input stream\n+   * <code>in</code>, for later use.  An internal\n+   * buffer array of length  <code>size</code>\n+   * is created and stored in <code>buf</code>.\n+   *\n+   * @param   in     the underlying input stream.\n+   * @param   size   the buffer size.\n+   * @exception IllegalArgumentException if size <= 0.\n+   */\n+  public BufferedFSInputStream(FSInputStream in, int size) {\n+    super(in, size);\n+  }\n+\n+  public long getPos() throws IOException {\n+    return ((FSInputStream)in).getPos()-(count-pos);\n+  }\n+\n+  public long skip(long n) throws IOException {\n+    if (n <= 0) {\n+      return 0;\n+    }\n+\n+    seek(getPos()+n);\n+    return n;\n+  }\n+\n+  public void seek(long pos) throws IOException {\n+    if( pos<0 ) {\n+      return;\n+    }\n+    // optimize: check if the pos is in the buffer\n+    long end = ((FSInputStream)in).getPos();\n+    long start = end - count;\n+    if( pos>=start && pos<end) {\n+      this.pos = (int)(pos-start);\n+      return;\n+    }\n+\n+    // invalidate buffer\n+    this.pos = 0;\n+    this.count = 0;\n+\n+    ((FSInputStream)in).seek(pos);\n+  }\n+\n+  public boolean seekToNewSource(long targetPos) throws IOException {\n+    pos = 0;\n+    count = 0;\n+    return ((FSInputStream)in).seekToNewSource(targetPos);\n+  }\n+\n+  public int read(long position, byte[] buffer, int offset, int length) throws IOException {\n+    return ((FSInputStream)in).read(position, buffer, offset, length) ;\n+  }\n+\n+  public void readFully(long position, byte[] buffer, int offset, int length) throws IOException {\n+    ((FSInputStream)in).readFully(position, buffer, offset, length);\n+  }\n+\n+  public void readFully(long position, byte[] buffer) throws IOException {\n+    ((FSInputStream)in).readFully(position, buffer);\n+  }\n+}"
        },
        {
            "sha": "1304da9ac3fcb6a2c503d2f3e1bb19ee10a0fce3",
            "filename": "src/java/org/apache/hadoop/fs/ChecksumException.java",
            "status": "added",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FChecksumException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FChecksumException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FChecksumException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,35 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs;\n+\n+import java.io.IOException;\n+\n+/** Thrown for checksum errors. */\n+public class ChecksumException extends IOException {\n+  private static final long serialVersionUID = 1L;\n+  private long pos;\n+  public ChecksumException(String description, long pos) {\n+    super(description);\n+    this.pos = pos;\n+  }\n+  \n+  public long getPos() {\n+    return pos;\n+  }\n+}"
        },
        {
            "sha": "72a09bd75f2c4072aa2eef5dc0e9d0e12cc9ceef",
            "filename": "src/java/org/apache/hadoop/fs/ChecksumFileSystem.java",
            "status": "added",
            "additions": 547,
            "deletions": 0,
            "changes": 547,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FChecksumFileSystem.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FChecksumFileSystem.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FChecksumFileSystem.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,547 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs;\n+\n+import java.io.*;\n+import java.util.Arrays;\n+import java.util.zip.CRC32;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.hadoop.util.StringUtils;\n+\n+/****************************************************************\n+ * Abstract Checksumed FileSystem.\n+ * It provide a basice implementation of a Checksumed FileSystem,\n+ * which creates a checksum file for each raw file.\n+ * It generates & verifies checksums at the client side.\n+ *\n+ *****************************************************************/\n+public abstract class ChecksumFileSystem extends FilterFileSystem {\n+  private static final byte[] CHECKSUM_VERSION = new byte[] {'c', 'r', 'c', 0};\n+  private int bytesPerChecksum = 512;\n+  private boolean verifyChecksum = true;\n+\n+  public static double getApproxChkSumLength(long size) {\n+    return ChecksumFSOutputSummer.CHKSUM_AS_FRACTION * size;\n+  }\n+  \n+  public ChecksumFileSystem(FileSystem fs) {\n+    super(fs);\n+  }\n+\n+  public void setConf(Configuration conf) {\n+    super.setConf(conf);\n+    if (conf != null) {\n+      bytesPerChecksum = conf.getInt(\"io.bytes.per.checksum\", 512);\n+    }\n+  }\n+  \n+  /**\n+   * Set whether to verify checksum.\n+   */\n+  public void setVerifyChecksum(boolean verifyChecksum) {\n+    this.verifyChecksum = verifyChecksum;\n+  }\n+\n+  /** get the raw file system */\n+  public FileSystem getRawFileSystem() {\n+    return fs;\n+  }\n+\n+  /** Return the name of the checksum file associated with a file.*/\n+  public Path getChecksumFile(Path file) {\n+    return new Path(file.getParent(), \".\" + file.getName() + \".crc\");\n+  }\n+\n+  /** Return true iff file is a checksum file name.*/\n+  public static boolean isChecksumFile(Path file) {\n+    String name = file.getName();\n+    return name.startsWith(\".\") && name.endsWith(\".crc\");\n+  }\n+\n+  /** Return the length of the checksum file given the size of the \n+   * actual file.\n+   **/\n+  public long getChecksumFileLength(Path file, long fileSize) {\n+    return getChecksumLength(fileSize, getBytesPerSum());\n+  }\n+\n+  /** Return the bytes Per Checksum */\n+  public int getBytesPerSum() {\n+    return bytesPerChecksum;\n+  }\n+\n+  private int getSumBufferSize(int bytesPerSum, int bufferSize) {\n+    int defaultBufferSize = getConf().getInt(\"io.file.buffer.size\", 4096);\n+    int proportionalBufferSize = bufferSize / bytesPerSum;\n+    return Math.max(bytesPerSum,\n+                    Math.max(proportionalBufferSize, defaultBufferSize));\n+  }\n+\n+  /*******************************************************\n+   * For open()'s FSInputStream\n+   * It verifies that data matches checksums.\n+   *******************************************************/\n+  private static class ChecksumFSInputChecker extends FSInputChecker {\n+    public static final Log LOG \n+      = LogFactory.getLog(FSInputChecker.class);\n+    \n+    private ChecksumFileSystem fs;\n+    private FSDataInputStream datas;\n+    private FSDataInputStream sums;\n+    \n+    private static final int HEADER_LENGTH = 8;\n+    \n+    private int bytesPerSum = 1;\n+    private long fileLen = -1L;\n+    \n+    public ChecksumFSInputChecker(ChecksumFileSystem fs, Path file)\n+      throws IOException {\n+      this(fs, file, fs.getConf().getInt(\"io.file.buffer.size\", 4096));\n+    }\n+    \n+    public ChecksumFSInputChecker(ChecksumFileSystem fs, Path file, int bufferSize)\n+      throws IOException {\n+      super( file, fs.getFileStatus(file).getReplication() );\n+      this.datas = fs.getRawFileSystem().open(file, bufferSize);\n+      this.fs = fs;\n+      Path sumFile = fs.getChecksumFile(file);\n+      try {\n+        int sumBufferSize = fs.getSumBufferSize(fs.getBytesPerSum(), bufferSize);\n+        sums = fs.getRawFileSystem().open(sumFile, sumBufferSize);\n+\n+        byte[] version = new byte[CHECKSUM_VERSION.length];\n+        sums.readFully(version);\n+        if (!Arrays.equals(version, CHECKSUM_VERSION))\n+          throw new IOException(\"Not a checksum file: \"+sumFile);\n+        this.bytesPerSum = sums.readInt();\n+        set(fs.verifyChecksum, new CRC32(), bytesPerSum, 4);\n+      } catch (FileNotFoundException e) {         // quietly ignore\n+        set(fs.verifyChecksum, null, 1, 0);\n+      } catch (IOException e) {                   // loudly ignore\n+        LOG.warn(\"Problem opening checksum file: \"+ file + \n+                 \".  Ignoring exception: \" + \n+                 StringUtils.stringifyException(e));\n+        set(fs.verifyChecksum, null, 1, 0);\n+      }\n+    }\n+    \n+    private long getChecksumFilePos( long dataPos ) {\n+      return HEADER_LENGTH + 4*(dataPos/bytesPerSum);\n+    }\n+    \n+    protected long getChunkPosition( long dataPos ) {\n+      return dataPos/bytesPerSum*bytesPerSum;\n+    }\n+    \n+    public int available() throws IOException {\n+      return datas.available() + super.available();\n+    }\n+    \n+    public int read(long position, byte[] b, int off, int len)\n+      throws IOException {\n+      // parameter check\n+      if ((off | len | (off + len) | (b.length - (off + len))) < 0) {\n+        throw new IndexOutOfBoundsException();\n+      } else if (len == 0) {\n+        return 0;\n+      }\n+      if( position<0 ) {\n+        throw new IllegalArgumentException(\n+            \"Parameter position can not to be negative\");\n+      }\n+\n+      ChecksumFSInputChecker checker = new ChecksumFSInputChecker(fs, file);\n+      checker.seek(position);\n+      int nread = checker.read(b, off, len);\n+      checker.close();\n+      return nread;\n+    }\n+    \n+    public void close() throws IOException {\n+      datas.close();\n+      if( sums != null ) {\n+        sums.close();\n+      }\n+      set(fs.verifyChecksum, null, 1, 0);\n+    }\n+    \n+\n+    @Override\n+    public boolean seekToNewSource(long targetPos) throws IOException {\n+      long sumsPos = getChecksumFilePos(targetPos);\n+      fs.reportChecksumFailure(file, datas, targetPos, sums, sumsPos);\n+      boolean newDataSource = datas.seekToNewSource(targetPos);\n+      return sums.seekToNewSource(sumsPos) || newDataSource;\n+    }\n+\n+    @Override\n+    protected int readChunk(long pos, byte[] buf, int offset, int len,\n+        byte[] checksum) throws IOException {\n+      boolean eof = false;\n+      if(needChecksum()) {\n+        try {\n+          long checksumPos = getChecksumFilePos(pos); \n+          if(checksumPos != sums.getPos()) {\n+            sums.seek(checksumPos);\n+          }\n+          sums.readFully(checksum);\n+        } catch (EOFException e) {\n+          eof = true;\n+        }\n+        len = bytesPerSum;\n+      }\n+      if(pos != datas.getPos()) {\n+        datas.seek(pos);\n+      }\n+      int nread = readFully(datas, buf, offset, len);\n+      if( eof && nread > 0) {\n+        throw new ChecksumException(\"Checksum error: \"+file+\" at \"+pos, pos);\n+      }\n+      return nread;\n+    }\n+    \n+    /* Return the file length */\n+    private long getFileLength() throws IOException {\n+      if( fileLen==-1L ) {\n+        fileLen = fs.getContentSummary(file).getLength();\n+      }\n+      return fileLen;\n+    }\n+    \n+    /**\n+     * Skips over and discards <code>n</code> bytes of data from the\n+     * input stream.\n+     *\n+     *The <code>skip</code> method skips over some smaller number of bytes\n+     * when reaching end of file before <code>n</code> bytes have been skipped.\n+     * The actual number of bytes skipped is returned.  If <code>n</code> is\n+     * negative, no bytes are skipped.\n+     *\n+     * @param      n   the number of bytes to be skipped.\n+     * @return     the actual number of bytes skipped.\n+     * @exception  IOException  if an I/O error occurs.\n+     *             ChecksumException if the chunk to skip to is corrupted\n+     */\n+    public synchronized long skip(long n) throws IOException {\n+      long curPos = getPos();\n+      long fileLength = getFileLength();\n+      if( n+curPos > fileLength ) {\n+        n = fileLength - curPos;\n+      }\n+      return super.skip(n);\n+    }\n+    \n+    /**\n+     * Seek to the given position in the stream.\n+     * The next read() will be from that position.\n+     * \n+     * <p>This method does not allow seek past the end of the file.\n+     * This produces IOException.\n+     *\n+     * @param      pos   the postion to seek to.\n+     * @exception  IOException  if an I/O error occurs or seeks after EOF\n+     *             ChecksumException if the chunk to seek to is corrupted\n+     */\n+\n+    public synchronized void seek(long pos) throws IOException {\n+      if(pos>getFileLength()) {\n+        throw new IOException(\"Cannot seek after EOF\");\n+      }\n+      super.seek(pos);\n+    }\n+\n+  }\n+\n+  /**\n+   * Opens an FSDataInputStream at the indicated Path.\n+   * @param f the file name to open\n+   * @param bufferSize the size of the buffer to be used.\n+   */\n+  @Override\n+  public FSDataInputStream open(Path f, int bufferSize) throws IOException {\n+    return new FSDataInputStream(\n+        new ChecksumFSInputChecker(this, f, bufferSize));\n+  }\n+\n+  /** {@inheritDoc} */\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new IOException(\"Not supported\");\n+  }\n+\n+  /**\n+   * Calculated the length of the checksum file in bytes.\n+   * @param size the length of the data file in bytes\n+   * @param bytesPerSum the number of bytes in a checksum block\n+   * @return the number of bytes in the checksum file\n+   */\n+  public static long getChecksumLength(long size, int bytesPerSum) {\n+    //the checksum length is equal to size passed divided by bytesPerSum +\n+    //bytes written in the beginning of the checksum file.  \n+    return ((size + bytesPerSum - 1) / bytesPerSum) * 4 +\n+             CHECKSUM_VERSION.length + 4;  \n+  }\n+\n+  /** This class provides an output stream for a checksummed file.\n+   * It generates checksums for data. */\n+  private static class ChecksumFSOutputSummer extends FSOutputSummer {\n+    private FSDataOutputStream datas;    \n+    private FSDataOutputStream sums;\n+    private static final float CHKSUM_AS_FRACTION = 0.01f;\n+    \n+    public ChecksumFSOutputSummer(ChecksumFileSystem fs, \n+                          Path file, \n+                          boolean overwrite, \n+                          short replication,\n+                          long blockSize,\n+                          Configuration conf)\n+      throws IOException {\n+      this(fs, file, overwrite, \n+           conf.getInt(\"io.file.buffer.size\", 4096),\n+           replication, blockSize, null);\n+    }\n+    \n+    public ChecksumFSOutputSummer(ChecksumFileSystem fs, \n+                          Path file, \n+                          boolean overwrite,\n+                          int bufferSize,\n+                          short replication,\n+                          long blockSize,\n+                          Progressable progress)\n+      throws IOException {\n+      super(new CRC32(), fs.getBytesPerSum(), 4);\n+      int bytesPerSum = fs.getBytesPerSum();\n+      this.datas = fs.getRawFileSystem().create(file, overwrite, bufferSize, \n+                                         replication, blockSize, progress);\n+      int sumBufferSize = fs.getSumBufferSize(bytesPerSum, bufferSize);\n+      this.sums = fs.getRawFileSystem().create(fs.getChecksumFile(file), true, \n+                                               sumBufferSize, replication,\n+                                               blockSize);\n+      sums.write(CHECKSUM_VERSION, 0, CHECKSUM_VERSION.length);\n+      sums.writeInt(bytesPerSum);\n+    }\n+    \n+    public void close() throws IOException {\n+      flushBuffer();\n+      sums.close();\n+      datas.close();\n+    }\n+    \n+    @Override\n+    protected void writeChunk(byte[] b, int offset, int len, byte[] checksum)\n+    throws IOException {\n+      datas.write(b, offset, len);\n+      sums.write(checksum);\n+    }\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize, short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    Path parent = f.getParent();\n+    if (parent != null && !mkdirs(parent)) {\n+      throw new IOException(\"Mkdirs failed to create \" + parent);\n+    }\n+    final FSDataOutputStream out = new FSDataOutputStream(\n+        new ChecksumFSOutputSummer(this, f, overwrite, bufferSize, replication,\n+            blockSize, progress), null);\n+    if (permission != null) {\n+      setPermission(f, permission);\n+    }\n+    return out;\n+  }\n+\n+  /**\n+   * Set replication for an existing file.\n+   * Implement the abstract <tt>setReplication</tt> of <tt>FileSystem</tt>\n+   * @param src file name\n+   * @param replication new replication\n+   * @throws IOException\n+   * @return true if successful;\n+   *         false if file does not exist or is a directory\n+   */\n+  public boolean setReplication(Path src, short replication) throws IOException {\n+    boolean value = fs.setReplication(src, replication);\n+    if (!value)\n+      return false;\n+\n+    Path checkFile = getChecksumFile(src);\n+    if (exists(checkFile))\n+      fs.setReplication(checkFile, replication);\n+\n+    return true;\n+  }\n+\n+  /**\n+   * Rename files/dirs\n+   */\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    if (fs.isDirectory(src)) {\n+      return fs.rename(src, dst);\n+    } else {\n+\n+      boolean value = fs.rename(src, dst);\n+      if (!value)\n+        return false;\n+\n+      Path checkFile = getChecksumFile(src);\n+      if (fs.exists(checkFile)) { //try to rename checksum\n+        if (fs.isDirectory(dst)) {\n+          value = fs.rename(checkFile, dst);\n+        } else {\n+          value = fs.rename(checkFile, getChecksumFile(dst));\n+        }\n+      }\n+\n+      return value;\n+    }\n+  }\n+\n+  /**\n+   * Implement the delete(Path, boolean) in checksum\n+   * file system.\n+   */\n+  public boolean delete(Path f, boolean recursive) throws IOException{\n+    FileStatus fstatus = null;\n+    try {\n+      fstatus = fs.getFileStatus(f);\n+    } catch(FileNotFoundException e) {\n+      return false;\n+    }\n+    if(fstatus.isDir()) {\n+      //this works since the crcs are in the same\n+      //directories and the files. so we just delete\n+      //everything in the underlying filesystem\n+      return fs.delete(f, recursive);\n+    } else {\n+      Path checkFile = getChecksumFile(f);\n+      if (fs.exists(checkFile)) {\n+        fs.delete(checkFile, true);\n+      }\n+      return fs.delete(f, true);\n+    }\n+  }\n+    \n+  final private static PathFilter DEFAULT_FILTER = new PathFilter() {\n+    public boolean accept(Path file) {\n+      return !isChecksumFile(file);\n+    }\n+  };\n+\n+  /**\n+   * List the statuses of the files/directories in the given path if the path is\n+   * a directory.\n+   * \n+   * @param f\n+   *          given path\n+   * @return the statuses of the files/directories in the given patch\n+   * @throws IOException\n+   */\n+  @Override\n+  public FileStatus[] listStatus(Path f) throws IOException {\n+    return fs.listStatus(f, DEFAULT_FILTER);\n+  }\n+  \n+  @Override\n+  public boolean mkdirs(Path f) throws IOException {\n+    return fs.mkdirs(f);\n+  }\n+\n+  @Override\n+  public void copyFromLocalFile(boolean delSrc, Path src, Path dst)\n+    throws IOException {\n+    Configuration conf = getConf();\n+    FileUtil.copy(getLocal(conf), src, this, dst, delSrc, conf);\n+  }\n+\n+  /**\n+   * The src file is under FS, and the dst is on the local disk.\n+   * Copy it from FS control to the local dst name.\n+   */\n+  @Override\n+  public void copyToLocalFile(boolean delSrc, Path src, Path dst)\n+    throws IOException {\n+    Configuration conf = getConf();\n+    FileUtil.copy(this, src, getLocal(conf), dst, delSrc, conf);\n+  }\n+\n+  /**\n+   * The src file is under FS, and the dst is on the local disk.\n+   * Copy it from FS control to the local dst name.\n+   * If src and dst are directories, the copyCrc parameter\n+   * determines whether to copy CRC files.\n+   */\n+  public void copyToLocalFile(Path src, Path dst, boolean copyCrc)\n+    throws IOException {\n+    if (!fs.isDirectory(src)) { // source is a file\n+      fs.copyToLocalFile(src, dst);\n+      FileSystem localFs = getLocal(getConf()).getRawFileSystem();\n+      if (localFs.isDirectory(dst)) {\n+        dst = new Path(dst, src.getName());\n+      }\n+      dst = getChecksumFile(dst);\n+      if (localFs.exists(dst)) { //remove old local checksum file\n+        localFs.delete(dst, true);\n+      }\n+      Path checksumFile = getChecksumFile(src);\n+      if (copyCrc && fs.exists(checksumFile)) { //copy checksum file\n+        fs.copyToLocalFile(checksumFile, dst);\n+      }\n+    } else {\n+      FileStatus[] srcs = listStatus(src);\n+      for (FileStatus srcFile : srcs) {\n+        copyToLocalFile(srcFile.getPath(), \n+                        new Path(dst, srcFile.getPath().getName()), copyCrc);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile)\n+    throws IOException {\n+    return tmpLocalFile;\n+  }\n+\n+  @Override\n+  public void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile)\n+    throws IOException {\n+    moveFromLocalFile(tmpLocalFile, fsOutputFile);\n+  }\n+\n+  /**\n+   * Report a checksum error to the file system.\n+   * @param f the file name containing the error\n+   * @param in the stream open on the file\n+   * @param inPos the position of the beginning of the bad data in the file\n+   * @param sums the stream open on the checksum file\n+   * @param sumsPos the position of the beginning of the bad data in the checksum file\n+   * @return if retry is neccessary\n+   */\n+  public boolean reportChecksumFailure(Path f, FSDataInputStream in,\n+                                       long inPos, FSDataInputStream sums, long sumsPos) {\n+    return false;\n+  }\n+}"
        },
        {
            "sha": "2ec7959370d12fab973708a72b87ec143b08196b",
            "filename": "src/java/org/apache/hadoop/fs/ContentSummary.java",
            "status": "added",
            "additions": 164,
            "deletions": 0,
            "changes": 164,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FContentSummary.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FContentSummary.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FContentSummary.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,164 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.io.Writable;\n+\n+/** Store the summary of a content (a directory or a file). */\n+public class ContentSummary implements Writable{\n+  private long length;\n+  private long fileCount;\n+  private long directoryCount;\n+  private long quota;\n+  private long spaceConsumed;\n+  private long spaceQuota;\n+  \n+\n+  /** Constructor */\n+  public ContentSummary() {}\n+  \n+  /** Constructor */\n+  public ContentSummary(long length, long fileCount, long directoryCount) {\n+    this(length, fileCount, directoryCount, -1L, length, -1L);\n+  }\n+\n+  /** Constructor */\n+  public ContentSummary(\n+      long length, long fileCount, long directoryCount, long quota,\n+      long spaceConsumed, long spaceQuota) {\n+    this.length = length;\n+    this.fileCount = fileCount;\n+    this.directoryCount = directoryCount;\n+    this.quota = quota;\n+    this.spaceConsumed = spaceConsumed;\n+    this.spaceQuota = spaceQuota;\n+  }\n+\n+  /** @return the length */\n+  public long getLength() {return length;}\n+\n+  /** @return the directory count */\n+  public long getDirectoryCount() {return directoryCount;}\n+\n+  /** @return the file count */\n+  public long getFileCount() {return fileCount;}\n+  \n+  /** Return the directory quota */\n+  public long getQuota() {return quota;}\n+  \n+  /** Retuns (disk) space consumed */ \n+  public long getSpaceConsumed() {return spaceConsumed;}\n+\n+  /** Returns (disk) space quota */\n+  public long getSpaceQuota() {return spaceQuota;}\n+  \n+  /** {@inheritDoc} */\n+  public void write(DataOutput out) throws IOException {\n+    out.writeLong(length);\n+    out.writeLong(fileCount);\n+    out.writeLong(directoryCount);\n+    out.writeLong(quota);\n+    out.writeLong(spaceConsumed);\n+    out.writeLong(spaceQuota);\n+  }\n+\n+  /** {@inheritDoc} */\n+  public void readFields(DataInput in) throws IOException {\n+    this.length = in.readLong();\n+    this.fileCount = in.readLong();\n+    this.directoryCount = in.readLong();\n+    this.quota = in.readLong();\n+    this.spaceConsumed = in.readLong();\n+    this.spaceQuota = in.readLong();\n+  }\n+  \n+  /** \n+   * Output format:\n+   * <----12----> <----12----> <-------18------->\n+   *    DIR_COUNT   FILE_COUNT       CONTENT_SIZE FILE_NAME    \n+   */\n+  private static final String STRING_FORMAT = \"%12d %12d %18d \";\n+  /** \n+   * Output format:\n+   * <----12----> <----15----> <----15----> <----15----> <----12----> <----12----> <-------18------->\n+   *    QUOTA   REMAINING_QUATA SPACE_QUOTA SPACE_QUOTA_REM DIR_COUNT   FILE_COUNT   CONTENT_SIZE     FILE_NAME    \n+   */\n+  private static final String QUOTA_STRING_FORMAT = \"%12s %15s \";\n+  private static final String SPACE_QUOTA_STRING_FORMAT = \"%15s %15s \";\n+  \n+  /** The header string */\n+  private static final String HEADER = String.format(\n+      STRING_FORMAT.replace('d', 's'), \"directories\", \"files\", \"bytes\");\n+\n+  private static final String QUOTA_HEADER = String.format(\n+      QUOTA_STRING_FORMAT + SPACE_QUOTA_STRING_FORMAT, \n+      \"quota\", \"remaining quota\", \"space quota\", \"reamaining quota\") +\n+      HEADER;\n+  \n+  /** Return the header of the output.\n+   * if qOption is false, output directory count, file count, and content size;\n+   * if qOption is true, output quota and remaining quota as well.\n+   * \n+   * @param qOption a flag indicating if quota needs to be printed or not\n+   * @return the header of the output\n+   */\n+  public static String getHeader(boolean qOption) {\n+    return qOption ? QUOTA_HEADER : HEADER;\n+  }\n+  \n+  /** {@inheritDoc} */\n+  public String toString() {\n+    return toString(true);\n+  }\n+\n+  /** Return the string representation of the object in the output format.\n+   * if qOption is false, output directory count, file count, and content size;\n+   * if qOption is true, output quota and remaining quota as well.\n+   * \n+   * @param qOption a flag indicating if quota needs to be printed or not\n+   * @return the string representation of the object\n+   */\n+  public String toString(boolean qOption) {\n+    String prefix = \"\";\n+    if (qOption) {\n+      String quotaStr = \"none\";\n+      String quotaRem = \"inf\";\n+      String spaceQuotaStr = \"none\";\n+      String spaceQuotaRem = \"inf\";\n+      \n+      if (quota>0) {\n+        quotaStr = Long.toString(quota);\n+        quotaRem = Long.toString(quota-(directoryCount+fileCount));\n+      }\n+      if (spaceQuota>0) {\n+        spaceQuotaStr = Long.toString(spaceQuota);\n+        spaceQuotaRem = Long.toString(spaceQuota - spaceConsumed);        \n+      }\n+      \n+      prefix = String.format(QUOTA_STRING_FORMAT + SPACE_QUOTA_STRING_FORMAT, \n+                             quotaStr, quotaRem, spaceQuotaStr, spaceQuotaRem);\n+    }\n+    \n+    return prefix + String.format(STRING_FORMAT, directoryCount, \n+                                  fileCount, length);\n+  }\n+}"
        },
        {
            "sha": "70cea9eb023d2d78f46793f867b1249d984d0542",
            "filename": "src/java/org/apache/hadoop/fs/DF.java",
            "status": "added",
            "additions": 193,
            "deletions": 0,
            "changes": 193,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FDF.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FDF.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FDF.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,193 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.BufferedReader;\n+\n+import java.util.EnumSet;\n+import java.util.StringTokenizer;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.util.Shell;\n+\n+/** Filesystem disk space usage statistics.  Uses the unix 'df' program.\n+ * Tested on Linux, FreeBSD, Cygwin. */\n+public class DF extends Shell {\n+  public static final long DF_INTERVAL_DEFAULT = 3 * 1000; // default DF refresh interval \n+  \n+  private String dirPath;\n+  private String filesystem;\n+  private long capacity;\n+  private long used;\n+  private long available;\n+  private int percentUsed;\n+  private String mount;\n+\n+  enum OSType {\n+    OS_TYPE_UNIX(\"UNIX\"),\n+    OS_TYPE_WIN(\"Windows\"),\n+    OS_TYPE_SOLARIS(\"SunOS\"),\n+    OS_TYPE_MAC(\"Mac\"),\n+    OS_TYPE_AIX(\"AIX\");\n+\n+    private String id;\n+    OSType(String id) {\n+      this.id = id;\n+    }\n+    public boolean match(String osStr) {\n+      return osStr != null && osStr.indexOf(id) >= 0;\n+    }\n+    String getId() {\n+      return id;\n+    }\n+  }\n+\n+  private static final String OS_NAME = System.getProperty(\"os.name\");\n+  private static final OSType OS_TYPE = getOSType(OS_NAME);\n+\n+  protected static OSType getOSType(String osName) {\n+    for (OSType ost : EnumSet.allOf(OSType.class)) {\n+      if (ost.match(osName)) {\n+        return ost;\n+      }\n+    }\n+    return OSType.OS_TYPE_UNIX;\n+  }\n+\n+  public DF(File path, Configuration conf) throws IOException {\n+    this(path, conf.getLong(\"dfs.df.interval\", DF.DF_INTERVAL_DEFAULT));\n+  }\n+\n+  public DF(File path, long dfInterval) throws IOException {\n+    super(dfInterval);\n+    this.dirPath = path.getCanonicalPath();\n+  }\n+\n+  protected OSType getOSType() {\n+    return OS_TYPE;\n+  }\n+  \n+  /// ACCESSORS\n+\n+  public String getDirPath() {\n+    return dirPath;\n+  }\n+  \n+  public String getFilesystem() throws IOException { \n+    run(); \n+    return filesystem; \n+  }\n+  \n+  public long getCapacity() throws IOException { \n+    run(); \n+    return capacity; \n+  }\n+  \n+  public long getUsed() throws IOException { \n+    run(); \n+    return used;\n+  }\n+  \n+  public long getAvailable() throws IOException { \n+    run(); \n+    return available;\n+  }\n+  \n+  public int getPercentUsed() throws IOException {\n+    run();\n+    return percentUsed;\n+  }\n+\n+  public String getMount() throws IOException {\n+    run();\n+    return mount;\n+  }\n+  \n+  public String toString() {\n+    return\n+      \"df -k \" + mount +\"\\n\" +\n+      filesystem + \"\\t\" +\n+      capacity / 1024 + \"\\t\" +\n+      used / 1024 + \"\\t\" +\n+      available / 1024 + \"\\t\" +\n+      percentUsed + \"%\\t\" +\n+      mount;\n+  }\n+\n+  @Override\n+  protected String[] getExecString() {\n+    // ignoring the error since the exit code it enough\n+    return new String[] {\"bash\",\"-c\",\"exec 'df' '-k' '\" + dirPath \n+                         + \"' 2>/dev/null\"};\n+  }\n+\n+  @Override\n+  protected void parseExecResult(BufferedReader lines) throws IOException {\n+    lines.readLine();                         // skip headings\n+  \n+    String line = lines.readLine();\n+    if (line == null) {\n+      throw new IOException( \"Expecting a line not the end of stream\" );\n+    }\n+    StringTokenizer tokens =\n+      new StringTokenizer(line, \" \\t\\n\\r\\f%\");\n+    \n+    this.filesystem = tokens.nextToken();\n+    if (!tokens.hasMoreTokens()) {            // for long filesystem name\n+      line = lines.readLine();\n+      if (line == null) {\n+        throw new IOException( \"Expecting a line not the end of stream\" );\n+      }\n+      tokens = new StringTokenizer(line, \" \\t\\n\\r\\f%\");\n+    }\n+\n+    switch(getOSType()) {\n+      case OS_TYPE_AIX:\n+        this.capacity = Long.parseLong(tokens.nextToken()) * 1024;\n+        this.available = Long.parseLong(tokens.nextToken()) * 1024;\n+        this.percentUsed = Integer.parseInt(tokens.nextToken());\n+        tokens.nextToken();\n+        tokens.nextToken();\n+        this.mount = tokens.nextToken();\n+        this.used = this.capacity - this.available;\n+        break;\n+\n+      case OS_TYPE_WIN:\n+      case OS_TYPE_SOLARIS:\n+      case OS_TYPE_MAC:\n+      case OS_TYPE_UNIX:\n+      default:\n+        this.capacity = Long.parseLong(tokens.nextToken()) * 1024;\n+        this.used = Long.parseLong(tokens.nextToken()) * 1024;\n+        this.available = Long.parseLong(tokens.nextToken()) * 1024;\n+        this.percentUsed = Integer.parseInt(tokens.nextToken());\n+        this.mount = tokens.nextToken();\n+        break;\n+   }\n+  }\n+\n+  public static void main(String[] args) throws Exception {\n+    String path = \".\";\n+    if (args.length > 0)\n+      path = args[0];\n+\n+    System.out.println(new DF(new File(path), DF_INTERVAL_DEFAULT).toString());\n+  }\n+}"
        },
        {
            "sha": "2b65ae098759016358efb71bf0ede4338f54cdf5",
            "filename": "src/java/org/apache/hadoop/fs/DU.java",
            "status": "added",
            "additions": 198,
            "deletions": 0,
            "changes": 198,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FDU.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FDU.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FDU.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,198 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.util.Shell;\n+\n+import java.io.BufferedReader;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/** Filesystem disk space usage statistics.  Uses the unix 'du' program*/\n+public class DU extends Shell {\n+  private String  dirPath;\n+\n+  private AtomicLong used = new AtomicLong();\n+  private volatile boolean shouldRun = true;\n+  private Thread refreshUsed;\n+  private IOException duException = null;\n+  private long refreshInterval;\n+  \n+  /**\n+   * Keeps track of disk usage.\n+   * @param path the path to check disk usage in\n+   * @param interval refresh the disk usage at this interval\n+   * @throws IOException if we fail to refresh the disk usage\n+   */\n+  public DU(File path, long interval) throws IOException {\n+    super(0);\n+    \n+    //we set the Shell interval to 0 so it will always run our command\n+    //and use this one to set the thread sleep interval\n+    this.refreshInterval = interval;\n+    this.dirPath = path.getCanonicalPath();\n+    \n+    //populate the used variable\n+    run();\n+  }\n+  \n+  /**\n+   * Keeps track of disk usage.\n+   * @param path the path to check disk usage in\n+   * @param conf configuration object\n+   * @throws IOException if we fail to refresh the disk usage\n+   */\n+  public DU(File path, Configuration conf) throws IOException {\n+    this(path, 600000L);\n+    //10 minutes default refresh interval\n+  }\n+\n+  /**\n+   * This thread refreshes the \"used\" variable.\n+   * \n+   * Future improvements could be to not permanently\n+   * run this thread, instead run when getUsed is called.\n+   **/\n+  class DURefreshThread implements Runnable {\n+    \n+    public void run() {\n+      \n+      while(shouldRun) {\n+\n+        try {\n+          Thread.sleep(refreshInterval);\n+          \n+          try {\n+            //update the used variable\n+            DU.this.run();\n+          } catch (IOException e) {\n+            synchronized (DU.this) {\n+              //save the latest exception so we can return it in getUsed()\n+              duException = e;\n+            }\n+            \n+            LOG.warn(\"Could not get disk usage information\", e);\n+          }\n+        } catch (InterruptedException e) {\n+        }\n+      }\n+    }\n+  }\n+  \n+  /**\n+   * Decrease how much disk space we use.\n+   * @param value decrease by this value\n+   */\n+  public void decDfsUsed(long value) {\n+    used.addAndGet(-value);\n+  }\n+\n+  /**\n+   * Increase how much disk space we use.\n+   * @param value increase by this value\n+   */\n+  public void incDfsUsed(long value) {\n+    used.addAndGet(value);\n+  }\n+  \n+  /**\n+   * @return disk space used \n+   * @throws IOException if the shell command fails\n+   */\n+  public long getUsed() throws IOException {\n+    //if the updating thread isn't started, update on demand\n+    if(refreshUsed == null) {\n+      run();\n+    } else {\n+      synchronized (DU.this) {\n+        //if an exception was thrown in the last run, rethrow\n+        if(duException != null) {\n+          IOException tmp = duException;\n+          duException = null;\n+          throw tmp;\n+        }\n+      }\n+    }\n+    \n+    return used.longValue();\n+  }\n+\n+  /**\n+   * @return the path of which we're keeping track of disk usage\n+   */\n+  public String getDirPath() {\n+    return dirPath;\n+  }\n+  \n+  /**\n+   * Start the disk usage checking thread.\n+   */\n+  public void start() {\n+    //only start the thread if the interval is sane\n+    if(refreshInterval > 0) {\n+      refreshUsed = new Thread(new DURefreshThread(), \n+          \"refreshUsed-\"+dirPath);\n+      refreshUsed.setDaemon(true);\n+      refreshUsed.start();\n+    }\n+  }\n+  \n+  /**\n+   * Shut down the refreshing thread.\n+   */\n+  public void shutdown() {\n+    this.shouldRun = false;\n+    \n+    if(this.refreshUsed != null) {\n+      this.refreshUsed.interrupt();\n+    }\n+  }\n+  \n+  public String toString() {\n+    return\n+      \"du -sk \" + dirPath +\"\\n\" +\n+      used + \"\\t\" + dirPath;\n+  }\n+\n+  protected String[] getExecString() {\n+    return new String[] {\"du\", \"-sk\", dirPath};\n+  }\n+  \n+  protected void parseExecResult(BufferedReader lines) throws IOException {\n+    String line = lines.readLine();\n+    if (line == null) {\n+      throw new IOException(\"Expecting a line not the end of stream\");\n+    }\n+    String[] tokens = line.split(\"\\t\");\n+    if(tokens.length == 0) {\n+      throw new IOException(\"Illegal du output\");\n+    }\n+    this.used.set(Long.parseLong(tokens[0])*1024);\n+  }\n+\n+  public static void main(String[] args) throws Exception {\n+    String path = \".\";\n+    if (args.length > 0) {\n+      path = args[0];\n+    }\n+\n+    System.out.println(new DU(new File(path), new Configuration()).toString());\n+  }\n+}"
        },
        {
            "sha": "6c59b701f238d214670c602ac88f1cf8bb286307",
            "filename": "src/java/org/apache/hadoop/fs/FSDataInputStream.java",
            "status": "added",
            "additions": 62,
            "deletions": 0,
            "changes": 62,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSDataInputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSDataInputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSDataInputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,62 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.*;\n+\n+/** Utility that wraps a {@link FSInputStream} in a {@link DataInputStream}\n+ * and buffers input through a {@link BufferedInputStream}. */\n+public class FSDataInputStream extends DataInputStream\n+    implements Seekable, PositionedReadable {\n+\n+  public FSDataInputStream(InputStream in)\n+    throws IOException {\n+    super(in);\n+    if( !(in instanceof Seekable) || !(in instanceof PositionedReadable) ) {\n+      throw new IllegalArgumentException(\n+          \"In is not an instance of Seekable or PositionedReadable\");\n+    }\n+  }\n+  \n+  public synchronized void seek(long desired) throws IOException {\n+    ((Seekable)in).seek(desired);\n+  }\n+\n+  public long getPos() throws IOException {\n+    return ((Seekable)in).getPos();\n+  }\n+  \n+  public int read(long position, byte[] buffer, int offset, int length)\n+    throws IOException {\n+    return ((PositionedReadable)in).read(position, buffer, offset, length);\n+  }\n+  \n+  public void readFully(long position, byte[] buffer, int offset, int length)\n+    throws IOException {\n+    ((PositionedReadable)in).readFully(position, buffer, offset, length);\n+  }\n+  \n+  public void readFully(long position, byte[] buffer)\n+    throws IOException {\n+    ((PositionedReadable)in).readFully(position, buffer, 0, buffer.length);\n+  }\n+  \n+  public boolean seekToNewSource(long targetPos) throws IOException {\n+    return ((Seekable)in).seekToNewSource(targetPos); \n+  }\n+}"
        },
        {
            "sha": "ac13d74c3b2ff93e7353c37bb3bd43f69248a73b",
            "filename": "src/java/org/apache/hadoop/fs/FSDataOutputStream.java",
            "status": "added",
            "additions": 100,
            "deletions": 0,
            "changes": 100,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSDataOutputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSDataOutputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSDataOutputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,100 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.*;\n+\n+/** Utility that wraps a {@link OutputStream} in a {@link DataOutputStream},\n+ * buffers output through a {@link BufferedOutputStream} and creates a checksum\n+ * file. */\n+public class FSDataOutputStream extends DataOutputStream implements Syncable {\n+  private OutputStream wrappedStream;\n+\n+  private static class PositionCache extends FilterOutputStream {\n+    private FileSystem.Statistics statistics;\n+    long position;\n+\n+    public PositionCache(OutputStream out, \n+                         FileSystem.Statistics stats,\n+                         long pos) throws IOException {\n+      super(out);\n+      statistics = stats;\n+      position = pos;\n+    }\n+\n+    public void write(int b) throws IOException {\n+      out.write(b);\n+      position++;\n+      if (statistics != null) {\n+        statistics.incrementBytesWritten(1);\n+      }\n+    }\n+    \n+    public void write(byte b[], int off, int len) throws IOException {\n+      out.write(b, off, len);\n+      position += len;                            // update position\n+      if (statistics != null) {\n+        statistics.incrementBytesWritten(len);\n+      }\n+    }\n+      \n+    public long getPos() throws IOException {\n+      return position;                            // return cached position\n+    }\n+    \n+    public void close() throws IOException {\n+      out.close();\n+    }\n+  }\n+\n+  @Deprecated\n+  public FSDataOutputStream(OutputStream out) throws IOException {\n+    this(out, null);\n+  }\n+\n+  public FSDataOutputStream(OutputStream out, FileSystem.Statistics stats)\n+    throws IOException {\n+    this(out, stats, 0);\n+  }\n+\n+  public FSDataOutputStream(OutputStream out, FileSystem.Statistics stats,\n+                            long startPosition) throws IOException {\n+    super(new PositionCache(out, stats, startPosition));\n+    wrappedStream = out;\n+  }\n+  \n+  public long getPos() throws IOException {\n+    return ((PositionCache)out).getPos();\n+  }\n+\n+  public void close() throws IOException {\n+    out.close();         // This invokes PositionCache.close()\n+  }\n+\n+  // Returns the underlying output stream. This is used by unit tests.\n+  public OutputStream getWrappedStream() {\n+    return wrappedStream;\n+  }\n+\n+  /** {@inheritDoc} */\n+  public void sync() throws IOException {\n+    if (wrappedStream instanceof Syncable) {\n+      ((Syncable)wrappedStream).sync();\n+    }\n+  }\n+}"
        },
        {
            "sha": "8dd19125898ef1ac3e9e3327a367b5bb6f67994c",
            "filename": "src/java/org/apache/hadoop/fs/FSError.java",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSError.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSError.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSError.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,29 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs;\n+\n+/** Thrown for unexpected filesystem errors, presumed to reflect disk errors\n+ * in the native filesystem. */\n+public class FSError extends Error {\n+  private static final long serialVersionUID = 1L;\n+\n+  FSError(Throwable cause) {\n+    super(cause);\n+  }\n+}"
        },
        {
            "sha": "1d8e03ff9357e3c90ef677a2b34f83837368f392",
            "filename": "src/java/org/apache/hadoop/fs/FSInputChecker.java",
            "status": "added",
            "additions": 432,
            "deletions": 0,
            "changes": 432,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSInputChecker.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSInputChecker.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSInputChecker.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,432 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.zip.Checksum;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.util.StringUtils;\n+\n+/**\n+ * This is a generic input stream for verifying checksums for\n+ * data before it is read by a user.\n+ */\n+\n+abstract public class FSInputChecker extends FSInputStream {\n+  public static final Log LOG \n+  = LogFactory.getLog(FSInputChecker.class);\n+  \n+  /** The file name from which data is read from */\n+  protected Path file;\n+  private Checksum sum;\n+  private boolean verifyChecksum = true;\n+  private byte[] buf;\n+  private byte[] checksum;\n+  private int pos;\n+  private int count;\n+  \n+  private int numOfRetries;\n+  \n+  // cached file position\n+  private long chunkPos = 0;\n+  \n+  /** Constructor\n+   * \n+   * @param file The name of the file to be read\n+   * @param numOfRetries Number of read retries when ChecksumError occurs\n+   */\n+  protected FSInputChecker( Path file, int numOfRetries) {\n+    this.file = file;\n+    this.numOfRetries = numOfRetries;\n+  }\n+  \n+  /** Constructor\n+   * \n+   * @param file The name of the file to be read\n+   * @param numOfRetries Number of read retries when ChecksumError occurs\n+   * @param sum the type of Checksum engine\n+   * @param chunkSize maximun chunk size\n+   * @param checksumSize the number byte of each checksum\n+   */\n+  protected FSInputChecker( Path file, int numOfRetries, \n+      boolean verifyChecksum, Checksum sum, int chunkSize, int checksumSize ) {\n+    this(file, numOfRetries);\n+    set(verifyChecksum, sum, chunkSize, checksumSize);\n+  }\n+  \n+  /** Reads in next checksum chunk data into <code>buf</code> at <code>offset</code>\n+   * and checksum into <code>checksum</code>.\n+   * The method is used for implementing read, therefore, it should be optimized\n+   * for sequential reading\n+   * @param pos chunkPos\n+   * @param buf desitination buffer\n+   * @param offset offset in buf at which to store data\n+   * @param len maximun number of bytes to read\n+   * @return number of bytes read\n+   */\n+  abstract protected int readChunk(long pos, byte[] buf, int offset, int len,\n+      byte[] checksum) throws IOException;\n+\n+  /** Return position of beginning of chunk containing pos. \n+   *\n+   * @param pos a postion in the file\n+   * @return the starting position of the chunk which contains the byte\n+   */\n+  abstract protected long getChunkPosition(long pos);\n+\n+  /** Return true if there is a need for checksum verification */\n+  protected synchronized boolean needChecksum() {\n+    return verifyChecksum && sum != null;\n+  }\n+  \n+  /**\n+   * Read one checksum-verified byte\n+   * \n+   * @return     the next byte of data, or <code>-1</code> if the end of the\n+   *             stream is reached.\n+   * @exception  IOException  if an I/O error occurs.\n+   */\n+\n+  public synchronized int read() throws IOException {\n+    if (pos >= count) {\n+      fill();\n+      if (pos >= count) {\n+        return -1;\n+      }\n+    }\n+    return buf[pos++] & 0xff;\n+  }\n+  \n+  /**\n+   * Read checksum verified bytes from this byte-input stream into \n+   * the specified byte array, starting at the given offset.\n+   *\n+   * <p> This method implements the general contract of the corresponding\n+   * <code>{@link InputStream#read(byte[], int, int) read}</code> method of\n+   * the <code>{@link InputStream}</code> class.  As an additional\n+   * convenience, it attempts to read as many bytes as possible by repeatedly\n+   * invoking the <code>read</code> method of the underlying stream.  This\n+   * iterated <code>read</code> continues until one of the following\n+   * conditions becomes true: <ul>\n+   *\n+   *   <li> The specified number of bytes have been read,\n+   *\n+   *   <li> The <code>read</code> method of the underlying stream returns\n+   *   <code>-1</code>, indicating end-of-file.\n+   *\n+   * </ul> If the first <code>read</code> on the underlying stream returns\n+   * <code>-1</code> to indicate end-of-file then this method returns\n+   * <code>-1</code>.  Otherwise this method returns the number of bytes\n+   * actually read.\n+   *\n+   * @param      b     destination buffer.\n+   * @param      off   offset at which to start storing bytes.\n+   * @param      len   maximum number of bytes to read.\n+   * @return     the number of bytes read, or <code>-1</code> if the end of\n+   *             the stream has been reached.\n+   * @exception  IOException  if an I/O error occurs.\n+   *             ChecksumException if any checksum error occurs\n+   */\n+  public synchronized int read(byte[] b, int off, int len) throws IOException {\n+    // parameter check\n+    if ((off | len | (off + len) | (b.length - (off + len))) < 0) {\n+      throw new IndexOutOfBoundsException();\n+    } else if (len == 0) {\n+      return 0;\n+    }\n+\n+    int n = 0;\n+    for (;;) {\n+      int nread = read1(b, off + n, len - n);\n+      if (nread <= 0) \n+        return (n == 0) ? nread : n;\n+      n += nread;\n+      if (n >= len)\n+        return n;\n+    }\n+  }\n+  \n+  /**\n+   * Fills the buffer with a chunk data. \n+   * No mark is supported.\n+   * This method assumes that all data in the buffer has already been read in,\n+   * hence pos > count.\n+   */\n+  private void fill(  ) throws IOException {\n+    assert(pos>=count);\n+    // fill internal buffer\n+    count = readChecksumChunk(buf, 0, buf.length);\n+  }\n+  \n+  /*\n+   * Read characters into a portion of an array, reading from the underlying\n+   * stream at most once if necessary.\n+   */\n+  private int read1(byte b[], int off, int len)\n+  throws IOException {\n+    int avail = count-pos;\n+    if( avail <= 0 ) {\n+      if(len>=buf.length) {\n+        // read a chunk to user buffer directly; avoid one copy\n+        int nread = readChecksumChunk(b, off, len);\n+        return nread;\n+      } else {\n+        // read a chunk into the local buffer\n+        fill();\n+        if( count <= 0 ) {\n+          return -1;\n+        } else {\n+          avail = count;\n+        }\n+      }\n+    }\n+    \n+    // copy content of the local buffer to the user buffer\n+    int cnt = (avail < len) ? avail : len;\n+    System.arraycopy(buf, pos, b, off, cnt);\n+    pos += cnt;\n+    return cnt;    \n+  }\n+  \n+  /* Read up one checksum chunk to array <i>b</i> at pos <i>off</i>\n+   * It requires a checksum chunk boundary\n+   * in between <cur_pos, cur_pos+len> \n+   * and it stops reading at the boundary or at the end of the stream;\n+   * Otherwise an IllegalArgumentException is thrown.\n+   * This makes sure that all data read are checksum verified.\n+   * \n+   * @param b   the buffer into which the data is read.\n+   * @param off the start offset in array <code>b</code>\n+   *            at which the data is written.\n+   * @param len the maximum number of bytes to read.\n+   * @return    the total number of bytes read into the buffer, or\n+   *            <code>-1</code> if there is no more data because the end of\n+   *            the stream has been reached.\n+   * @throws IOException if an I/O error occurs.\n+   */ \n+  private int readChecksumChunk(byte b[], int off, int len)\n+  throws IOException {\n+    // invalidate buffer\n+    count = pos = 0;\n+          \n+    int read = 0;\n+    boolean retry = true;\n+    int retriesLeft = numOfRetries; \n+    do {\n+      retriesLeft--;\n+\n+      try {\n+        read = readChunk(chunkPos, b, off, len, checksum);\n+        if( read > 0 ) {\n+          if( needChecksum() ) {\n+            sum.update(b, off, read);\n+            verifySum(chunkPos);\n+          }\n+          chunkPos += read;\n+        } \n+        retry = false;\n+      } catch (ChecksumException ce) {\n+          LOG.info(\"Found checksum error: b[\" + off + \", \" + (off+read) + \"]=\"\n+              + StringUtils.byteToHexString(b, off, off + read), ce);\n+          if (retriesLeft == 0) {\n+            throw ce;\n+          }\n+          \n+          // try a new replica\n+          if (seekToNewSource(chunkPos)) {\n+            // Since at least one of the sources is different, \n+            // the read might succeed, so we'll retry.\n+            seek(chunkPos);\n+          } else {\n+            // Neither the data stream nor the checksum stream are being read\n+            // from different sources, meaning we'll still get a checksum error \n+            // if we try to do the read again.  We throw an exception instead.\n+            throw ce;\n+          }\n+        }\n+    } while (retry);\n+    return read;\n+  }\n+  \n+  /* verify checksum for the chunk.\n+   * @throws ChecksumException if there is a mismatch\n+   */\n+  private void verifySum(long errPos) throws ChecksumException {\n+    long crc = getChecksum();\n+    long sumValue = sum.getValue();\n+    sum.reset();\n+    if (crc != sumValue) {\n+      throw new ChecksumException(\n+          \"Checksum error: \"+file+\" at \"+errPos, errPos);\n+    }\n+  }\n+  \n+  /* calculate checksum value */\n+  private long getChecksum() {\n+    return checksum2long(checksum);\n+  }\n+\n+  /** Convert a checksum byte array to a long */\n+  static public long checksum2long(byte[] checksum) {\n+    long crc = 0L;\n+    for(int i=0; i<checksum.length; i++) {\n+      crc |= (0xffL&(long)checksum[i])<<((checksum.length-i-1)*8);\n+    }\n+    return crc;\n+  }\n+  \n+  @Override\n+  public synchronized long getPos() throws IOException {\n+    return chunkPos-(count-pos);\n+  }\n+\n+  @Override\n+  public synchronized int available() throws IOException {\n+    return count-pos;\n+  }\n+  \n+  /**\n+   * Skips over and discards <code>n</code> bytes of data from the\n+   * input stream.\n+   *\n+   * <p>This method may skip more bytes than are remaining in the backing\n+   * file. This produces no exception and the number of bytes skipped\n+   * may include some number of bytes that were beyond the EOF of the\n+   * backing file. Attempting to read from the stream after skipping past\n+   * the end will result in -1 indicating the end of the file.\n+   *\n+   *<p>If <code>n</code> is negative, no bytes are skipped.\n+   *\n+   * @param      n   the number of bytes to be skipped.\n+   * @return     the actual number of bytes skipped.\n+   * @exception  IOException  if an I/O error occurs.\n+   *             ChecksumException if the chunk to skip to is corrupted\n+   */\n+  public synchronized long skip(long n) throws IOException {\n+    if (n <= 0) {\n+      return 0;\n+    }\n+\n+    seek(getPos()+n);\n+    return n;\n+  }\n+\n+  /**\n+   * Seek to the given position in the stream.\n+   * The next read() will be from that position.\n+   * \n+   * <p>This method may seek past the end of the file.\n+   * This produces no exception and an attempt to read from\n+   * the stream will result in -1 indicating the end of the file.\n+   *\n+   * @param      pos   the postion to seek to.\n+   * @exception  IOException  if an I/O error occurs.\n+   *             ChecksumException if the chunk to seek to is corrupted\n+   */\n+\n+  public synchronized void seek(long pos) throws IOException {\n+    if( pos<0 ) {\n+      return;\n+    }\n+    // optimize: check if the pos is in the buffer\n+    long start = chunkPos - this.count;\n+    if( pos>=start && pos<chunkPos) {\n+      this.pos = (int)(pos-start);\n+      return;\n+    }\n+    \n+    // reset the current state\n+    resetState();\n+    \n+    // seek to a checksum boundary\n+    chunkPos = getChunkPosition(pos);\n+    \n+    // scan to the desired position\n+    int delta = (int)(pos - chunkPos);\n+    if( delta > 0) {\n+      readFully(this, new byte[delta], 0, delta);\n+    }\n+  }\n+\n+  /**\n+   * A utility function that tries to read up to <code>len</code> bytes from\n+   * <code>stm</code>\n+   * \n+   * @param stm    an input stream\n+   * @param buf    destiniation buffer\n+   * @param offset offset at which to store data\n+   * @param len    number of bytes to read\n+   * @return actual number of bytes read\n+   * @throws IOException if there is any IO error\n+   */\n+  protected static int readFully(InputStream stm, \n+      byte[] buf, int offset, int len) throws IOException {\n+    int n = 0;\n+    for (;;) {\n+      int nread = stm.read(buf, offset + n, len - n);\n+      if (nread <= 0) \n+        return (n == 0) ? nread : n;\n+      n += nread;\n+      if (n >= len)\n+        return n;\n+    }\n+  }\n+  \n+  /**\n+   * Set the checksum related parameters\n+   * @param verifyChecksum whether to verify checksum\n+   * @param sum which type of checksum to use\n+   * @param maxChunkSize maximun chunk size\n+   * @param checksumSize checksum size\n+   */\n+  final protected synchronized void set(boolean verifyChecksum,\n+      Checksum sum, int maxChunkSize, int checksumSize ) {\n+    this.verifyChecksum = verifyChecksum;\n+    this.sum = sum;\n+    this.buf = new byte[maxChunkSize];\n+    this.checksum = new byte[checksumSize];\n+    this.count = 0;\n+    this.pos = 0;\n+  }\n+\n+  final public boolean markSupported() {\n+    return false;\n+  }\n+  \n+  final public void mark(int readlimit) {\n+  }\n+  \n+  final public void reset() throws IOException {\n+    throw new IOException(\"mark/reset not supported\");\n+  }\n+  \n+\n+  /* reset this FSInputChecker's state */\n+  private void resetState() {\n+    // invalidate buffer\n+    count = 0;\n+    pos = 0;\n+    // reset Checksum\n+    if (sum != null) {\n+      sum.reset();\n+    }\n+  }\n+}"
        },
        {
            "sha": "91cac46cdc5dfccc29555617945c5f8d12cb21b9",
            "filename": "src/java/org/apache/hadoop/fs/FSInputStream.java",
            "status": "added",
            "additions": 78,
            "deletions": 0,
            "changes": 78,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSInputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSInputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSInputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,78 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.*;\n+\n+/****************************************************************\n+ * FSInputStream is a generic old InputStream with a little bit\n+ * of RAF-style seek ability.\n+ *\n+ *****************************************************************/\n+public abstract class FSInputStream extends InputStream\n+    implements Seekable, PositionedReadable {\n+  /**\n+   * Seek to the given offset from the start of the file.\n+   * The next read() will be from that location.  Can't\n+   * seek past the end of the file.\n+   */\n+  public abstract void seek(long pos) throws IOException;\n+\n+  /**\n+   * Return the current offset from the start of the file\n+   */\n+  public abstract long getPos() throws IOException;\n+\n+  /**\n+   * Seeks a different copy of the data.  Returns true if \n+   * found a new source, false otherwise.\n+   */\n+  public abstract boolean seekToNewSource(long targetPos) throws IOException;\n+\n+  public int read(long position, byte[] buffer, int offset, int length)\n+    throws IOException {\n+    synchronized (this) {\n+      long oldPos = getPos();\n+      int nread = -1;\n+      try {\n+        seek(position);\n+        nread = read(buffer, offset, length);\n+      } finally {\n+        seek(oldPos);\n+      }\n+      return nread;\n+    }\n+  }\n+    \n+  public void readFully(long position, byte[] buffer, int offset, int length)\n+    throws IOException {\n+    int nread = 0;\n+    while (nread < length) {\n+      int nbytes = read(position+nread, buffer, offset+nread, length-nread);\n+      if (nbytes < 0) {\n+        throw new EOFException(\"End of file reached before reading fully.\");\n+      }\n+      nread += nbytes;\n+    }\n+  }\n+    \n+  public void readFully(long position, byte[] buffer)\n+    throws IOException {\n+    readFully(position, buffer, 0, buffer.length);\n+  }\n+}"
        },
        {
            "sha": "d730671f539dad56746a456036abe62da037aed0",
            "filename": "src/java/org/apache/hadoop/fs/FSOutputSummer.java",
            "status": "added",
            "additions": 176,
            "deletions": 0,
            "changes": 176,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSOutputSummer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSOutputSummer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFSOutputSummer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,176 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.util.zip.Checksum;\n+\n+/**\n+ * This is a generic output stream for generating checksums for\n+ * data before it is written to the underlying stream\n+ */\n+\n+abstract public class FSOutputSummer extends OutputStream {\n+  // data checksum\n+  private Checksum sum;\n+  // internal buffer for storing data before it is checksumed\n+  private byte buf[];\n+  // internal buffer for storing checksum\n+  private byte checksum[];\n+  // The number of valid bytes in the buffer.\n+  private int count;\n+  \n+  protected FSOutputSummer(Checksum sum, int maxChunkSize, int checksumSize) {\n+    this.sum = sum;\n+    this.buf = new byte[maxChunkSize];\n+    this.checksum = new byte[checksumSize];\n+    this.count = 0;\n+  }\n+  \n+  /* write the data chunk in <code>b</code> staring at <code>offset</code> with\n+   * a length of <code>len</code>, and its checksum\n+   */\n+  protected abstract void writeChunk(byte[] b, int offset, int len, byte[] checksum)\n+  throws IOException;\n+\n+  /** Write one byte */\n+  public synchronized void write(int b) throws IOException {\n+    sum.update(b);\n+    buf[count++] = (byte)b;\n+    if(count == buf.length) {\n+      flushBuffer();\n+    }\n+  }\n+\n+  /**\n+   * Writes <code>len</code> bytes from the specified byte array \n+   * starting at offset <code>off</code> and generate a checksum for\n+   * each data chunk.\n+   *\n+   * <p> This method stores bytes from the given array into this\n+   * stream's buffer before it gets checksumed. The buffer gets checksumed \n+   * and flushed to the underlying output stream when all data \n+   * in a checksum chunk are in the buffer.  If the buffer is empty and\n+   * requested length is at least as large as the size of next checksum chunk\n+   * size, this method will checksum and write the chunk directly \n+   * to the underlying output stream.  Thus it avoids uneccessary data copy.\n+   *\n+   * @param      b     the data.\n+   * @param      off   the start offset in the data.\n+   * @param      len   the number of bytes to write.\n+   * @exception  IOException  if an I/O error occurs.\n+   */\n+  public synchronized void write(byte b[], int off, int len)\n+  throws IOException {\n+    if (off < 0 || len < 0 || off > b.length - len) {\n+      throw new ArrayIndexOutOfBoundsException();\n+    }\n+\n+    for (int n=0;n<len;n+=write1(b, off+n, len-n)) {\n+    }\n+  }\n+  \n+  /**\n+   * Write a portion of an array, flushing to the underlying\n+   * stream at most once if necessary.\n+   */\n+  private int write1(byte b[], int off, int len) throws IOException {\n+    if(count==0 && len>=buf.length) {\n+      // local buffer is empty and user data has one chunk\n+      // checksum and output data\n+      final int length = buf.length;\n+      sum.update(b, off, length);\n+      writeChecksumChunk(b, off, length, false);\n+      return length;\n+    }\n+    \n+    // copy user data to local buffer\n+    int bytesToCopy = buf.length-count;\n+    bytesToCopy = (len<bytesToCopy) ? len : bytesToCopy;\n+    sum.update(b, off, bytesToCopy);\n+    System.arraycopy(b, off, buf, count, bytesToCopy);\n+    count += bytesToCopy;\n+    if (count == buf.length) {\n+      // local buffer is full\n+      flushBuffer();\n+    } \n+    return bytesToCopy;\n+  }\n+\n+  /* Forces any buffered output bytes to be checksumed and written out to\n+   * the underlying output stream. \n+   */\n+  protected synchronized void flushBuffer() throws IOException {\n+    flushBuffer(false);\n+  }\n+\n+  /* Forces any buffered output bytes to be checksumed and written out to\n+   * the underlying output stream.  If keep is true, then the state of \n+   * this object remains intact.\n+   */\n+  protected synchronized void flushBuffer(boolean keep) throws IOException {\n+    if (count != 0) {\n+      int chunkLen = count;\n+      count = 0;\n+      writeChecksumChunk(buf, 0, chunkLen, keep);\n+      if (keep) {\n+        count = chunkLen;\n+      }\n+    }\n+  }\n+  \n+  /** Generate checksum for the data chunk and output data chunk & checksum\n+   * to the underlying output stream. If keep is true then keep the\n+   * current checksum intact, do not reset it.\n+   */\n+  private void writeChecksumChunk(byte b[], int off, int len, boolean keep)\n+  throws IOException {\n+    int tempChecksum = (int)sum.getValue();\n+    if (!keep) {\n+      sum.reset();\n+    }\n+    int2byte(tempChecksum, checksum);\n+    writeChunk(b, off, len, checksum);\n+  }\n+\n+  /**\n+   * Converts a checksum integer value to a byte stream\n+   */\n+  static public byte[] convertToByteStream(Checksum sum, int checksumSize) {\n+    return int2byte((int)sum.getValue(), new byte[checksumSize]);\n+  }\n+\n+  static byte[] int2byte(int integer, byte[] bytes) {\n+    bytes[0] = (byte)((integer >>> 24) & 0xFF);\n+    bytes[1] = (byte)((integer >>> 16) & 0xFF);\n+    bytes[2] = (byte)((integer >>>  8) & 0xFF);\n+    bytes[3] = (byte)((integer >>>  0) & 0xFF);\n+    return bytes;\n+  }\n+\n+  /**\n+   * Resets existing buffer with a new one of the specified size.\n+   */\n+  protected synchronized void resetChecksumChunk(int size) {\n+    sum.reset();\n+    this.buf = new byte[size];\n+    this.count = 0;\n+  }\n+}"
        },
        {
            "sha": "4fe66d0cd7000dc8adae9c3bc436ba4bc72efe87",
            "filename": "src/java/org/apache/hadoop/fs/FileChecksum.java",
            "status": "added",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFileChecksum.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFileChecksum.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFileChecksum.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,53 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.util.Arrays;\n+\n+import org.apache.hadoop.io.Writable;\n+\n+/** An abstract class representing file checksums for files. */\n+public abstract class FileChecksum implements Writable {\n+  /** The checksum algorithm name */ \n+  public abstract String getAlgorithmName();\n+\n+  /** The length of the checksum in bytes */ \n+  public abstract int getLength();\n+\n+  /** The value of the checksum in bytes */ \n+  public abstract byte[] getBytes();\n+\n+  /** Return true if both the algorithms and the values are the same. */\n+  public boolean equals(Object other) {\n+    if (other == this) {\n+      return true;\n+    }\n+    if (other == null || !(other instanceof FileChecksum)) {\n+      return false;\n+    }\n+\n+    final FileChecksum that = (FileChecksum)other;\n+    return this.getAlgorithmName().equals(that.getAlgorithmName())\n+      && Arrays.equals(this.getBytes(), that.getBytes());\n+  }\n+  \n+  /** {@inheritDoc} */\n+  public int hashCode() {\n+    return getAlgorithmName().hashCode() ^ Arrays.hashCode(getBytes());\n+  }\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "124984658cd5e2d353fd8bb8f7fc69dfb6dbdaed",
            "filename": "src/java/org/apache/hadoop/fs/FileStatus.java",
            "status": "added",
            "additions": 252,
            "deletions": 0,
            "changes": 252,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFileStatus.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFileStatus.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFileStatus.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,252 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+\n+/** Interface that represents the client side information for a file.\n+ */\n+public class FileStatus implements Writable, Comparable {\n+\n+  private Path path;\n+  private long length;\n+  private boolean isdir;\n+  private short block_replication;\n+  private long blocksize;\n+  private long modification_time;\n+  private long access_time;\n+  private FsPermission permission;\n+  private String owner;\n+  private String group;\n+  \n+  public FileStatus() { this(0, false, 0, 0, 0, 0, null, null, null, null); }\n+  \n+  //We should deprecate this soon?\n+  public FileStatus(long length, boolean isdir, int block_replication,\n+                    long blocksize, long modification_time, Path path) {\n+\n+    this(length, isdir, block_replication, blocksize, modification_time,\n+         0, null, null, null, path);\n+  }\n+  \n+  public FileStatus(long length, boolean isdir, int block_replication,\n+                    long blocksize, long modification_time, long access_time,\n+                    FsPermission permission, String owner, String group, \n+                    Path path) {\n+    this.length = length;\n+    this.isdir = isdir;\n+    this.block_replication = (short)block_replication;\n+    this.blocksize = blocksize;\n+    this.modification_time = modification_time;\n+    this.access_time = access_time;\n+    this.permission = (permission == null) ? \n+                      FsPermission.getDefault() : permission;\n+    this.owner = (owner == null) ? \"\" : owner;\n+    this.group = (group == null) ? \"\" : group;\n+    this.path = path;\n+  }\n+\n+  /* \n+   * @return the length of this file, in blocks\n+   */\n+  public long getLen() {\n+    return length;\n+  }\n+\n+  /**\n+   * Is this a directory?\n+   * @return true if this is a directory\n+   */\n+  public boolean isDir() {\n+    return isdir;\n+  }\n+\n+  /**\n+   * Get the block size of the file.\n+   * @return the number of bytes\n+   */\n+  public long getBlockSize() {\n+    return blocksize;\n+  }\n+\n+  /**\n+   * Get the replication factor of a file.\n+   * @return the replication factor of a file.\n+   */\n+  public short getReplication() {\n+    return block_replication;\n+  }\n+\n+  /**\n+   * Get the modification time of the file.\n+   * @return the modification time of file in milliseconds since January 1, 1970 UTC.\n+   */\n+  public long getModificationTime() {\n+    return modification_time;\n+  }\n+\n+  /**\n+   * Get the access time of the file.\n+   * @return the access time of file in milliseconds since January 1, 1970 UTC.\n+   */\n+  public long getAccessTime() {\n+    return access_time;\n+  }\n+\n+  /**\n+   * Get FsPermission associated with the file.\n+   * @return permssion. If a filesystem does not have a notion of permissions\n+   *         or if permissions could not be determined, then default \n+   *         permissions equivalent of \"rwxrwxrwx\" is returned.\n+   */\n+  public FsPermission getPermission() {\n+    return permission;\n+  }\n+  \n+  /**\n+   * Get the owner of the file.\n+   * @return owner of the file. The string could be empty if there is no\n+   *         notion of owner of a file in a filesystem or if it could not \n+   *         be determined (rare).\n+   */\n+  public String getOwner() {\n+    return owner;\n+  }\n+  \n+  /**\n+   * Get the group associated with the file.\n+   * @return group for the file. The string could be empty if there is no\n+   *         notion of group of a file in a filesystem or if it could not \n+   *         be determined (rare).\n+   */\n+  public String getGroup() {\n+    return group;\n+  }\n+  \n+  public Path getPath() {\n+    return path;\n+  }\n+\n+  /* These are provided so that these values could be loaded lazily \n+   * by a filesystem (e.g. local file system).\n+   */\n+  \n+  /**\n+   * Sets permission.\n+   * @param permission if permission is null, default value is set\n+   */\n+  protected void setPermission(FsPermission permission) {\n+    this.permission = (permission == null) ? \n+                      FsPermission.getDefault() : permission;\n+  }\n+  \n+  /**\n+   * Sets owner.\n+   * @param owner if it is null, default value is set\n+   */  \n+  protected void setOwner(String owner) {\n+    this.owner = (owner == null) ? \"\" : owner;\n+  }\n+  \n+  /**\n+   * Sets group.\n+   * @param group if it is null, default value is set\n+   */  \n+  protected void setGroup(String group) {\n+    this.group = (group == null) ? \"\" :  group;\n+  }\n+\n+  //////////////////////////////////////////////////\n+  // Writable\n+  //////////////////////////////////////////////////\n+  public void write(DataOutput out) throws IOException {\n+    Text.writeString(out, getPath().toString());\n+    out.writeLong(length);\n+    out.writeBoolean(isdir);\n+    out.writeShort(block_replication);\n+    out.writeLong(blocksize);\n+    out.writeLong(modification_time);\n+    out.writeLong(access_time);\n+    permission.write(out);\n+    Text.writeString(out, owner);\n+    Text.writeString(out, group);\n+  }\n+\n+  public void readFields(DataInput in) throws IOException {\n+    String strPath = Text.readString(in);\n+    this.path = new Path(strPath);\n+    this.length = in.readLong();\n+    this.isdir = in.readBoolean();\n+    this.block_replication = in.readShort();\n+    blocksize = in.readLong();\n+    modification_time = in.readLong();\n+    access_time = in.readLong();\n+    permission.readFields(in);\n+    owner = Text.readString(in);\n+    group = Text.readString(in);\n+  }\n+\n+  /**\n+   * Compare this object to another object\n+   * \n+   * @param   o the object to be compared.\n+   * @return  a negative integer, zero, or a positive integer as this object\n+   *   is less than, equal to, or greater than the specified object.\n+   * \n+   * @throws ClassCastException if the specified object's is not of \n+   *         type FileStatus\n+   */\n+  public int compareTo(Object o) {\n+    FileStatus other = (FileStatus)o;\n+    return this.getPath().compareTo(other.getPath());\n+  }\n+  \n+  /** Compare if this object is equal to another object\n+   * @param   o the object to be compared.\n+   * @return  true if two file status has the same path name; false if not.\n+   */\n+  public boolean equals(Object o) {\n+    if (o == null) {\n+      return false;\n+    }\n+    if (this == o) {\n+      return true;\n+    }\n+    if (!(o instanceof FileStatus)) {\n+      return false;\n+    }\n+    FileStatus other = (FileStatus)o;\n+    return this.getPath().equals(other.getPath());\n+  }\n+  \n+  /**\n+   * Returns a hash code value for the object, which is defined as\n+   * the hash code of the path name.\n+   *\n+   * @return  a hash code value for the path name.\n+   */\n+  public int hashCode() {\n+    return getPath().hashCode();\n+  }\n+}"
        },
        {
            "sha": "fcc5817d27ea25bc3b3e522702b409062969edc2",
            "filename": "src/java/org/apache/hadoop/fs/FileSystem.java",
            "status": "added",
            "additions": 1648,
            "deletions": 0,
            "changes": 1648,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFileSystem.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFileSystem.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFileSystem.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,1648 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.Closeable;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.IdentityHashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeSet;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.regex.Pattern;\n+\n+import javax.security.auth.login.LoginException;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.MultipleIOException;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+/****************************************************************\n+ * An abstract base class for a fairly generic filesystem.  It\n+ * may be implemented as a distributed filesystem, or as a \"local\"\n+ * one that reflects the locally-connected disk.  The local version\n+ * exists for small Hadoop instances and for testing.\n+ *\n+ * <p>\n+ *\n+ * All user code that may potentially use the Hadoop Distributed\n+ * File System should be written to use a FileSystem object.  The\n+ * Hadoop DFS is a multi-machine system that appears as a single\n+ * disk.  It's useful because of its fault tolerance and potentially\n+ * very large capacity.\n+ * \n+ * <p>\n+ * The local implementation is {@link LocalFileSystem} and distributed\n+ * implementation is DistributedFileSystem.\n+ *****************************************************************/\n+public abstract class FileSystem extends Configured implements Closeable {\n+  private static final String FS_DEFAULT_NAME_KEY = \"fs.default.name\";\n+\n+  public static final Log LOG = LogFactory.getLog(FileSystem.class);\n+\n+  /** FileSystem cache */\n+  private static final Cache CACHE = new Cache();\n+\n+  /** The key this instance is stored under in the cache. */\n+  private Cache.Key key;\n+\n+  /** Recording statistics per a FileSystem class */\n+  private static final Map<Class<? extends FileSystem>, Statistics> \n+    statisticsTable =\n+      new IdentityHashMap<Class<? extends FileSystem>, Statistics>();\n+  \n+  /**\n+   * The statistics for this file system.\n+   */\n+  protected Statistics statistics;\n+\n+  /**\n+   * A cache of files that should be deleted when filsystem is closed\n+   * or the JVM is exited.\n+   */\n+  private Set<Path> deleteOnExit = new TreeSet<Path>();\n+\n+  /** Returns the configured filesystem implementation.*/\n+  public static FileSystem get(Configuration conf) throws IOException {\n+    return get(getDefaultUri(conf), conf);\n+  }\n+  \n+  /** Get the default filesystem URI from a configuration.\n+   * @param conf the configuration to access\n+   * @return the uri of the default filesystem\n+   */\n+  public static URI getDefaultUri(Configuration conf) {\n+    return URI.create(fixName(conf.get(FS_DEFAULT_NAME_KEY, \"file:///\")));\n+  }\n+\n+  /** Set the default filesystem URI in a configuration.\n+   * @param conf the configuration to alter\n+   * @param uri the new default filesystem uri\n+   */\n+  public static void setDefaultUri(Configuration conf, URI uri) {\n+    conf.set(FS_DEFAULT_NAME_KEY, uri.toString());\n+  }\n+\n+  /** Set the default filesystem URI in a configuration.\n+   * @param conf the configuration to alter\n+   * @param uri the new default filesystem uri\n+   */\n+  public static void setDefaultUri(Configuration conf, String uri) {\n+    setDefaultUri(conf, URI.create(fixName(uri)));\n+  }\n+\n+  /** Called after a new FileSystem instance is constructed.\n+   * @param name a uri whose authority section names the host, port, etc.\n+   *   for this FileSystem\n+   * @param conf the configuration\n+   */\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    statistics = getStatistics(name.getScheme(), getClass());    \n+  }\n+\n+  /** Returns a URI whose scheme and authority identify this FileSystem.*/\n+  public abstract URI getUri();\n+  \n+  /** Update old-format filesystem names, for back-compatibility.  This should\n+   * eventually be replaced with a checkName() method that throws an exception\n+   * for old-format names. */ \n+  private static String fixName(String name) {\n+    // convert old-format name to new-format name\n+    if (name.equals(\"local\")) {         // \"local\" is now \"file:///\".\n+      LOG.warn(\"\\\"local\\\" is a deprecated filesystem name.\"\n+               +\" Use \\\"file:///\\\" instead.\");\n+      name = \"file:///\";\n+    } else if (name.indexOf('/')==-1) {   // unqualified is \"hdfs://\"\n+      LOG.warn(\"\\\"\"+name+\"\\\" is a deprecated filesystem name.\"\n+               +\" Use \\\"hdfs://\"+name+\"/\\\" instead.\");\n+      name = \"hdfs://\"+name;\n+    }\n+    return name;\n+  }\n+\n+  /**\n+   * Get the local file syste\n+   * @param conf the configuration to configure the file system with\n+   * @return a LocalFileSystem\n+   */\n+  public static LocalFileSystem getLocal(Configuration conf)\n+    throws IOException {\n+    return (LocalFileSystem)get(LocalFileSystem.NAME, conf);\n+  }\n+\n+  /** Returns the FileSystem for this URI's scheme and authority.  The scheme\n+   * of the URI determines a configuration property name,\n+   * <tt>fs.<i>scheme</i>.class</tt> whose value names the FileSystem class.\n+   * The entire URI is passed to the FileSystem instance's initialize method.\n+   */\n+  public static FileSystem get(URI uri, Configuration conf) throws IOException {\n+    String scheme = uri.getScheme();\n+    String authority = uri.getAuthority();\n+\n+    if (scheme == null) {                       // no scheme: use default FS\n+      return get(conf);\n+    }\n+\n+    if (authority == null) {                       // no authority\n+      URI defaultUri = getDefaultUri(conf);\n+      if (scheme.equals(defaultUri.getScheme())    // if scheme matches default\n+          && defaultUri.getAuthority() != null) {  // & default has authority\n+        return get(defaultUri, conf);              // return default\n+      }\n+    }\n+\n+    return CACHE.get(uri, conf);\n+  }\n+\n+  /** Returns the FileSystem for this URI's scheme and authority.  The scheme\n+   * of the URI determines a configuration property name,\n+   * <tt>fs.<i>scheme</i>.class</tt> whose value names the FileSystem class.\n+   * The entire URI is passed to the FileSystem instance's initialize method.\n+   * This always returns a new FileSystem object.\n+   */\n+  public static FileSystem newInstance(URI uri, Configuration conf) throws IOException {\n+    String scheme = uri.getScheme();\n+    String authority = uri.getAuthority();\n+\n+    if (scheme == null) {                       // no scheme: use default FS\n+      return newInstance(conf);\n+    }\n+\n+    if (authority == null) {                       // no authority\n+      URI defaultUri = getDefaultUri(conf);\n+      if (scheme.equals(defaultUri.getScheme())    // if scheme matches default\n+          && defaultUri.getAuthority() != null) {  // & default has authority\n+        return newInstance(defaultUri, conf);              // return default\n+      }\n+    }\n+    return CACHE.getUnique(uri, conf);\n+  }\n+\n+  /** Returns a unique configured filesystem implementation.\n+   * This always returns a new FileSystem object. */\n+  public static FileSystem newInstance(Configuration conf) throws IOException {\n+    return newInstance(getDefaultUri(conf), conf);\n+  }\n+\n+  /**\n+   * Get a unique local file system object\n+   * @param conf the configuration to configure the file system with\n+   * @return a LocalFileSystem\n+   * This always returns a new FileSystem object.\n+   */\n+  public static LocalFileSystem newInstanceLocal(Configuration conf)\n+    throws IOException {\n+    return (LocalFileSystem)newInstance(LocalFileSystem.NAME, conf);\n+  }\n+\n+  private static class ClientFinalizer extends Thread {\n+    public synchronized void run() {\n+      try {\n+        FileSystem.closeAll();\n+      } catch (IOException e) {\n+        LOG.info(\"FileSystem.closeAll() threw an exception:\\n\" + e);\n+      }\n+    }\n+  }\n+  private static final ClientFinalizer clientFinalizer = new ClientFinalizer();\n+\n+  /**\n+   * Close all cached filesystems. Be sure those filesystems are not\n+   * used anymore.\n+   * \n+   * @throws IOException\n+   */\n+  public static void closeAll() throws IOException {\n+    CACHE.closeAll();\n+  }\n+\n+  /** Make sure that a path specifies a FileSystem. */\n+  public Path makeQualified(Path path) {\n+    checkPath(path);\n+    return path.makeQualified(this);\n+  }\n+    \n+  /** create a file with the provided permission\n+   * The permission of the file is set to be the provided permission as in\n+   * setPermission, not permission&~umask\n+   * \n+   * It is implemented using two RPCs. It is understood that it is inefficient,\n+   * but the implementation is thread-safe. The other option is to change the\n+   * value of umask in configuration to be 0, but it is not thread-safe.\n+   * \n+   * @param fs file system handle\n+   * @param file the name of the file to be created\n+   * @param permission the permission of the file\n+   * @return an output stream\n+   * @throws IOException\n+   */\n+  public static FSDataOutputStream create(FileSystem fs,\n+      Path file, FsPermission permission) throws IOException {\n+    // create the file with default permission\n+    FSDataOutputStream out = fs.create(file);\n+    // set its permission to the supplied one\n+    fs.setPermission(file, permission);\n+    return out;\n+  }\n+\n+  /** create a directory with the provided permission\n+   * The permission of the directory is set to be the provided permission as in\n+   * setPermission, not permission&~umask\n+   * \n+   * @see #create(FileSystem, Path, FsPermission)\n+   * \n+   * @param fs file system handle\n+   * @param dir the name of the directory to be created\n+   * @param permission the permission of the directory\n+   * @return true if the directory creation succeeds; false otherwise\n+   * @throws IOException\n+   */\n+  public static boolean mkdirs(FileSystem fs, Path dir, FsPermission permission)\n+  throws IOException {\n+    // create the directory using the default permission\n+    boolean result = fs.mkdirs(dir);\n+    // set its permission to be the supplied one\n+    fs.setPermission(dir, permission);\n+    return result;\n+  }\n+\n+  ///////////////////////////////////////////////////////////////\n+  // FileSystem\n+  ///////////////////////////////////////////////////////////////\n+\n+  protected FileSystem() {\n+    super(null);\n+  }\n+\n+  /** Check that a Path belongs to this FileSystem. */\n+  protected void checkPath(Path path) {\n+    URI uri = path.toUri();\n+    if (uri.getScheme() == null)                // fs is relative \n+      return;\n+    String thisScheme = this.getUri().getScheme();\n+    String thatScheme = uri.getScheme();\n+    String thisAuthority = this.getUri().getAuthority();\n+    String thatAuthority = uri.getAuthority();\n+    //authority and scheme are not case sensitive\n+    if (thisScheme.equalsIgnoreCase(thatScheme)) {// schemes match\n+      if (thisAuthority == thatAuthority ||       // & authorities match\n+          (thisAuthority != null && \n+           thisAuthority.equalsIgnoreCase(thatAuthority)))\n+        return;\n+\n+      if (thatAuthority == null &&                // path's authority is null\n+          thisAuthority != null) {                // fs has an authority\n+        URI defaultUri = getDefaultUri(getConf()); // & is the conf default \n+        if (thisScheme.equalsIgnoreCase(defaultUri.getScheme()) &&\n+            thisAuthority.equalsIgnoreCase(defaultUri.getAuthority()))\n+          return;\n+        try {                                     // or the default fs's uri\n+          defaultUri = get(getConf()).getUri();\n+        } catch (IOException e) {\n+          throw new RuntimeException(e);\n+        }\n+        if (thisScheme.equalsIgnoreCase(defaultUri.getScheme()) &&\n+            thisAuthority.equalsIgnoreCase(defaultUri.getAuthority()))\n+          return;\n+      }\n+    }\n+    throw new IllegalArgumentException(\"Wrong FS: \"+path+\n+                                       \", expected: \"+this.getUri());\n+  }\n+\n+  /**\n+   * Return an array containing hostnames, offset and size of \n+   * portions of the given file.  For a nonexistent \n+   * file or regions, null will be returned.\n+   *\n+   * This call is most helpful with DFS, where it returns \n+   * hostnames of machines that contain the given file.\n+   *\n+   * The FileSystem will simply return an elt containing 'localhost'.\n+   */\n+  public BlockLocation[] getFileBlockLocations(FileStatus file, \n+      long start, long len) throws IOException {\n+    if (file == null) {\n+      return null;\n+    }\n+\n+    if ( (start<0) || (len < 0) ) {\n+      throw new IllegalArgumentException(\"Invalid start or len parameter\");\n+    }\n+\n+    if (file.getLen() < start) {\n+      return new BlockLocation[0];\n+\n+    }\n+    String[] name = { \"localhost:50010\" };\n+    String[] host = { \"localhost\" };\n+    return new BlockLocation[] { new BlockLocation(name, host, 0, file.getLen()) };\n+  }\n+  \n+  /**\n+   * Opens an FSDataInputStream at the indicated Path.\n+   * @param f the file name to open\n+   * @param bufferSize the size of the buffer to be used.\n+   */\n+  public abstract FSDataInputStream open(Path f, int bufferSize)\n+    throws IOException;\n+    \n+  /**\n+   * Opens an FSDataInputStream at the indicated Path.\n+   * @param f the file to open\n+   */\n+  public FSDataInputStream open(Path f) throws IOException {\n+    return open(f, getConf().getInt(\"io.file.buffer.size\", 4096));\n+  }\n+\n+  /**\n+   * Opens an FSDataOutputStream at the indicated Path.\n+   * Files are overwritten by default.\n+   */\n+  public FSDataOutputStream create(Path f) throws IOException {\n+    return create(f, true);\n+  }\n+\n+  /**\n+   * Opens an FSDataOutputStream at the indicated Path.\n+   */\n+  public FSDataOutputStream create(Path f, boolean overwrite)\n+    throws IOException {\n+    return create(f, overwrite, \n+                  getConf().getInt(\"io.file.buffer.size\", 4096),\n+                  getDefaultReplication(),\n+                  getDefaultBlockSize());\n+  }\n+\n+  /**\n+   * Create an FSDataOutputStream at the indicated Path with write-progress\n+   * reporting.\n+   * Files are overwritten by default.\n+   */\n+  public FSDataOutputStream create(Path f, Progressable progress) throws IOException {\n+    return create(f, true, \n+                  getConf().getInt(\"io.file.buffer.size\", 4096),\n+                  getDefaultReplication(),\n+                  getDefaultBlockSize(), progress);\n+  }\n+\n+  /**\n+   * Opens an FSDataOutputStream at the indicated Path.\n+   * Files are overwritten by default.\n+   */\n+  public FSDataOutputStream create(Path f, short replication)\n+    throws IOException {\n+    return create(f, true, \n+                  getConf().getInt(\"io.file.buffer.size\", 4096),\n+                  replication,\n+                  getDefaultBlockSize());\n+  }\n+\n+  /**\n+   * Opens an FSDataOutputStream at the indicated Path with write-progress\n+   * reporting.\n+   * Files are overwritten by default.\n+   */\n+  public FSDataOutputStream create(Path f, short replication, Progressable progress)\n+    throws IOException {\n+    return create(f, true, \n+                  getConf().getInt(\"io.file.buffer.size\", 4096),\n+                  replication,\n+                  getDefaultBlockSize(), progress);\n+  }\n+\n+    \n+  /**\n+   * Opens an FSDataOutputStream at the indicated Path.\n+   * @param f the file name to open\n+   * @param overwrite if a file with this name already exists, then if true,\n+   *   the file will be overwritten, and if false an error will be thrown.\n+   * @param bufferSize the size of the buffer to be used.\n+   */\n+  public FSDataOutputStream create(Path f, \n+                                   boolean overwrite,\n+                                   int bufferSize\n+                                   ) throws IOException {\n+    return create(f, overwrite, bufferSize, \n+                  getDefaultReplication(),\n+                  getDefaultBlockSize());\n+  }\n+    \n+  /**\n+   * Opens an FSDataOutputStream at the indicated Path with write-progress\n+   * reporting.\n+   * @param f the file name to open\n+   * @param overwrite if a file with this name already exists, then if true,\n+   *   the file will be overwritten, and if false an error will be thrown.\n+   * @param bufferSize the size of the buffer to be used.\n+   */\n+  public FSDataOutputStream create(Path f, \n+                                   boolean overwrite,\n+                                   int bufferSize,\n+                                   Progressable progress\n+                                   ) throws IOException {\n+    return create(f, overwrite, bufferSize, \n+                  getDefaultReplication(),\n+                  getDefaultBlockSize(), progress);\n+  }\n+    \n+    \n+  /**\n+   * Opens an FSDataOutputStream at the indicated Path.\n+   * @param f the file name to open\n+   * @param overwrite if a file with this name already exists, then if true,\n+   *   the file will be overwritten, and if false an error will be thrown.\n+   * @param bufferSize the size of the buffer to be used.\n+   * @param replication required block replication for the file. \n+   */\n+  public FSDataOutputStream create(Path f, \n+                                   boolean overwrite,\n+                                   int bufferSize,\n+                                   short replication,\n+                                   long blockSize\n+                                   ) throws IOException {\n+    return create(f, overwrite, bufferSize, replication, blockSize, null);\n+  }\n+\n+  /**\n+   * Opens an FSDataOutputStream at the indicated Path with write-progress\n+   * reporting.\n+   * @param f the file name to open\n+   * @param overwrite if a file with this name already exists, then if true,\n+   *   the file will be overwritten, and if false an error will be thrown.\n+   * @param bufferSize the size of the buffer to be used.\n+   * @param replication required block replication for the file. \n+   */\n+  public FSDataOutputStream create(Path f,\n+                                            boolean overwrite,\n+                                            int bufferSize,\n+                                            short replication,\n+                                            long blockSize,\n+                                            Progressable progress\n+                                            ) throws IOException {\n+    return this.create(f, FsPermission.getDefault(),\n+        overwrite, bufferSize, replication, blockSize, progress);\n+  }\n+\n+  /**\n+   * Opens an FSDataOutputStream at the indicated Path with write-progress\n+   * reporting.\n+   * @param f the file name to open\n+   * @param permission\n+   * @param overwrite if a file with this name already exists, then if true,\n+   *   the file will be overwritten, and if false an error will be thrown.\n+   * @param bufferSize the size of the buffer to be used.\n+   * @param replication required block replication for the file.\n+   * @param blockSize\n+   * @param progress\n+   * @throws IOException\n+   * @see #setPermission(Path, FsPermission)\n+   */\n+  public abstract FSDataOutputStream create(Path f,\n+      FsPermission permission,\n+      boolean overwrite,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException;\n+\n+  /**\n+   * Creates the given Path as a brand-new zero-length file.  If\n+   * create fails, or if it already existed, return false.\n+   */\n+  public boolean createNewFile(Path f) throws IOException {\n+    if (exists(f)) {\n+      return false;\n+    } else {\n+      create(f, false, getConf().getInt(\"io.file.buffer.size\", 4096)).close();\n+      return true;\n+    }\n+  }\n+\n+  /**\n+   * Append to an existing file (optional operation).\n+   * Same as append(f, getConf().getInt(\"io.file.buffer.size\", 4096), null)\n+   * @param f the existing file to be appended.\n+   * @throws IOException\n+   */\n+  public FSDataOutputStream append(Path f) throws IOException {\n+    return append(f, getConf().getInt(\"io.file.buffer.size\", 4096), null);\n+  }\n+  /**\n+   * Append to an existing file (optional operation).\n+   * Same as append(f, bufferSize, null).\n+   * @param f the existing file to be appended.\n+   * @param bufferSize the size of the buffer to be used.\n+   * @throws IOException\n+   */\n+  public FSDataOutputStream append(Path f, int bufferSize) throws IOException {\n+    return append(f, bufferSize, null);\n+  }\n+\n+  /**\n+   * Append to an existing file (optional operation).\n+   * @param f the existing file to be appended.\n+   * @param bufferSize the size of the buffer to be used.\n+   * @param progress for reporting progress if it is not null.\n+   * @throws IOException\n+   */\n+  public abstract FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException;\n+\n+  /**\n+   * Set replication for an existing file.\n+   * \n+   * @param src file name\n+   * @param replication new replication\n+   * @throws IOException\n+   * @return true if successful;\n+   *         false if file does not exist or is a directory\n+   */\n+  public boolean setReplication(Path src, short replication)\n+    throws IOException {\n+    return true;\n+  }\n+\n+  /**\n+   * Renames Path src to Path dst.  Can take place on local fs\n+   * or remote DFS.\n+   */\n+  public abstract boolean rename(Path src, Path dst) throws IOException;\n+    \n+  /** Delete a file.\n+   *\n+   * @param f the path to delete.\n+   * @param recursive if path is a directory and set to \n+   * true, the directory is deleted else throws an exception. In\n+   * case of a file the recursive can be set to either true or false. \n+   * @return  true if delete is successful else false. \n+   * @throws IOException\n+   */\n+  public abstract boolean delete(Path f, boolean recursive) throws IOException;\n+\n+  /**\n+   * Mark a path to be deleted when FileSystem is closed.\n+   * When the JVM shuts down,\n+   * all FileSystem objects will be closed automatically.\n+   * Then,\n+   * the marked path will be deleted as a result of closing the FileSystem.\n+   *\n+   * The path has to exist in the file system.\n+   * \n+   * @param f the path to delete.\n+   * @return  true if deleteOnExit is successful, otherwise false.\n+   * @throws IOException\n+   */\n+  public boolean deleteOnExit(Path f) throws IOException {\n+    if (!exists(f)) {\n+      return false;\n+    }\n+    synchronized (deleteOnExit) {\n+      deleteOnExit.add(f);\n+    }\n+    return true;\n+  }\n+\n+  /**\n+   * Delete all files that were marked as delete-on-exit. This recursively\n+   * deletes all files in the specified paths.\n+   */\n+  protected void processDeleteOnExit() {\n+    synchronized (deleteOnExit) {\n+      for (Iterator<Path> iter = deleteOnExit.iterator(); iter.hasNext();) {\n+        Path path = iter.next();\n+        try {\n+          delete(path, true);\n+        }\n+        catch (IOException e) {\n+          LOG.info(\"Ignoring failure to deleteOnExit for path \" + path);\n+        }\n+        iter.remove();\n+      }\n+    }\n+  }\n+  \n+  /** Check if exists.\n+   * @param f source file\n+   */\n+  public boolean exists(Path f) throws IOException {\n+    try {\n+      return getFileStatus(f) != null;\n+    } catch (FileNotFoundException e) {\n+      return false;\n+    }\n+  }\n+\n+  /** True iff the named path is a directory.\n+   * Note: Avoid using this method. Instead reuse the FileStatus \n+   * returned by getFileStatus() or listStatus() methods.\n+   */\n+  public boolean isDirectory(Path f) throws IOException {\n+    try {\n+      return getFileStatus(f).isDir();\n+    } catch (FileNotFoundException e) {\n+      return false;               // f does not exist\n+    }\n+  }\n+\n+  /** True iff the named path is a regular file.\n+   * Note: Avoid using this method. Instead reuse the FileStatus \n+   * returned by getFileStatus() or listStatus() methods.\n+   */\n+  public boolean isFile(Path f) throws IOException {\n+    try {\n+      return !getFileStatus(f).isDir();\n+    } catch (FileNotFoundException e) {\n+      return false;               // f does not exist\n+    }\n+  }\n+    \n+  /** Return the {@link ContentSummary} of a given {@link Path}. */\n+  public ContentSummary getContentSummary(Path f) throws IOException {\n+    FileStatus status = getFileStatus(f);\n+    if (!status.isDir()) {\n+      // f is a file\n+      return new ContentSummary(status.getLen(), 1, 0);\n+    }\n+    // f is a directory\n+    long[] summary = {0, 0, 1};\n+    for(FileStatus s : listStatus(f)) {\n+      ContentSummary c = s.isDir() ? getContentSummary(s.getPath()) :\n+                                     new ContentSummary(s.getLen(), 1, 0);\n+      summary[0] += c.getLength();\n+      summary[1] += c.getFileCount();\n+      summary[2] += c.getDirectoryCount();\n+    }\n+    return new ContentSummary(summary[0], summary[1], summary[2]);\n+  }\n+\n+  final private static PathFilter DEFAULT_FILTER = new PathFilter() {\n+      public boolean accept(Path file) {\n+        return true;\n+      }     \n+    };\n+    \n+  /**\n+   * List the statuses of the files/directories in the given path if the path is\n+   * a directory.\n+   * \n+   * @param f\n+   *          given path\n+   * @return the statuses of the files/directories in the given patch\n+   * @throws IOException\n+   */\n+  public abstract FileStatus[] listStatus(Path f) throws IOException;\n+    \n+  /*\n+   * Filter files/directories in the given path using the user-supplied path\n+   * filter. Results are added to the given array <code>results</code>.\n+   */\n+  private void listStatus(ArrayList<FileStatus> results, Path f,\n+      PathFilter filter) throws IOException {\n+    FileStatus listing[] = listStatus(f);\n+    if (listing != null) {\n+      for (int i = 0; i < listing.length; i++) {\n+        if (filter.accept(listing[i].getPath())) {\n+          results.add(listing[i]);\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Filter files/directories in the given path using the user-supplied path\n+   * filter.\n+   * \n+   * @param f\n+   *          a path name\n+   * @param filter\n+   *          the user-supplied path filter\n+   * @return an array of FileStatus objects for the files under the given path\n+   *         after applying the filter\n+   * @throws IOException\n+   *           if encounter any problem while fetching the status\n+   */\n+  public FileStatus[] listStatus(Path f, PathFilter filter) throws IOException {\n+    ArrayList<FileStatus> results = new ArrayList<FileStatus>();\n+    listStatus(results, f, filter);\n+    return results.toArray(new FileStatus[results.size()]);\n+  }\n+\n+  /**\n+   * Filter files/directories in the given list of paths using default\n+   * path filter.\n+   * \n+   * @param files\n+   *          a list of paths\n+   * @return a list of statuses for the files under the given paths after\n+   *         applying the filter default Path filter\n+   * @exception IOException\n+   */\n+  public FileStatus[] listStatus(Path[] files)\n+      throws IOException {\n+    return listStatus(files, DEFAULT_FILTER);\n+  }\n+\n+  /**\n+   * Filter files/directories in the given list of paths using user-supplied\n+   * path filter.\n+   * \n+   * @param files\n+   *          a list of paths\n+   * @param filter\n+   *          the user-supplied path filter\n+   * @return a list of statuses for the files under the given paths after\n+   *         applying the filter\n+   * @exception IOException\n+   */\n+  public FileStatus[] listStatus(Path[] files, PathFilter filter)\n+      throws IOException {\n+    ArrayList<FileStatus> results = new ArrayList<FileStatus>();\n+    for (int i = 0; i < files.length; i++) {\n+      listStatus(results, files[i], filter);\n+    }\n+    return results.toArray(new FileStatus[results.size()]);\n+  }\n+\n+  /**\n+   * <p>Return all the files that match filePattern and are not checksum\n+   * files. Results are sorted by their names.\n+   * \n+   * <p>\n+   * A filename pattern is composed of <i>regular</i> characters and\n+   * <i>special pattern matching</i> characters, which are:\n+   *\n+   * <dl>\n+   *  <dd>\n+   *   <dl>\n+   *    <p>\n+   *    <dt> <tt> ? </tt>\n+   *    <dd> Matches any single character.\n+   *\n+   *    <p>\n+   *    <dt> <tt> * </tt>\n+   *    <dd> Matches zero or more characters.\n+   *\n+   *    <p>\n+   *    <dt> <tt> [<i>abc</i>] </tt>\n+   *    <dd> Matches a single character from character set\n+   *     <tt>{<i>a,b,c</i>}</tt>.\n+   *\n+   *    <p>\n+   *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>\n+   *    <dd> Matches a single character from the character range\n+   *     <tt>{<i>a...b</i>}</tt>.  Note that character <tt><i>a</i></tt> must be\n+   *     lexicographically less than or equal to character <tt><i>b</i></tt>.\n+   *\n+   *    <p>\n+   *    <dt> <tt> [^<i>a</i>] </tt>\n+   *    <dd> Matches a single character that is not from character set or range\n+   *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur\n+   *     immediately to the right of the opening bracket.\n+   *\n+   *    <p>\n+   *    <dt> <tt> \\<i>c</i> </tt>\n+   *    <dd> Removes (escapes) any special meaning of character <i>c</i>.\n+   *\n+   *    <p>\n+   *    <dt> <tt> {ab,cd} </tt>\n+   *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>\n+   *    \n+   *    <p>\n+   *    <dt> <tt> {ab,c{de,fh}} </tt>\n+   *    <dd> Matches a string from the string set <tt>{<i>ab, cde, cfh</i>}</tt>\n+   *\n+   *   </dl>\n+   *  </dd>\n+   * </dl>\n+   *\n+   * @param pathPattern a regular expression specifying a pth pattern\n+\n+   * @return an array of paths that match the path pattern\n+   * @throws IOException\n+   */\n+  public FileStatus[] globStatus(Path pathPattern) throws IOException {\n+    return globStatus(pathPattern, DEFAULT_FILTER);\n+  }\n+  \n+  /**\n+   * Return an array of FileStatus objects whose path names match pathPattern\n+   * and is accepted by the user-supplied path filter. Results are sorted by\n+   * their path names.\n+   * Return null if pathPattern has no glob and the path does not exist.\n+   * Return an empty array if pathPattern has a glob and no path matches it. \n+   * \n+   * @param pathPattern\n+   *          a regular expression specifying the path pattern\n+   * @param filter\n+   *          a user-supplied path filter\n+   * @return an array of FileStatus objects\n+   * @throws IOException if any I/O error occurs when fetching file status\n+   */\n+  public FileStatus[] globStatus(Path pathPattern, PathFilter filter)\n+      throws IOException {\n+    String filename = pathPattern.toUri().getPath();\n+    List<String> filePatterns = GlobExpander.expand(filename);\n+    if (filePatterns.size() == 1) {\n+      return globStatusInternal(pathPattern, filter);\n+    } else {\n+      List<FileStatus> results = new ArrayList<FileStatus>();\n+      for (String filePattern : filePatterns) {\n+        FileStatus[] files = globStatusInternal(new Path(filePattern), filter);\n+        for (FileStatus file : files) {\n+          results.add(file);\n+        }\n+      }\n+      return results.toArray(new FileStatus[results.size()]);\n+    }\n+  }\n+\n+  private FileStatus[] globStatusInternal(Path pathPattern, PathFilter filter)\n+      throws IOException {\n+    Path[] parents = new Path[1];\n+    int level = 0;\n+    String filename = pathPattern.toUri().getPath();\n+    \n+    // path has only zero component\n+    if (\"\".equals(filename) || Path.SEPARATOR.equals(filename)) {\n+      return getFileStatus(new Path[]{pathPattern});\n+    }\n+\n+    // path has at least one component\n+    String[] components = filename.split(Path.SEPARATOR);\n+    // get the first component\n+    if (pathPattern.isAbsolute()) {\n+      parents[0] = new Path(Path.SEPARATOR);\n+      level = 1;\n+    } else {\n+      parents[0] = new Path(Path.CUR_DIR);\n+    }\n+\n+    // glob the paths that match the parent path, i.e., [0, components.length-1]\n+    boolean[] hasGlob = new boolean[]{false};\n+    Path[] parentPaths = globPathsLevel(parents, components, level, hasGlob);\n+    FileStatus[] results;\n+    if (parentPaths == null || parentPaths.length == 0) {\n+      results = null;\n+    } else {\n+      // Now work on the last component of the path\n+      GlobFilter fp = new GlobFilter(components[components.length - 1], filter);\n+      if (fp.hasPattern()) { // last component has a pattern\n+        // list parent directories and then glob the results\n+        results = listStatus(parentPaths, fp);\n+        hasGlob[0] = true;\n+      } else { // last component does not have a pattern\n+        // get all the path names\n+        ArrayList<Path> filteredPaths = new ArrayList<Path>(parentPaths.length);\n+        for (int i = 0; i < parentPaths.length; i++) {\n+          parentPaths[i] = new Path(parentPaths[i],\n+            components[components.length - 1]);\n+          if (fp.accept(parentPaths[i])) {\n+            filteredPaths.add(parentPaths[i]);\n+          }\n+        }\n+        // get all their statuses\n+        results = getFileStatus(\n+            filteredPaths.toArray(new Path[filteredPaths.size()]));\n+      }\n+    }\n+\n+    // Decide if the pathPattern contains a glob or not\n+    if (results == null) {\n+      if (hasGlob[0]) {\n+        results = new FileStatus[0];\n+      }\n+    } else {\n+      if (results.length == 0 ) {\n+        if (!hasGlob[0]) {\n+          results = null;\n+        }\n+      } else {\n+        Arrays.sort(results);\n+      }\n+    }\n+    return results;\n+  }\n+\n+  /*\n+   * For a path of N components, return a list of paths that match the\n+   * components [<code>level</code>, <code>N-1</code>].\n+   */\n+  private Path[] globPathsLevel(Path[] parents, String[] filePattern,\n+      int level, boolean[] hasGlob) throws IOException {\n+    if (level == filePattern.length - 1)\n+      return parents;\n+    if (parents == null || parents.length == 0) {\n+      return null;\n+    }\n+    GlobFilter fp = new GlobFilter(filePattern[level]);\n+    if (fp.hasPattern()) {\n+      parents = FileUtil.stat2Paths(listStatus(parents, fp));\n+      hasGlob[0] = true;\n+    } else {\n+      for (int i = 0; i < parents.length; i++) {\n+        parents[i] = new Path(parents[i], filePattern[level]);\n+      }\n+    }\n+    return globPathsLevel(parents, filePattern, level + 1, hasGlob);\n+  }\n+\n+  /* A class that could decide if a string matches the glob or not */\n+  private static class GlobFilter implements PathFilter {\n+    private PathFilter userFilter = DEFAULT_FILTER;\n+    private Pattern regex;\n+    private boolean hasPattern = false;\n+      \n+    /** Default pattern character: Escape any special meaning. */\n+    private static final char  PAT_ESCAPE = '\\\\';\n+    /** Default pattern character: Any single character. */\n+    private static final char  PAT_ANY = '.';\n+    /** Default pattern character: Character set close. */\n+    private static final char  PAT_SET_CLOSE = ']';\n+      \n+    GlobFilter() {\n+    }\n+      \n+    GlobFilter(String filePattern) throws IOException {\n+      setRegex(filePattern);\n+    }\n+      \n+    GlobFilter(String filePattern, PathFilter filter) throws IOException {\n+      userFilter = filter;\n+      setRegex(filePattern);\n+    }\n+      \n+    private boolean isJavaRegexSpecialChar(char pChar) {\n+      return pChar == '.' || pChar == '$' || pChar == '(' || pChar == ')' ||\n+             pChar == '|' || pChar == '+';\n+    }\n+    void setRegex(String filePattern) throws IOException {\n+      int len;\n+      int setOpen;\n+      int curlyOpen;\n+      boolean setRange;\n+\n+      StringBuilder fileRegex = new StringBuilder();\n+\n+      // Validate the pattern\n+      len = filePattern.length();\n+      if (len == 0)\n+        return;\n+\n+      setOpen = 0;\n+      setRange = false;\n+      curlyOpen = 0;\n+\n+      for (int i = 0; i < len; i++) {\n+        char pCh;\n+          \n+        // Examine a single pattern character\n+        pCh = filePattern.charAt(i);\n+        if (pCh == PAT_ESCAPE) {\n+          fileRegex.append(pCh);\n+          i++;\n+          if (i >= len)\n+            error(\"An escaped character does not present\", filePattern, i);\n+          pCh = filePattern.charAt(i);\n+        } else if (isJavaRegexSpecialChar(pCh)) {\n+          fileRegex.append(PAT_ESCAPE);\n+        } else if (pCh == '*') {\n+          fileRegex.append(PAT_ANY);\n+          hasPattern = true;\n+        } else if (pCh == '?') {\n+          pCh = PAT_ANY;\n+          hasPattern = true;\n+        } else if (pCh == '{') {\n+          fileRegex.append('(');\n+          pCh = '(';\n+          curlyOpen++;\n+          hasPattern = true;\n+        } else if (pCh == ',' && curlyOpen > 0) {\n+          fileRegex.append(\")|\");\n+          pCh = '(';\n+        } else if (pCh == '}' && curlyOpen > 0) {\n+          // End of a group\n+          curlyOpen--;\n+          fileRegex.append(\")\");\n+          pCh = ')';\n+        } else if (pCh == '[' && setOpen == 0) {\n+          setOpen++;\n+          hasPattern = true;\n+        } else if (pCh == '^' && setOpen > 0) {\n+        } else if (pCh == '-' && setOpen > 0) {\n+          // Character set range\n+          setRange = true;\n+        } else if (pCh == PAT_SET_CLOSE && setRange) {\n+          // Incomplete character set range\n+          error(\"Incomplete character set range\", filePattern, i);\n+        } else if (pCh == PAT_SET_CLOSE && setOpen > 0) {\n+          // End of a character set\n+          if (setOpen < 2)\n+            error(\"Unexpected end of set\", filePattern, i);\n+          setOpen = 0;\n+        } else if (setOpen > 0) {\n+          // Normal character, or the end of a character set range\n+          setOpen++;\n+          setRange = false;\n+        }\n+        fileRegex.append(pCh);\n+      }\n+        \n+      // Check for a well-formed pattern\n+      if (setOpen > 0 || setRange || curlyOpen > 0) {\n+        // Incomplete character set or character range\n+        error(\"Expecting set closure character or end of range, or }\", \n+            filePattern, len);\n+      }\n+      regex = Pattern.compile(fileRegex.toString());\n+    }\n+      \n+    boolean hasPattern() {\n+      return hasPattern;\n+    }\n+      \n+    public boolean accept(Path path) {\n+      return regex.matcher(path.getName()).matches() && userFilter.accept(path);\n+    }\n+      \n+    private void error(String s, String pattern, int pos) throws IOException {\n+      throw new IOException(\"Illegal file pattern: \"\n+                            +s+ \" for glob \"+ pattern + \" at \" + pos);\n+    }\n+  }\n+    \n+  /** Return the current user's home directory in this filesystem.\n+   * The default implementation returns \"/user/$USER/\".\n+   */\n+  public Path getHomeDirectory() {\n+    return new Path(\"/user/\"+System.getProperty(\"user.name\"))\n+      .makeQualified(this);\n+  }\n+\n+\n+  /**\n+   * Set the current working directory for the given file system. All relative\n+   * paths will be resolved relative to it.\n+   * \n+   * @param new_dir\n+   */\n+  public abstract void setWorkingDirectory(Path new_dir);\n+    \n+  /**\n+   * Get the current working directory for the given file system\n+   * @return the directory pathname\n+   */\n+  public abstract Path getWorkingDirectory();\n+\n+  /**\n+   * Call {@link #mkdirs(Path, FsPermission)} with default permission.\n+   */\n+  public boolean mkdirs(Path f) throws IOException {\n+    return mkdirs(f, FsPermission.getDefault());\n+  }\n+\n+  /**\n+   * Make the given file and all non-existent parents into\n+   * directories. Has the semantics of Unix 'mkdir -p'.\n+   * Existence of the directory hierarchy is not an error.\n+   */\n+  public abstract boolean mkdirs(Path f, FsPermission permission\n+      ) throws IOException;\n+\n+  /**\n+   * The src file is on the local disk.  Add it to FS at\n+   * the given dst name and the source is kept intact afterwards\n+   */\n+  public void copyFromLocalFile(Path src, Path dst)\n+    throws IOException {\n+    copyFromLocalFile(false, src, dst);\n+  }\n+\n+  /**\n+   * The src files is on the local disk.  Add it to FS at\n+   * the given dst name, removing the source afterwards.\n+   */\n+  public void moveFromLocalFile(Path[] srcs, Path dst)\n+    throws IOException {\n+    copyFromLocalFile(true, true, srcs, dst);\n+  }\n+\n+  /**\n+   * The src file is on the local disk.  Add it to FS at\n+   * the given dst name, removing the source afterwards.\n+   */\n+  public void moveFromLocalFile(Path src, Path dst)\n+    throws IOException {\n+    copyFromLocalFile(true, src, dst);\n+  }\n+\n+  /**\n+   * The src file is on the local disk.  Add it to FS at\n+   * the given dst name.\n+   * delSrc indicates if the source should be removed\n+   */\n+  public void copyFromLocalFile(boolean delSrc, Path src, Path dst)\n+    throws IOException {\n+    copyFromLocalFile(delSrc, true, src, dst);\n+  }\n+  \n+  /**\n+   * The src files are on the local disk.  Add it to FS at\n+   * the given dst name.\n+   * delSrc indicates if the source should be removed\n+   */\n+  public void copyFromLocalFile(boolean delSrc, boolean overwrite, \n+                                Path[] srcs, Path dst)\n+    throws IOException {\n+    Configuration conf = getConf();\n+    FileUtil.copy(getLocal(conf), srcs, this, dst, delSrc, overwrite, conf);\n+  }\n+  \n+  /**\n+   * The src file is on the local disk.  Add it to FS at\n+   * the given dst name.\n+   * delSrc indicates if the source should be removed\n+   */\n+  public void copyFromLocalFile(boolean delSrc, boolean overwrite, \n+                                Path src, Path dst)\n+    throws IOException {\n+    Configuration conf = getConf();\n+    FileUtil.copy(getLocal(conf), src, this, dst, delSrc, overwrite, conf);\n+  }\n+    \n+  /**\n+   * The src file is under FS, and the dst is on the local disk.\n+   * Copy it from FS control to the local dst name.\n+   */\n+  public void copyToLocalFile(Path src, Path dst) throws IOException {\n+    copyToLocalFile(false, src, dst);\n+  }\n+    \n+  /**\n+   * The src file is under FS, and the dst is on the local disk.\n+   * Copy it from FS control to the local dst name.\n+   * Remove the source afterwards\n+   */\n+  public void moveToLocalFile(Path src, Path dst) throws IOException {\n+    copyToLocalFile(true, src, dst);\n+  }\n+\n+  /**\n+   * The src file is under FS, and the dst is on the local disk.\n+   * Copy it from FS control to the local dst name.\n+   * delSrc indicates if the src will be removed or not.\n+   */   \n+  public void copyToLocalFile(boolean delSrc, Path src, Path dst)\n+    throws IOException {\n+    FileUtil.copy(this, src, getLocal(getConf()), dst, delSrc, getConf());\n+  }\n+\n+  /**\n+   * Returns a local File that the user can write output to.  The caller\n+   * provides both the eventual FS target name and the local working\n+   * file.  If the FS is local, we write directly into the target.  If\n+   * the FS is remote, we write into the tmp local area.\n+   */\n+  public Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile)\n+    throws IOException {\n+    return tmpLocalFile;\n+  }\n+\n+  /**\n+   * Called when we're all done writing to the target.  A local FS will\n+   * do nothing, because we've written to exactly the right place.  A remote\n+   * FS will copy the contents of tmpLocalFile to the correct target at\n+   * fsOutputFile.\n+   */\n+  public void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile)\n+    throws IOException {\n+    moveFromLocalFile(tmpLocalFile, fsOutputFile);\n+  }\n+\n+  /**\n+   * No more filesystem operations are needed.  Will\n+   * release any held locks.\n+   */\n+  public void close() throws IOException {\n+    // delete all files that were marked as delete-on-exit.\n+    processDeleteOnExit();\n+    CACHE.remove(this.key, this);\n+  }\n+\n+  /** Return the total size of all files in the filesystem.*/\n+  public long getUsed() throws IOException{\n+    long used = 0;\n+    FileStatus[] files = listStatus(new Path(\"/\"));\n+    for(FileStatus file:files){\n+      used += file.getLen();\n+    }\n+    return used;\n+  }\n+\n+  /** Return the number of bytes that large input files should be optimally\n+   * be split into to minimize i/o time. */\n+  public long getDefaultBlockSize() {\n+    // default to 32MB: large enough to minimize the impact of seeks\n+    return getConf().getLong(\"fs.local.block.size\", 32 * 1024 * 1024);\n+  }\n+    \n+  /**\n+   * Get the default replication.\n+   */\n+  public short getDefaultReplication() { return 1; }\n+\n+  /**\n+   * Return a file status object that represents the path.\n+   * @param f The path we want information from\n+   * @return a FileStatus object\n+   * @throws FileNotFoundException when the path does not exist;\n+   *         IOException see specific implementation\n+   */\n+  public abstract FileStatus getFileStatus(Path f) throws IOException;\n+\n+  /**\n+   * Get the checksum of a file.\n+   *\n+   * @param f The file path\n+   * @return The file checksum.  The default return value is null,\n+   *  which indicates that no checksum algorithm is implemented\n+   *  in the corresponding FileSystem.\n+   */\n+  public FileChecksum getFileChecksum(Path f) throws IOException {\n+    return null;\n+  }\n+  \n+  /**\n+   * Set the verify checksum flag. This is only applicable if the \n+   * corresponding FileSystem supports checksum. By default doesn't do anything.\n+   * @param verifyChecksum\n+   */\n+  public void setVerifyChecksum(boolean verifyChecksum) {\n+    //doesn't do anything\n+  }\n+\n+  /**\n+   * Return a list of file status objects that corresponds to the list of paths\n+   * excluding those non-existent paths.\n+   * \n+   * @param paths\n+   *          the list of paths we want information from\n+   * @return a list of FileStatus objects\n+   * @throws IOException\n+   *           see specific implementation\n+   */\n+  private FileStatus[] getFileStatus(Path[] paths) throws IOException {\n+    if (paths == null) {\n+      return null;\n+    }\n+    ArrayList<FileStatus> results = new ArrayList<FileStatus>(paths.length);\n+    for (int i = 0; i < paths.length; i++) {\n+      try {\n+        results.add(getFileStatus(paths[i]));\n+      } catch (FileNotFoundException e) { // do nothing\n+      }\n+    }\n+    return results.toArray(new FileStatus[results.size()]);\n+  }\n+  \n+  /**\n+   * Returns a status object describing the use and capacity of the\n+   * file system. If the file system has multiple partitions, the\n+   * use and capacity of the root partition is reflected.\n+   * \n+   * @return a FsStatus object\n+   * @throws IOException\n+   *           see specific implementation\n+   */\n+  public FsStatus getStatus() throws IOException {\n+    return getStatus(null);\n+  }\n+\n+  /**\n+   * Returns a status object describing the use and capacity of the\n+   * file system. If the file system has multiple partitions, the\n+   * use and capacity of the partition pointed to by the specified\n+   * path is reflected.\n+   * @param p Path for which status should be obtained. null means\n+   * the default partition. \n+   * @return a FsStatus object\n+   * @throws IOException\n+   *           see specific implementation\n+   */\n+  public FsStatus getStatus(Path p) throws IOException {\n+    return new FsStatus(Long.MAX_VALUE, 0, Long.MAX_VALUE);\n+  }\n+\n+  /**\n+   * Set permission of a path.\n+   * @param p\n+   * @param permission\n+   */\n+  public void setPermission(Path p, FsPermission permission\n+      ) throws IOException {\n+  }\n+\n+  /**\n+   * Set owner of a path (i.e. a file or a directory).\n+   * The parameters username and groupname cannot both be null.\n+   * @param p The path\n+   * @param username If it is null, the original username remains unchanged.\n+   * @param groupname If it is null, the original groupname remains unchanged.\n+   */\n+  public void setOwner(Path p, String username, String groupname\n+      ) throws IOException {\n+  }\n+\n+  /**\n+   * Set access time of a file\n+   * @param p The path\n+   * @param mtime Set the modification time of this file.\n+   *              The number of milliseconds since Jan 1, 1970. \n+   *              A value of -1 means that this call should not set modification time.\n+   * @param atime Set the access time of this file.\n+   *              The number of milliseconds since Jan 1, 1970. \n+   *              A value of -1 means that this call should not set access time.\n+   */\n+  public void setTimes(Path p, long mtime, long atime\n+      ) throws IOException {\n+  }\n+\n+  private static FileSystem createFileSystem(URI uri, Configuration conf\n+      ) throws IOException {\n+    Class<?> clazz = conf.getClass(\"fs.\" + uri.getScheme() + \".impl\", null);\n+    if (clazz == null) {\n+      throw new IOException(\"No FileSystem for scheme: \" + uri.getScheme());\n+    }\n+    FileSystem fs = (FileSystem)ReflectionUtils.newInstance(clazz, conf);\n+    fs.initialize(uri, conf);\n+    return fs;\n+  }\n+\n+  /** Caching FileSystem objects */\n+  static class Cache {\n+    private final Map<Key, FileSystem> map = new HashMap<Key, FileSystem>();\n+\n+    /** A variable that makes all objects in the cache unique */\n+    private static AtomicLong unique = new AtomicLong(1);\n+\n+    synchronized FileSystem get(URI uri, Configuration conf) throws IOException{\n+      Key key = new Key(uri, conf);\n+      return getInternal(uri, conf, key);\n+    }\n+\n+    /** The objects inserted into the cache using this method are all unique */\n+    synchronized FileSystem getUnique(URI uri, Configuration conf) throws IOException{\n+      Key key = new Key(uri, conf, unique.getAndIncrement());\n+      return getInternal(uri, conf, key);\n+    }\n+\n+    private FileSystem getInternal(URI uri, Configuration conf, Key key) throws IOException{\n+      FileSystem fs = map.get(key);\n+      if (fs == null) {\n+        fs = createFileSystem(uri, conf);\n+        if (map.isEmpty() && !clientFinalizer.isAlive()) {\n+          Runtime.getRuntime().addShutdownHook(clientFinalizer);\n+        }\n+        fs.key = key;\n+        map.put(key, fs);\n+      }\n+      return fs;\n+    }\n+\n+    synchronized void remove(Key key, FileSystem fs) {\n+      if (map.containsKey(key) && fs == map.get(key)) {\n+        map.remove(key);\n+        if (map.isEmpty() && !clientFinalizer.isAlive()) {\n+          if (!Runtime.getRuntime().removeShutdownHook(clientFinalizer)) {\n+            LOG.info(\"Could not cancel cleanup thread, though no \" +\n+                     \"FileSystems are open\");\n+          }\n+        }\n+      }\n+    }\n+\n+    synchronized void closeAll() throws IOException {\n+      List<IOException> exceptions = new ArrayList<IOException>();\n+      for(; !map.isEmpty(); ) {\n+        Map.Entry<Key, FileSystem> e = map.entrySet().iterator().next();\n+        final Key key = e.getKey();\n+        final FileSystem fs = e.getValue();\n+\n+        //remove from cache\n+        remove(key, fs);\n+\n+        if (fs != null) {\n+          try {\n+            fs.close();\n+          }\n+          catch(IOException ioe) {\n+            exceptions.add(ioe);\n+          }\n+        }\n+      }\n+\n+      if (!exceptions.isEmpty()) {\n+        throw MultipleIOException.createIOException(exceptions);\n+      }\n+    }\n+\n+    /** FileSystem.Cache.Key */\n+    static class Key {\n+      final String scheme;\n+      final String authority;\n+      final String username;\n+      final long unique;   // an artificial way to make a key unique\n+\n+      Key(URI uri, Configuration conf) throws IOException {\n+        this(uri, conf, 0);\n+      }\n+\n+      Key(URI uri, Configuration conf, long unique) throws IOException {\n+        scheme = uri.getScheme()==null?\"\":uri.getScheme().toLowerCase();\n+        authority = uri.getAuthority()==null?\"\":uri.getAuthority().toLowerCase();\n+        this.unique = unique;\n+        UserGroupInformation ugi = UserGroupInformation.readFrom(conf);\n+        if (ugi == null) {\n+          try {\n+            ugi = UserGroupInformation.login(conf);\n+          } catch(LoginException e) {\n+            LOG.warn(\"uri=\" + uri, e);\n+          }\n+        }\n+        username = ugi == null? null: ugi.getUserName();\n+      }\n+\n+      /** {@inheritDoc} */\n+      public int hashCode() {\n+        return (scheme + authority + username).hashCode() + (int)unique;\n+      }\n+\n+      static boolean isEqual(Object a, Object b) {\n+        return a == b || (a != null && a.equals(b));        \n+      }\n+\n+      /** {@inheritDoc} */\n+      public boolean equals(Object obj) {\n+        if (obj == this) {\n+          return true;\n+        }\n+        if (obj != null && obj instanceof Key) {\n+          Key that = (Key)obj;\n+          return isEqual(this.scheme, that.scheme)\n+                 && isEqual(this.authority, that.authority)\n+                 && isEqual(this.username, that.username)\n+                 && (this.unique == that.unique);\n+        }\n+        return false;        \n+      }\n+\n+      /** {@inheritDoc} */\n+      public String toString() {\n+        return username + \"@\" + scheme + \"://\" + authority;        \n+      }\n+    }\n+  }\n+  \n+  public static final class Statistics {\n+    private final String scheme;\n+    private AtomicLong bytesRead = new AtomicLong();\n+    private AtomicLong bytesWritten = new AtomicLong();\n+    \n+    public Statistics(String scheme) {\n+      this.scheme = scheme;\n+    }\n+\n+    /**\n+     * Increment the bytes read in the statistics\n+     * @param newBytes the additional bytes read\n+     */\n+    public void incrementBytesRead(long newBytes) {\n+      bytesRead.getAndAdd(newBytes);\n+    }\n+    \n+    /**\n+     * Increment the bytes written in the statistics\n+     * @param newBytes the additional bytes written\n+     */\n+    public void incrementBytesWritten(long newBytes) {\n+      bytesWritten.getAndAdd(newBytes);\n+    }\n+    \n+    /**\n+     * Get the total number of bytes read\n+     * @return the number of bytes\n+     */\n+    public long getBytesRead() {\n+      return bytesRead.get();\n+    }\n+    \n+    /**\n+     * Get the total number of bytes written\n+     * @return the number of bytes\n+     */\n+    public long getBytesWritten() {\n+      return bytesWritten.get();\n+    }\n+    \n+    public String toString() {\n+      return bytesRead + \" bytes read and \" + bytesWritten + \n+             \" bytes written\";\n+    }\n+    \n+    /**\n+     * Reset the counts of bytes to 0.\n+     */\n+    public void reset() {\n+      bytesWritten.set(0);\n+      bytesRead.set(0);\n+    }\n+    \n+    /**\n+     * Get the uri scheme associated with this statistics object.\n+     * @return the schema associated with this set of statistics\n+     */\n+    public String getScheme() {\n+      return scheme;\n+    }\n+  }\n+  \n+  /**\n+   * Get the Map of Statistics object indexed by URI Scheme.\n+   * @return a Map having a key as URI scheme and value as Statistics object\n+   * @deprecated use {@link #getAllStatistics} instead\n+   */\n+  @Deprecated\n+  public static synchronized Map<String, Statistics> getStatistics() {\n+    Map<String, Statistics> result = new HashMap<String, Statistics>();\n+    for(Statistics stat: statisticsTable.values()) {\n+      result.put(stat.getScheme(), stat);\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Return the FileSystem classes that have Statistics\n+   */\n+  public static synchronized List<Statistics> getAllStatistics() {\n+    return new ArrayList<Statistics>(statisticsTable.values());\n+  }\n+  \n+  /**\n+   * Get the statistics for a particular file system\n+   * @param cls the class to lookup\n+   * @return a statistics object\n+   */\n+  public static synchronized \n+  Statistics getStatistics(String scheme, Class<? extends FileSystem> cls) {\n+    Statistics result = statisticsTable.get(cls);\n+    if (result == null) {\n+      result = new Statistics(scheme);\n+      statisticsTable.put(cls, result);\n+    }\n+    return result;\n+  }\n+  \n+  public static synchronized void clearStatistics() {\n+    for(Statistics stat: statisticsTable.values()) {\n+      stat.reset();\n+    }\n+  }\n+\n+  public static synchronized\n+  void printStatistics() throws IOException {\n+    for (Map.Entry<Class<? extends FileSystem>, Statistics> pair: \n+            statisticsTable.entrySet()) {\n+      System.out.println(\"  FileSystem \" + pair.getKey().getName() + \n+                         \": \" + pair.getValue());\n+    }\n+  }\n+}"
        },
        {
            "sha": "d1b1d0b89f838d739f74cdd8637b25d2b4dbe056",
            "filename": "src/java/org/apache/hadoop/fs/FileUtil.java",
            "status": "added",
            "additions": 794,
            "deletions": 0,
            "changes": 794,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFileUtil.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFileUtil.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFileUtil.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,794 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs;\n+\n+import java.io.*;\n+import java.util.Enumeration;\n+import java.util.zip.ZipEntry;\n+import java.util.zip.ZipFile;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.util.StringUtils;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.Shell.ShellCommandExecutor;\n+import org.mortbay.log.Log;\n+\n+/**\n+ * A collection of file-processing util methods\n+ */\n+public class FileUtil {\n+  /**\n+   * convert an array of FileStatus to an array of Path\n+   * \n+   * @param stats\n+   *          an array of FileStatus objects\n+   * @return an array of paths corresponding to the input\n+   */\n+  public static Path[] stat2Paths(FileStatus[] stats) {\n+    if (stats == null)\n+      return null;\n+    Path[] ret = new Path[stats.length];\n+    for (int i = 0; i < stats.length; ++i) {\n+      ret[i] = stats[i].getPath();\n+    }\n+    return ret;\n+  }\n+\n+  /**\n+   * convert an array of FileStatus to an array of Path.\n+   * If stats if null, return path\n+   * @param stats\n+   *          an array of FileStatus objects\n+   * @param path\n+   *          default path to return in stats is null\n+   * @return an array of paths corresponding to the input\n+   */\n+  public static Path[] stat2Paths(FileStatus[] stats, Path path) {\n+    if (stats == null)\n+      return new Path[]{path};\n+    else\n+      return stat2Paths(stats);\n+  }\n+  \n+  /**\n+   * Delete a directory and all its contents.  If\n+   * we return false, the directory may be partially-deleted.\n+   */\n+  public static boolean fullyDelete(File dir) throws IOException {\n+    File contents[] = dir.listFiles();\n+    if (contents != null) {\n+      for (int i = 0; i < contents.length; i++) {\n+        if (contents[i].isFile()) {\n+          if (!contents[i].delete()) {\n+            return false;\n+          }\n+        } else {\n+          //try deleting the directory\n+          // this might be a symlink\n+          boolean b = false;\n+          b = contents[i].delete();\n+          if (b){\n+            //this was indeed a symlink or an empty directory\n+            continue;\n+          }\n+          // if not an empty directory or symlink let\n+          // fullydelete handle it.\n+          if (!fullyDelete(contents[i])) {\n+            return false;\n+          }\n+        }\n+      }\n+    }\n+    return dir.delete();\n+  }\n+\n+  /**\n+   * Recursively delete a directory.\n+   * \n+   * @param fs {@link FileSystem} on which the path is present\n+   * @param dir directory to recursively delete \n+   * @throws IOException\n+   * @deprecated Use {@link FileSystem#delete(Path, boolean)}\n+   */\n+  @Deprecated\n+  public static void fullyDelete(FileSystem fs, Path dir) \n+  throws IOException {\n+    fs.delete(dir, true);\n+  }\n+\n+  //\n+  // If the destination is a subdirectory of the source, then\n+  // generate exception\n+  //\n+  private static void checkDependencies(FileSystem srcFS, \n+                                        Path src, \n+                                        FileSystem dstFS, \n+                                        Path dst)\n+                                        throws IOException {\n+    if (srcFS == dstFS) {\n+      String srcq = src.makeQualified(srcFS).toString() + Path.SEPARATOR;\n+      String dstq = dst.makeQualified(dstFS).toString() + Path.SEPARATOR;\n+      if (dstq.startsWith(srcq)) {\n+        if (srcq.length() == dstq.length()) {\n+          throw new IOException(\"Cannot copy \" + src + \" to itself.\");\n+        } else {\n+          throw new IOException(\"Cannot copy \" + src + \" to its subdirectory \" +\n+                                dst);\n+        }\n+      }\n+    }\n+  }\n+\n+  /** Copy files between FileSystems. */\n+  public static boolean copy(FileSystem srcFS, Path src, \n+                             FileSystem dstFS, Path dst, \n+                             boolean deleteSource,\n+                             Configuration conf) throws IOException {\n+    return copy(srcFS, src, dstFS, dst, deleteSource, true, conf);\n+  }\n+\n+  public static boolean copy(FileSystem srcFS, Path[] srcs, \n+                             FileSystem dstFS, Path dst,\n+                             boolean deleteSource, \n+                             boolean overwrite, Configuration conf)\n+                             throws IOException {\n+    boolean gotException = false;\n+    boolean returnVal = true;\n+    StringBuffer exceptions = new StringBuffer();\n+\n+    if (srcs.length == 1)\n+      return copy(srcFS, srcs[0], dstFS, dst, deleteSource, overwrite, conf);\n+\n+    // Check if dest is directory\n+    if (!dstFS.exists(dst)) {\n+      throw new IOException(\"`\" + dst +\"': specified destination directory \" +\n+                            \"doest not exist\");\n+    } else {\n+      FileStatus sdst = dstFS.getFileStatus(dst);\n+      if (!sdst.isDir()) \n+        throw new IOException(\"copying multiple files, but last argument `\" +\n+                              dst + \"' is not a directory\");\n+    }\n+\n+    for (Path src : srcs) {\n+      try {\n+        if (!copy(srcFS, src, dstFS, dst, deleteSource, overwrite, conf))\n+          returnVal = false;\n+      } catch (IOException e) {\n+        gotException = true;\n+        exceptions.append(e.getMessage());\n+        exceptions.append(\"\\n\");\n+      }\n+    }\n+    if (gotException) {\n+      throw new IOException(exceptions.toString());\n+    }\n+    return returnVal;\n+  }\n+\n+  /** Copy files between FileSystems. */\n+  public static boolean copy(FileSystem srcFS, Path src, \n+                             FileSystem dstFS, Path dst, \n+                             boolean deleteSource,\n+                             boolean overwrite,\n+                             Configuration conf) throws IOException {\n+    FileStatus fileStatus = srcFS.getFileStatus(src);\n+    return copy(srcFS, fileStatus, dstFS, dst, deleteSource, overwrite, conf);\n+  }\n+\n+  /** Copy files between FileSystems. */\n+  private static boolean copy(FileSystem srcFS, FileStatus srcStatus,\n+                              FileSystem dstFS, Path dst,\n+                              boolean deleteSource,\n+                              boolean overwrite,\n+                              Configuration conf) throws IOException {\n+    Path src = srcStatus.getPath();\n+    dst = checkDest(src.getName(), dstFS, dst, overwrite);\n+    if (srcStatus.isDir()) {\n+      checkDependencies(srcFS, src, dstFS, dst);\n+      if (!dstFS.mkdirs(dst)) {\n+        return false;\n+      }\n+      FileStatus contents[] = srcFS.listStatus(src);\n+      for (int i = 0; i < contents.length; i++) {\n+        copy(srcFS, contents[i], dstFS,\n+             new Path(dst, contents[i].getPath().getName()),\n+             deleteSource, overwrite, conf);\n+      }\n+    } else {\n+      InputStream in=null;\n+      OutputStream out = null;\n+      try {\n+        in = srcFS.open(src);\n+        out = dstFS.create(dst, overwrite);\n+        IOUtils.copyBytes(in, out, conf, true);\n+      } catch (IOException e) {\n+        IOUtils.closeStream(out);\n+        IOUtils.closeStream(in);\n+        throw e;\n+      }\n+    }\n+    if (deleteSource) {\n+      return srcFS.delete(src, true);\n+    } else {\n+      return true;\n+    }\n+  \n+  }\n+\n+  /** Copy all files in a directory to one output file (merge). */\n+  public static boolean copyMerge(FileSystem srcFS, Path srcDir, \n+                                  FileSystem dstFS, Path dstFile, \n+                                  boolean deleteSource,\n+                                  Configuration conf, String addString) throws IOException {\n+    dstFile = checkDest(srcDir.getName(), dstFS, dstFile, false);\n+\n+    if (!srcFS.getFileStatus(srcDir).isDir())\n+      return false;\n+   \n+    OutputStream out = dstFS.create(dstFile);\n+    \n+    try {\n+      FileStatus contents[] = srcFS.listStatus(srcDir);\n+      for (int i = 0; i < contents.length; i++) {\n+        if (!contents[i].isDir()) {\n+          InputStream in = srcFS.open(contents[i].getPath());\n+          try {\n+            IOUtils.copyBytes(in, out, conf, false);\n+            if (addString!=null)\n+              out.write(addString.getBytes(\"UTF-8\"));\n+                \n+          } finally {\n+            in.close();\n+          } \n+        }\n+      }\n+    } finally {\n+      out.close();\n+    }\n+    \n+\n+    if (deleteSource) {\n+      return srcFS.delete(srcDir, true);\n+    } else {\n+      return true;\n+    }\n+  }  \n+  \n+  /** Copy local files to a FileSystem. */\n+  public static boolean copy(File src,\n+                             FileSystem dstFS, Path dst,\n+                             boolean deleteSource,\n+                             Configuration conf) throws IOException {\n+    dst = checkDest(src.getName(), dstFS, dst, false);\n+\n+    if (src.isDirectory()) {\n+      if (!dstFS.mkdirs(dst)) {\n+        return false;\n+      }\n+      File contents[] = src.listFiles();\n+      for (int i = 0; i < contents.length; i++) {\n+        copy(contents[i], dstFS, new Path(dst, contents[i].getName()),\n+             deleteSource, conf);\n+      }\n+    } else if (src.isFile()) {\n+      InputStream in = null;\n+      OutputStream out =null;\n+      try {\n+        in = new FileInputStream(src);\n+        out = dstFS.create(dst);\n+        IOUtils.copyBytes(in, out, conf);\n+      } catch (IOException e) {\n+        IOUtils.closeStream( out );\n+        IOUtils.closeStream( in );\n+        throw e;\n+      }\n+    } else {\n+      throw new IOException(src.toString() + \n+                            \": No such file or directory\");\n+    }\n+    if (deleteSource) {\n+      return FileUtil.fullyDelete(src);\n+    } else {\n+      return true;\n+    }\n+  }\n+\n+  /** Copy FileSystem files to local files. */\n+  public static boolean copy(FileSystem srcFS, Path src, \n+                             File dst, boolean deleteSource,\n+                             Configuration conf) throws IOException {\n+    FileStatus filestatus = srcFS.getFileStatus(src);\n+    return copy(srcFS, filestatus, dst, deleteSource, conf);\n+  }\n+\n+  /** Copy FileSystem files to local files. */\n+  private static boolean copy(FileSystem srcFS, FileStatus srcStatus,\n+                              File dst, boolean deleteSource,\n+                              Configuration conf) throws IOException {\n+    Path src = srcStatus.getPath();\n+    if (srcStatus.isDir()) {\n+      if (!dst.mkdirs()) {\n+        return false;\n+      }\n+      FileStatus contents[] = srcFS.listStatus(src);\n+      for (int i = 0; i < contents.length; i++) {\n+        copy(srcFS, contents[i],\n+             new File(dst, contents[i].getPath().getName()),\n+             deleteSource, conf);\n+      }\n+    } else {\n+      InputStream in = srcFS.open(src);\n+      IOUtils.copyBytes(in, new FileOutputStream(dst), conf);\n+    }\n+    if (deleteSource) {\n+      return srcFS.delete(src, true);\n+    } else {\n+      return true;\n+    }\n+  }\n+\n+  private static Path checkDest(String srcName, FileSystem dstFS, Path dst,\n+      boolean overwrite) throws IOException {\n+    if (dstFS.exists(dst)) {\n+      FileStatus sdst = dstFS.getFileStatus(dst);\n+      if (sdst.isDir()) {\n+        if (null == srcName) {\n+          throw new IOException(\"Target \" + dst + \" is a directory\");\n+        }\n+        return checkDest(null, dstFS, new Path(dst, srcName), overwrite);\n+      } else if (!overwrite) {\n+        throw new IOException(\"Target \" + dst + \" already exists\");\n+      }\n+    }\n+    return dst;\n+  }\n+\n+  /**\n+   * This class is only used on windows to invoke the cygpath command.\n+   */\n+  private static class CygPathCommand extends Shell {\n+    String[] command;\n+    String result;\n+    CygPathCommand(String path) throws IOException {\n+      command = new String[]{\"cygpath\", \"-u\", path};\n+      run();\n+    }\n+    String getResult() throws IOException {\n+      return result;\n+    }\n+    protected String[] getExecString() {\n+      return command;\n+    }\n+    protected void parseExecResult(BufferedReader lines) throws IOException {\n+      String line = lines.readLine();\n+      if (line == null) {\n+        throw new IOException(\"Can't convert '\" + command[2] + \n+                              \" to a cygwin path\");\n+      }\n+      result = line;\n+    }\n+  }\n+\n+  /**\n+   * Convert a os-native filename to a path that works for the shell.\n+   * @param filename The filename to convert\n+   * @return The unix pathname\n+   * @throws IOException on windows, there can be problems with the subprocess\n+   */\n+  public static String makeShellPath(String filename) throws IOException {\n+    if (Path.WINDOWS) {\n+      return new CygPathCommand(filename).getResult();\n+    } else {\n+      return filename;\n+    }    \n+  }\n+  \n+  /**\n+   * Convert a os-native filename to a path that works for the shell.\n+   * @param file The filename to convert\n+   * @return The unix pathname\n+   * @throws IOException on windows, there can be problems with the subprocess\n+   */\n+  public static String makeShellPath(File file) throws IOException {\n+    return makeShellPath(file, false);\n+  }\n+\n+  /**\n+   * Convert a os-native filename to a path that works for the shell.\n+   * @param file The filename to convert\n+   * @param makeCanonicalPath \n+   *          Whether to make canonical path for the file passed\n+   * @return The unix pathname\n+   * @throws IOException on windows, there can be problems with the subprocess\n+   */\n+  public static String makeShellPath(File file, boolean makeCanonicalPath) \n+  throws IOException {\n+    if (makeCanonicalPath) {\n+      return makeShellPath(file.getCanonicalPath());\n+    } else {\n+      return makeShellPath(file.toString());\n+    }\n+  }\n+\n+  /**\n+   * Takes an input dir and returns the du on that local directory. Very basic\n+   * implementation.\n+   * \n+   * @param dir\n+   *          The input dir to get the disk space of this local dir\n+   * @return The total disk space of the input local directory\n+   */\n+  public static long getDU(File dir) {\n+    long size = 0;\n+    if (!dir.exists())\n+      return 0;\n+    if (!dir.isDirectory()) {\n+      return dir.length();\n+    } else {\n+      size = dir.length();\n+      File[] allFiles = dir.listFiles();\n+      for (int i = 0; i < allFiles.length; i++) {\n+        size = size + getDU(allFiles[i]);\n+      }\n+      return size;\n+    }\n+  }\n+    \n+  /**\n+   * Given a File input it will unzip the file in a the unzip directory\n+   * passed as the second parameter\n+   * @param inFile The zip file as input\n+   * @param unzipDir The unzip directory where to unzip the zip file.\n+   * @throws IOException\n+   */\n+  public static void unZip(File inFile, File unzipDir) throws IOException {\n+    Enumeration<? extends ZipEntry> entries;\n+    ZipFile zipFile = new ZipFile(inFile);\n+\n+    try {\n+      entries = zipFile.entries();\n+      while (entries.hasMoreElements()) {\n+        ZipEntry entry = entries.nextElement();\n+        if (!entry.isDirectory()) {\n+          InputStream in = zipFile.getInputStream(entry);\n+          try {\n+            File file = new File(unzipDir, entry.getName());\n+            if (!file.getParentFile().mkdirs()) {           \n+              if (!file.getParentFile().isDirectory()) {\n+                throw new IOException(\"Mkdirs failed to create \" + \n+                                      file.getParentFile().toString());\n+              }\n+            }\n+            OutputStream out = new FileOutputStream(file);\n+            try {\n+              byte[] buffer = new byte[8192];\n+              int i;\n+              while ((i = in.read(buffer)) != -1) {\n+                out.write(buffer, 0, i);\n+              }\n+            } finally {\n+              out.close();\n+            }\n+          } finally {\n+            in.close();\n+          }\n+        }\n+      }\n+    } finally {\n+      zipFile.close();\n+    }\n+  }\n+\n+  /**\n+   * Given a Tar File as input it will untar the file in a the untar directory\n+   * passed as the second parameter\n+   * \n+   * This utility will untar \".tar\" files and \".tar.gz\",\"tgz\" files.\n+   *  \n+   * @param inFile The tar file as input. \n+   * @param untarDir The untar directory where to untar the tar file.\n+   * @throws IOException\n+   */\n+  public static void unTar(File inFile, File untarDir) throws IOException {\n+    if (!untarDir.mkdirs()) {           \n+      if (!untarDir.isDirectory()) {\n+        throw new IOException(\"Mkdirs failed to create \" + untarDir);\n+      }\n+    }\n+\n+    StringBuffer untarCommand = new StringBuffer();\n+    boolean gzipped = inFile.toString().endsWith(\"gz\");\n+    if (gzipped) {\n+      untarCommand.append(\" gzip -dc '\");\n+      untarCommand.append(FileUtil.makeShellPath(inFile));\n+      untarCommand.append(\"' | (\");\n+    } \n+    untarCommand.append(\"cd '\");\n+    untarCommand.append(FileUtil.makeShellPath(untarDir)); \n+    untarCommand.append(\"' ; \");\n+    untarCommand.append(\"tar -xf \");\n+    \n+    if (gzipped) {\n+      untarCommand.append(\" -)\");\n+    } else {\n+      untarCommand.append(FileUtil.makeShellPath(inFile));\n+    }\n+    String[] shellCmd = { \"bash\", \"-c\", untarCommand.toString() };\n+    ShellCommandExecutor shexec = new ShellCommandExecutor(shellCmd);\n+    shexec.execute();\n+    int exitcode = shexec.getExitCode();\n+    if (exitcode != 0) {\n+      throw new IOException(\"Error untarring file \" + inFile + \n+                  \". Tar process exited with exit code \" + exitcode);\n+    }\n+  }\n+\n+  /**\n+   * Class for creating hardlinks.\n+   * Supports Unix, Cygwin, WindXP.\n+   *  \n+   */\n+  public static class HardLink { \n+    enum OSType {\n+      OS_TYPE_UNIX, \n+      OS_TYPE_WINXP,\n+      OS_TYPE_SOLARIS,\n+      OS_TYPE_MAC; \n+    }\n+  \n+    private static String[] hardLinkCommand;\n+    private static String[] getLinkCountCommand;\n+    private static OSType osType;\n+    \n+    static {\n+      osType = getOSType();\n+      switch(osType) {\n+      case OS_TYPE_WINXP:\n+        hardLinkCommand = new String[] {\"fsutil\",\"hardlink\",\"create\", null, null};\n+        getLinkCountCommand = new String[] {\"stat\",\"-c%h\"};\n+        break;\n+      case OS_TYPE_SOLARIS:\n+        hardLinkCommand = new String[] {\"ln\", null, null};\n+        getLinkCountCommand = new String[] {\"ls\",\"-l\"};\n+        break;\n+      case OS_TYPE_MAC:\n+        hardLinkCommand = new String[] {\"ln\", null, null};\n+        getLinkCountCommand = new String[] {\"stat\",\"-f%l\"};\n+        break;\n+      case OS_TYPE_UNIX:\n+      default:\n+        hardLinkCommand = new String[] {\"ln\", null, null};\n+        getLinkCountCommand = new String[] {\"stat\",\"-c%h\"};\n+      }\n+    }\n+\n+    static private OSType getOSType() {\n+      String osName = System.getProperty(\"os.name\");\n+      if (osName.indexOf(\"Windows\") >= 0 && \n+          (osName.indexOf(\"XP\") >= 0 || osName.indexOf(\"2003\") >= 0 || osName.indexOf(\"Vista\") >= 0))\n+        return OSType.OS_TYPE_WINXP;\n+      else if (osName.indexOf(\"SunOS\") >= 0)\n+         return OSType.OS_TYPE_SOLARIS;\n+      else if (osName.indexOf(\"Mac\") >= 0)\n+         return OSType.OS_TYPE_MAC;\n+      else\n+        return OSType.OS_TYPE_UNIX;\n+    }\n+    \n+    /**\n+     * Creates a hardlink \n+     */\n+    public static void createHardLink(File target, \n+                                      File linkName) throws IOException {\n+      int len = hardLinkCommand.length;\n+      if (osType == OSType.OS_TYPE_WINXP) {\n+       hardLinkCommand[len-1] = target.getCanonicalPath();\n+       hardLinkCommand[len-2] = linkName.getCanonicalPath();\n+      } else {\n+       hardLinkCommand[len-2] = makeShellPath(target, true);\n+       hardLinkCommand[len-1] = makeShellPath(linkName, true);\n+      }\n+      // execute shell command\n+      Process process = Runtime.getRuntime().exec(hardLinkCommand);\n+      try {\n+        if (process.waitFor() != 0) {\n+          String errMsg = new BufferedReader(new InputStreamReader(\n+                                                                   process.getInputStream())).readLine();\n+          if (errMsg == null)  errMsg = \"\";\n+          String inpMsg = new BufferedReader(new InputStreamReader(\n+                                                                   process.getErrorStream())).readLine();\n+          if (inpMsg == null)  inpMsg = \"\";\n+          throw new IOException(errMsg + inpMsg);\n+        }\n+      } catch (InterruptedException e) {\n+        throw new IOException(StringUtils.stringifyException(e));\n+      } finally {\n+        process.destroy();\n+      }\n+    }\n+\n+    /**\n+     * Retrieves the number of links to the specified file.\n+     */\n+    public static int getLinkCount(File fileName) throws IOException {\n+      int len = getLinkCountCommand.length;\n+      String[] cmd = new String[len + 1];\n+      for (int i = 0; i < len; i++) {\n+        cmd[i] = getLinkCountCommand[i];\n+      }\n+      cmd[len] = fileName.toString();\n+      String inpMsg = \"\";\n+      String errMsg = \"\";\n+      int exitValue = -1;\n+      BufferedReader in = null;\n+      BufferedReader err = null;\n+\n+      // execute shell command\n+      Process process = Runtime.getRuntime().exec(cmd);\n+      try {\n+        exitValue = process.waitFor();\n+        in = new BufferedReader(new InputStreamReader(\n+                                    process.getInputStream()));\n+        inpMsg = in.readLine();\n+        if (inpMsg == null)  inpMsg = \"\";\n+        \n+        err = new BufferedReader(new InputStreamReader(\n+                                     process.getErrorStream()));\n+        errMsg = err.readLine();\n+        if (errMsg == null)  errMsg = \"\";\n+        if (exitValue != 0) {\n+          throw new IOException(inpMsg + errMsg);\n+        }\n+        if (getOSType() == OSType.OS_TYPE_SOLARIS) {\n+          String[] result = inpMsg.split(\"\\\\s+\");\n+          return Integer.parseInt(result[1]);\n+        } else {\n+          return Integer.parseInt(inpMsg);\n+        }\n+      } catch (NumberFormatException e) {\n+        throw new IOException(StringUtils.stringifyException(e) + \n+                              inpMsg + errMsg +\n+                              \" on file:\" + fileName);\n+      } catch (InterruptedException e) {\n+        throw new IOException(StringUtils.stringifyException(e) + \n+                              inpMsg + errMsg +\n+                              \" on file:\" + fileName);\n+      } finally {\n+        process.destroy();\n+        if (in != null) in.close();\n+        if (err != null) err.close();\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Create a soft link between a src and destination\n+   * only on a local disk. HDFS does not support this\n+   * @param target the target for symlink \n+   * @param linkname the symlink\n+   * @return value returned by the command\n+   */\n+  public static int symLink(String target, String linkname) throws IOException{\n+    String cmd = \"ln -s \" + target + \" \" + linkname;\n+    Process p = Runtime.getRuntime().exec(cmd, null);\n+    int returnVal = -1;\n+    try{\n+      returnVal = p.waitFor();\n+    } catch(InterruptedException e){\n+      //do nothing as of yet\n+    }\n+    return returnVal;\n+  }\n+  \n+  /**\n+   * Change the permissions on a filename.\n+   * @param filename the name of the file to change\n+   * @param perm the permission string\n+   * @return the exit code from the command\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  public static int chmod(String filename, String perm\n+                          ) throws IOException, InterruptedException {\n+    return chmod(filename, perm, false);\n+  }\n+\n+  /**\n+   * Change the permissions on a file / directory, recursively, if\n+   * needed.\n+   * @param filename name of the file whose permissions are to change\n+   * @param perm permission string\n+   * @param recursive true, if permissions should be changed recursively\n+   * @return the exit code from the command.\n+   * @throws IOException\n+   * @throws InterruptedException\n+   */\n+  public static int chmod(String filename, String perm, boolean recursive)\n+                            throws IOException, InterruptedException {\n+    StringBuffer cmdBuf = new StringBuffer();\n+    cmdBuf.append(\"chmod \");\n+    if (recursive) {\n+      cmdBuf.append(\"-R \");\n+    }\n+    cmdBuf.append(perm).append(\" \");\n+    cmdBuf.append(filename);\n+    String[] shellCmd = {\"bash\", \"-c\" ,cmdBuf.toString()};\n+    ShellCommandExecutor shExec = new ShellCommandExecutor(shellCmd);\n+    try {\n+      shExec.execute();\n+    }catch(Exception e) {\n+      if(Log.isDebugEnabled()) {\n+        Log.debug(\"Error while changing permission : \" + filename \n+            +\" Exception: \" + StringUtils.stringifyException(e));\n+      }\n+    }\n+    return shExec.getExitCode();\n+  }\n+  \n+  /**\n+   * Create a tmp file for a base file.\n+   * @param basefile the base file of the tmp\n+   * @param prefix file name prefix of tmp\n+   * @param isDeleteOnExit if true, the tmp will be deleted when the VM exits\n+   * @return a newly created tmp file\n+   * @exception IOException If a tmp file cannot created\n+   * @see java.io.File#createTempFile(String, String, File)\n+   * @see java.io.File#deleteOnExit()\n+   */\n+  public static final File createLocalTempFile(final File basefile,\n+                                               final String prefix,\n+                                               final boolean isDeleteOnExit)\n+    throws IOException {\n+    File tmp = File.createTempFile(prefix + basefile.getName(),\n+                                   \"\", basefile.getParentFile());\n+    if (isDeleteOnExit) {\n+      tmp.deleteOnExit();\n+    }\n+    return tmp;\n+  }\n+\n+  /**\n+   * Move the src file to the name specified by target.\n+   * @param src the source file\n+   * @param target the target file\n+   * @exception IOException If this operation fails\n+   */\n+  public static void replaceFile(File src, File target) throws IOException {\n+    /* renameTo() has two limitations on Windows platform.\n+     * src.renameTo(target) fails if\n+     * 1) If target already exists OR\n+     * 2) If target is already open for reading/writing.\n+     */\n+    if (!src.renameTo(target)) {\n+      int retries = 5;\n+      while (target.exists() && !target.delete() && retries-- >= 0) {\n+        try {\n+          Thread.sleep(1000);\n+        } catch (InterruptedException e) {\n+          throw new IOException(\"replaceFile interrupted.\");\n+        }\n+      }\n+      if (!src.renameTo(target)) {\n+        throw new IOException(\"Unable to rename \" + src +\n+                              \" to \" + target);\n+      }\n+    }\n+  }\n+}"
        },
        {
            "sha": "2a2aa619afcf9b89315837ff6021cd58cf7b6577",
            "filename": "src/java/org/apache/hadoop/fs/FilterFileSystem.java",
            "status": "added",
            "additions": 278,
            "deletions": 0,
            "changes": 278,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFilterFileSystem.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFilterFileSystem.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFilterFileSystem.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,278 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs;\n+\n+import java.io.*;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.util.Progressable;\n+\n+/****************************************************************\n+ * A <code>FilterFileSystem</code> contains\n+ * some other file system, which it uses as\n+ * its  basic file system, possibly transforming\n+ * the data along the way or providing  additional\n+ * functionality. The class <code>FilterFileSystem</code>\n+ * itself simply overrides all  methods of\n+ * <code>FileSystem</code> with versions that\n+ * pass all requests to the contained  file\n+ * system. Subclasses of <code>FilterFileSystem</code>\n+ * may further override some of  these methods\n+ * and may also provide additional methods\n+ * and fields.\n+ *\n+ *****************************************************************/\n+public class FilterFileSystem extends FileSystem {\n+  \n+  protected FileSystem fs;\n+  \n+  /*\n+   * so that extending classes can define it\n+   */\n+  public FilterFileSystem() {\n+  }\n+  \n+  public FilterFileSystem(FileSystem fs) {\n+    this.fs = fs;\n+    this.statistics = fs.statistics;\n+  }\n+\n+  /** Called after a new FileSystem instance is constructed.\n+   * @param name a uri whose authority section names the host, port, etc.\n+   *   for this FileSystem\n+   * @param conf the configuration\n+   */\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    fs.initialize(name, conf);\n+  }\n+\n+  /** Returns a URI whose scheme and authority identify this FileSystem.*/\n+  public URI getUri() {\n+    return fs.getUri();\n+  }\n+\n+  /** Make sure that a path specifies a FileSystem. */\n+  public Path makeQualified(Path path) {\n+    return fs.makeQualified(path);\n+  }\n+  \n+  ///////////////////////////////////////////////////////////////\n+  // FileSystem\n+  ///////////////////////////////////////////////////////////////\n+\n+  /** Check that a Path belongs to this FileSystem. */\n+  protected void checkPath(Path path) {\n+    fs.checkPath(path);\n+  }\n+\n+  public BlockLocation[] getFileBlockLocations(FileStatus file, long start,\n+    long len) throws IOException {\n+      return fs.getFileBlockLocations(file, start, len);\n+  }\n+  \n+  /**\n+   * Opens an FSDataInputStream at the indicated Path.\n+   * @param f the file name to open\n+   * @param bufferSize the size of the buffer to be used.\n+   */\n+  public FSDataInputStream open(Path f, int bufferSize) throws IOException {\n+    return fs.open(f, bufferSize);\n+  }\n+\n+  /** {@inheritDoc} */\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    return fs.append(f, bufferSize, progress);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize, short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    return fs.create(f, permission,\n+        overwrite, bufferSize, replication, blockSize, progress);\n+  }\n+\n+  /**\n+   * Set replication for an existing file.\n+   * \n+   * @param src file name\n+   * @param replication new replication\n+   * @throws IOException\n+   * @return true if successful;\n+   *         false if file does not exist or is a directory\n+   */\n+  public boolean setReplication(Path src, short replication) throws IOException {\n+    return fs.setReplication(src, replication);\n+  }\n+  \n+  /**\n+   * Renames Path src to Path dst.  Can take place on local fs\n+   * or remote DFS.\n+   */\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    return fs.rename(src, dst);\n+  }\n+  \n+  /** Delete a file */\n+  public boolean delete(Path f, boolean recursive) throws IOException {\n+    return fs.delete(f, recursive);\n+  }\n+  \n+  /** List files in a directory. */\n+  public FileStatus[] listStatus(Path f) throws IOException {\n+    return fs.listStatus(f);\n+  }\n+  \n+  public Path getHomeDirectory() {\n+    return fs.getHomeDirectory();\n+  }\n+\n+\n+  /**\n+   * Set the current working directory for the given file system. All relative\n+   * paths will be resolved relative to it.\n+   * \n+   * @param newDir\n+   */\n+  public void setWorkingDirectory(Path newDir) {\n+    fs.setWorkingDirectory(newDir);\n+  }\n+  \n+  /**\n+   * Get the current working directory for the given file system\n+   * \n+   * @return the directory pathname\n+   */\n+  public Path getWorkingDirectory() {\n+    return fs.getWorkingDirectory();\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public FsStatus getStatus(Path p) throws IOException {\n+    return fs.getStatus(p);\n+  }\n+  \n+  /** {@inheritDoc} */\n+  @Override\n+  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n+    return fs.mkdirs(f, permission);\n+  }\n+\n+  /**\n+   * The src file is on the local disk.  Add it to FS at\n+   * the given dst name.\n+   * delSrc indicates if the source should be removed\n+   */\n+  public void copyFromLocalFile(boolean delSrc, Path src, Path dst)\n+    throws IOException {\n+    fs.copyFromLocalFile(delSrc, src, dst);\n+  }\n+  \n+  /**\n+   * The src file is under FS, and the dst is on the local disk.\n+   * Copy it from FS control to the local dst name.\n+   * delSrc indicates if the src will be removed or not.\n+   */   \n+  public void copyToLocalFile(boolean delSrc, Path src, Path dst)\n+    throws IOException {\n+    fs.copyToLocalFile(delSrc, src, dst);\n+  }\n+  \n+  /**\n+   * Returns a local File that the user can write output to.  The caller\n+   * provides both the eventual FS target name and the local working\n+   * file.  If the FS is local, we write directly into the target.  If\n+   * the FS is remote, we write into the tmp local area.\n+   */\n+  public Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile)\n+    throws IOException {\n+    return fs.startLocalOutput(fsOutputFile, tmpLocalFile);\n+  }\n+\n+  /**\n+   * Called when we're all done writing to the target.  A local FS will\n+   * do nothing, because we've written to exactly the right place.  A remote\n+   * FS will copy the contents of tmpLocalFile to the correct target at\n+   * fsOutputFile.\n+   */\n+  public void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile)\n+    throws IOException {\n+    fs.completeLocalOutput(fsOutputFile, tmpLocalFile);\n+  }\n+\n+  /** Return the number of bytes that large input files should be optimally\n+   * be split into to minimize i/o time. */\n+  public long getDefaultBlockSize() {\n+    return fs.getDefaultBlockSize();\n+  }\n+  \n+  /**\n+   * Get the default replication.\n+   */\n+  public short getDefaultReplication() {\n+    return fs.getDefaultReplication();\n+  }\n+\n+  /**\n+   * Get file status.\n+   */\n+  public FileStatus getFileStatus(Path f) throws IOException {\n+    return fs.getFileStatus(f);\n+  }\n+\n+  /** {@inheritDoc} */\n+  public FileChecksum getFileChecksum(Path f) throws IOException {\n+    return fs.getFileChecksum(f);\n+  }\n+  \n+  /** {@inheritDoc} */\n+  public void setVerifyChecksum(boolean verifyChecksum) {\n+    fs.setVerifyChecksum(verifyChecksum);\n+  }\n+\n+  @Override\n+  public Configuration getConf() {\n+    return fs.getConf();\n+  }\n+  \n+  @Override\n+  public void close() throws IOException {\n+    super.close();\n+    fs.close();\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public void setOwner(Path p, String username, String groupname\n+      ) throws IOException {\n+    fs.setOwner(p, username, groupname);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public void setPermission(Path p, FsPermission permission\n+      ) throws IOException {\n+    fs.setPermission(p, permission);\n+  }\n+}"
        },
        {
            "sha": "987b4999668793bcf4d9f7092b14c6f1ddcae3b3",
            "filename": "src/java/org/apache/hadoop/fs/FsShell.java",
            "status": "added",
            "additions": 1925,
            "deletions": 0,
            "changes": 1925,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsShell.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsShell.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsShell.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,1925 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import java.text.SimpleDateFormat;\n+import java.util.*;\n+import java.util.zip.GZIPInputStream;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.shell.CommandFormat;\n+import org.apache.hadoop.fs.shell.Count;\n+import org.apache.hadoop.io.DataInputBuffer;\n+import org.apache.hadoop.io.DataOutputBuffer;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.io.SequenceFile;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.io.WritableComparable;\n+import org.apache.hadoop.ipc.RPC;\n+import org.apache.hadoop.ipc.RemoteException;\n+import org.apache.hadoop.util.ReflectionUtils;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+import org.apache.hadoop.util.StringUtils;\n+\n+/** Provide command line access to a FileSystem. */\n+public class FsShell extends Configured implements Tool {\n+\n+  protected FileSystem fs;\n+  private Trash trash;\n+  public static final SimpleDateFormat dateForm = \n+    new SimpleDateFormat(\"yyyy-MM-dd HH:mm\");\n+  protected static final SimpleDateFormat modifFmt =\n+    new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");\n+  static final int BORDER = 2;\n+  static {\n+    modifFmt.setTimeZone(TimeZone.getTimeZone(\"UTC\"));\n+  }\n+  static final String SETREP_SHORT_USAGE=\"-setrep [-R] [-w] <rep> <path/file>\";\n+  static final String GET_SHORT_USAGE = \"-get [-ignoreCrc] [-crc] <src> <localdst>\";\n+  static final String COPYTOLOCAL_SHORT_USAGE = GET_SHORT_USAGE.replace(\n+      \"-get\", \"-copyToLocal\");\n+  static final String TAIL_USAGE=\"-tail [-f] <file>\";\n+\n+  /**\n+   */\n+  public FsShell() {\n+    this(null);\n+  }\n+\n+  public FsShell(Configuration conf) {\n+    super(conf);\n+    fs = null;\n+    trash = null;\n+  }\n+  \n+  protected void init() throws IOException {\n+    getConf().setQuietMode(true);\n+    if (this.fs == null) {\n+     this.fs = FileSystem.get(getConf());\n+    }\n+    if (this.trash == null) {\n+      this.trash = new Trash(getConf());\n+    }\n+  }\n+\n+  \n+  /**\n+   * Copies from stdin to the indicated file.\n+   */\n+  private void copyFromStdin(Path dst, FileSystem dstFs) throws IOException {\n+    if (dstFs.isDirectory(dst)) {\n+      throw new IOException(\"When source is stdin, destination must be a file.\");\n+    }\n+    if (dstFs.exists(dst)) {\n+      throw new IOException(\"Target \" + dst.toString() + \" already exists.\");\n+    }\n+    FSDataOutputStream out = dstFs.create(dst); \n+    try {\n+      IOUtils.copyBytes(System.in, out, getConf(), false);\n+    } \n+    finally {\n+      out.close();\n+    }\n+  }\n+\n+  /** \n+   * Print from src to stdout.\n+   */\n+  private void printToStdout(InputStream in) throws IOException {\n+    try {\n+      IOUtils.copyBytes(in, System.out, getConf(), false);\n+    } finally {\n+      in.close();\n+    }\n+  }\n+\n+  \n+  /**\n+   * Add local files to the indicated FileSystem name. src is kept.\n+   */\n+  void copyFromLocal(Path[] srcs, String dstf) throws IOException {\n+    Path dstPath = new Path(dstf);\n+    FileSystem dstFs = dstPath.getFileSystem(getConf());\n+    if (srcs.length == 1 && srcs[0].toString().equals(\"-\"))\n+      copyFromStdin(dstPath, dstFs);\n+    else\n+      dstFs.copyFromLocalFile(false, false, srcs, dstPath);\n+  }\n+  \n+  /**\n+   * Add local files to the indicated FileSystem name. src is removed.\n+   */\n+  void moveFromLocal(Path[] srcs, String dstf) throws IOException {\n+    Path dstPath = new Path(dstf);\n+    FileSystem dstFs = dstPath.getFileSystem(getConf());\n+    dstFs.moveFromLocalFile(srcs, dstPath);\n+  }\n+\n+  /**\n+   * Add a local file to the indicated FileSystem name. src is removed.\n+   */\n+  void moveFromLocal(Path src, String dstf) throws IOException {\n+    moveFromLocal((new Path[]{src}), dstf);\n+  }\n+\n+  /**\n+   * Obtain the indicated files that match the file pattern <i>srcf</i>\n+   * and copy them to the local name. srcf is kept.\n+   * When copying multiple files, the destination must be a directory. \n+   * Otherwise, IOException is thrown.\n+   * @param argv: arguments\n+   * @param pos: Ignore everything before argv[pos]  \n+   * @exception: IOException  \n+   * @see org.apache.hadoop.fs.FileSystem.globStatus \n+   */\n+  void copyToLocal(String[]argv, int pos) throws IOException {\n+    CommandFormat cf = new CommandFormat(\"copyToLocal\", 2,2,\"crc\",\"ignoreCrc\");\n+    \n+    String srcstr = null;\n+    String dststr = null;\n+    try {\n+      List<String> parameters = cf.parse(argv, pos);\n+      srcstr = parameters.get(0);\n+      dststr = parameters.get(1);\n+    }\n+    catch(IllegalArgumentException iae) {\n+      System.err.println(\"Usage: java FsShell \" + GET_SHORT_USAGE);\n+      throw iae;\n+    }\n+    boolean copyCrc = cf.getOpt(\"crc\");\n+    final boolean verifyChecksum = !cf.getOpt(\"ignoreCrc\");\n+\n+    if (dststr.equals(\"-\")) {\n+      if (copyCrc) {\n+        System.err.println(\"-crc option is not valid when destination is stdout.\");\n+      }\n+      cat(srcstr, verifyChecksum);\n+    } else {\n+      File dst = new File(dststr);      \n+      Path srcpath = new Path(srcstr);\n+      FileSystem srcFS = getSrcFileSystem(srcpath, verifyChecksum);\n+      if (copyCrc && !(srcFS instanceof ChecksumFileSystem)) {\n+        System.err.println(\"-crc option is not valid when source file system \" +\n+            \"does not have crc files. Automatically turn the option off.\");\n+        copyCrc = false;\n+      }\n+      FileStatus[] srcs = srcFS.globStatus(srcpath);\n+      boolean dstIsDir = dst.isDirectory(); \n+      if (srcs.length > 1 && !dstIsDir) {\n+        throw new IOException(\"When copying multiple files, \"\n+                              + \"destination should be a directory.\");\n+      }\n+      for (FileStatus status : srcs) {\n+        Path p = status.getPath();\n+        File f = dstIsDir? new File(dst, p.getName()): dst;\n+        copyToLocal(srcFS, status, f, copyCrc);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Return the {@link FileSystem} specified by src and the conf.\n+   * It the {@link FileSystem} supports checksum, set verifyChecksum.\n+   */\n+  private FileSystem getSrcFileSystem(Path src, boolean verifyChecksum\n+      ) throws IOException { \n+    FileSystem srcFs = src.getFileSystem(getConf());\n+    srcFs.setVerifyChecksum(verifyChecksum);\n+    return srcFs;\n+  }\n+\n+  /**\n+   * The prefix for the tmp file used in copyToLocal.\n+   * It must be at least three characters long, required by\n+   * {@link java.io.File#createTempFile(String, String, File)}.\n+   */\n+  static final String COPYTOLOCAL_PREFIX = \"_copyToLocal_\";\n+\n+  /**\n+   * Copy a source file from a given file system to local destination.\n+   * @param srcFS source file system\n+   * @param src source path\n+   * @param dst destination\n+   * @param copyCrc copy CRC files?\n+   * @exception IOException If some IO failed\n+   */\n+  private void copyToLocal(final FileSystem srcFS, final FileStatus srcStatus,\n+                           final File dst, final boolean copyCrc)\n+    throws IOException {\n+    /* Keep the structure similar to ChecksumFileSystem.copyToLocal(). \n+     * Ideal these two should just invoke FileUtil.copy() and not repeat\n+     * recursion here. Of course, copy() should support two more options :\n+     * copyCrc and useTmpFile (may be useTmpFile need not be an option).\n+     */\n+    \n+    Path src = srcStatus.getPath();\n+    if (!srcStatus.isDir()) {\n+      if (dst.exists()) {\n+        // match the error message in FileUtil.checkDest():\n+        throw new IOException(\"Target \" + dst + \" already exists\");\n+      }\n+      \n+      // use absolute name so that tmp file is always created under dest dir\n+      File tmp = FileUtil.createLocalTempFile(dst.getAbsoluteFile(),\n+                                              COPYTOLOCAL_PREFIX, true);\n+      if (!FileUtil.copy(srcFS, src, tmp, false, srcFS.getConf())) {\n+        throw new IOException(\"Failed to copy \" + src + \" to \" + dst); \n+      }\n+      \n+      if (!tmp.renameTo(dst)) {\n+        throw new IOException(\"Failed to rename tmp file \" + tmp + \n+                              \" to local destination \\\"\" + dst + \"\\\".\");\n+      }\n+\n+      if (copyCrc) {\n+        if (!(srcFS instanceof ChecksumFileSystem)) {\n+          throw new IOException(\"Source file system does not have crc files\");\n+        }\n+        \n+        ChecksumFileSystem csfs = (ChecksumFileSystem) srcFS;\n+        File dstcs = FileSystem.getLocal(srcFS.getConf())\n+          .pathToFile(csfs.getChecksumFile(new Path(dst.getCanonicalPath())));\n+        FileSystem fs = csfs.getRawFileSystem();\n+        FileStatus status = csfs.getFileStatus(csfs.getChecksumFile(src));\n+        copyToLocal(fs, status, dstcs, false);\n+      } \n+    } else {\n+      // once FileUtil.copy() supports tmp file, we don't need to mkdirs().\n+      if (!dst.mkdirs()) {\n+        throw new IOException(\"Failed to create local destination \\\"\" +\n+                              dst + \"\\\".\");\n+      }\n+      for(FileStatus status : srcFS.listStatus(src)) {\n+        copyToLocal(srcFS, status,\n+                    new File(dst, status.getPath().getName()), copyCrc);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Get all the files in the directories that match the source file \n+   * pattern and merge and sort them to only one file on local fs \n+   * srcf is kept.\n+   * @param srcf: a file pattern specifying source files\n+   * @param dstf: a destination local file/directory \n+   * @exception: IOException  \n+   * @see org.apache.hadoop.fs.FileSystem.globStatus \n+   */\n+  void copyMergeToLocal(String srcf, Path dst) throws IOException {\n+    copyMergeToLocal(srcf, dst, false);\n+  }    \n+    \n+\n+  /**\n+   * Get all the files in the directories that match the source file pattern\n+   * and merge and sort them to only one file on local fs \n+   * srcf is kept.\n+   * \n+   * Also adds a string between the files (useful for adding \\n\n+   * to a text file)\n+   * @param srcf: a file pattern specifying source files\n+   * @param dstf: a destination local file/directory\n+   * @param endline: if an end of line character is added to a text file \n+   * @exception: IOException  \n+   * @see org.apache.hadoop.fs.FileSystem.globStatus \n+   */\n+  void copyMergeToLocal(String srcf, Path dst, boolean endline) throws IOException {\n+    Path srcPath = new Path(srcf);\n+    FileSystem srcFs = srcPath.getFileSystem(getConf());\n+    Path [] srcs = FileUtil.stat2Paths(srcFs.globStatus(srcPath), \n+                                       srcPath);\n+    for(int i=0; i<srcs.length; i++) {\n+      if (endline) {\n+        FileUtil.copyMerge(srcFs, srcs[i], \n+                           FileSystem.getLocal(getConf()), dst, false, getConf(), \"\\n\");\n+      } else {\n+        FileUtil.copyMerge(srcFs, srcs[i], \n+                           FileSystem.getLocal(getConf()), dst, false, getConf(), null);\n+      }\n+    }\n+  }      \n+\n+  /**\n+   * Obtain the indicated file and copy to the local name.\n+   * srcf is removed.\n+   */\n+  void moveToLocal(String srcf, Path dst) throws IOException {\n+    System.err.println(\"Option '-moveToLocal' is not implemented yet.\");\n+  }\n+\n+  /**\n+   * Fetch all files that match the file pattern <i>srcf</i> and display\n+   * their content on stdout. \n+   * @param srcf: a file pattern specifying source files\n+   * @exception: IOException\n+   * @see org.apache.hadoop.fs.FileSystem.globStatus \n+   */\n+  void cat(String src, boolean verifyChecksum) throws IOException {\n+    //cat behavior in Linux\n+    //  [~/1207]$ ls ?.txt\n+    //  x.txt  z.txt\n+    //  [~/1207]$ cat x.txt y.txt z.txt\n+    //  xxx\n+    //  cat: y.txt: No such file or directory\n+    //  zzz\n+\n+    Path srcPattern = new Path(src);\n+    new DelayedExceptionThrowing() {\n+      @Override\n+      void process(Path p, FileSystem srcFs) throws IOException {\n+        if (srcFs.getFileStatus(p).isDir()) {\n+          throw new IOException(\"Source must be a file.\");\n+        }\n+        printToStdout(srcFs.open(p));\n+      }\n+    }.globAndProcess(srcPattern, getSrcFileSystem(srcPattern, verifyChecksum));\n+  }\n+\n+  private class TextRecordInputStream extends InputStream {\n+    SequenceFile.Reader r;\n+    WritableComparable key;\n+    Writable val;\n+\n+    DataInputBuffer inbuf;\n+    DataOutputBuffer outbuf;\n+\n+    public TextRecordInputStream(FileStatus f) throws IOException {\n+      r = new SequenceFile.Reader(fs, f.getPath(), getConf());\n+      key = ReflectionUtils.newInstance(r.getKeyClass().asSubclass(WritableComparable.class),\n+                                        getConf());\n+      val = ReflectionUtils.newInstance(r.getValueClass().asSubclass(Writable.class),\n+                                        getConf());\n+      inbuf = new DataInputBuffer();\n+      outbuf = new DataOutputBuffer();\n+    }\n+\n+    public int read() throws IOException {\n+      int ret;\n+      if (null == inbuf || -1 == (ret = inbuf.read())) {\n+        if (!r.next(key, val)) {\n+          return -1;\n+        }\n+        byte[] tmp = key.toString().getBytes();\n+        outbuf.write(tmp, 0, tmp.length);\n+        outbuf.write('\\t');\n+        tmp = val.toString().getBytes();\n+        outbuf.write(tmp, 0, tmp.length);\n+        outbuf.write('\\n');\n+        inbuf.reset(outbuf.getData(), outbuf.getLength());\n+        outbuf.reset();\n+        ret = inbuf.read();\n+      }\n+      return ret;\n+    }\n+  }\n+\n+  private InputStream forMagic(Path p, FileSystem srcFs) throws IOException {\n+    FSDataInputStream i = srcFs.open(p);\n+    switch(i.readShort()) {\n+      case 0x1f8b: // RFC 1952\n+        i.seek(0);\n+        return new GZIPInputStream(i);\n+      case 0x5345: // 'S' 'E'\n+        if (i.readByte() == 'Q') {\n+          i.close();\n+          return new TextRecordInputStream(srcFs.getFileStatus(p));\n+        }\n+        break;\n+    }\n+    i.seek(0);\n+    return i;\n+  }\n+\n+  void text(String srcf) throws IOException {\n+    Path srcPattern = new Path(srcf);\n+    new DelayedExceptionThrowing() {\n+      @Override\n+      void process(Path p, FileSystem srcFs) throws IOException {\n+        if (srcFs.isDirectory(p)) {\n+          throw new IOException(\"Source must be a file.\");\n+        }\n+        printToStdout(forMagic(p, srcFs));\n+      }\n+    }.globAndProcess(srcPattern, srcPattern.getFileSystem(getConf()));\n+  }\n+\n+  /**\n+   * Parse the incoming command string\n+   * @param cmd\n+   * @param pos ignore anything before this pos in cmd\n+   * @throws IOException \n+   */\n+  private void setReplication(String[] cmd, int pos) throws IOException {\n+    CommandFormat c = new CommandFormat(\"setrep\", 2, 2, \"R\", \"w\");\n+    String dst = null;\n+    short rep = 0;\n+\n+    try {\n+      List<String> parameters = c.parse(cmd, pos);\n+      rep = Short.parseShort(parameters.get(0));\n+      dst = parameters.get(1);\n+    } catch (NumberFormatException nfe) {\n+      System.err.println(\"Illegal replication, a positive integer expected\");\n+      throw nfe;\n+    }\n+    catch(IllegalArgumentException iae) {\n+      System.err.println(\"Usage: java FsShell \" + SETREP_SHORT_USAGE);\n+      throw iae;\n+    }\n+\n+    if (rep < 1) {\n+      System.err.println(\"Cannot set replication to: \" + rep);\n+      throw new IllegalArgumentException(\"replication must be >= 1\");\n+    }\n+\n+    List<Path> waitList = c.getOpt(\"w\")? new ArrayList<Path>(): null;\n+    setReplication(rep, dst, c.getOpt(\"R\"), waitList);\n+\n+    if (waitList != null) {\n+      waitForReplication(waitList, rep);\n+    }\n+  }\n+    \n+  /**\n+   * Wait for all files in waitList to have replication number equal to rep.\n+   * @param waitList The files are waited for.\n+   * @param rep The new replication number.\n+   * @throws IOException IOException\n+   */\n+  void waitForReplication(List<Path> waitList, int rep) throws IOException {\n+    for(Path f : waitList) {\n+      System.out.print(\"Waiting for \" + f + \" ...\");\n+      System.out.flush();\n+\n+      boolean printWarning = false;\n+      FileStatus status = fs.getFileStatus(f);\n+      long len = status.getLen();\n+\n+      for(boolean done = false; !done; ) {\n+        BlockLocation[] locations = fs.getFileBlockLocations(status, 0, len);\n+        int i = 0;\n+        for(; i < locations.length && \n+          locations[i].getHosts().length == rep; i++)\n+          if (!printWarning && locations[i].getHosts().length > rep) {\n+            System.out.println(\"\\nWARNING: the waiting time may be long for \"\n+                + \"DECREASING the number of replication.\");\n+            printWarning = true;\n+          }\n+        done = i == locations.length;\n+\n+        if (!done) {\n+          System.out.print(\".\");\n+          System.out.flush();\n+          try {Thread.sleep(10000);} catch (InterruptedException e) {}\n+        }\n+      }\n+\n+      System.out.println(\" done\");\n+    }\n+  }\n+\n+  /**\n+   * Set the replication for files that match file pattern <i>srcf</i>\n+   * if it's a directory and recursive is true,\n+   * set replication for all the subdirs and those files too.\n+   * @param newRep new replication factor\n+   * @param srcf a file pattern specifying source files\n+   * @param recursive if need to set replication factor for files in subdirs\n+   * @throws IOException  \n+   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)\n+   */\n+  void setReplication(short newRep, String srcf, boolean recursive,\n+                      List<Path> waitingList)\n+    throws IOException {\n+    Path srcPath = new Path(srcf);\n+    FileSystem srcFs = srcPath.getFileSystem(getConf());\n+    Path[] srcs = FileUtil.stat2Paths(srcFs.globStatus(srcPath),\n+                                      srcPath);\n+    for(int i=0; i<srcs.length; i++) {\n+      setReplication(newRep, srcFs, srcs[i], recursive, waitingList);\n+    }\n+  }\n+\n+  private void setReplication(short newRep, FileSystem srcFs, \n+                              Path src, boolean recursive,\n+                              List<Path> waitingList)\n+    throws IOException {\n+    if (!srcFs.getFileStatus(src).isDir()) {\n+      setFileReplication(src, srcFs, newRep, waitingList);\n+      return;\n+    }\n+    FileStatus items[] = srcFs.listStatus(src);\n+    if (items == null) {\n+      throw new IOException(\"Could not get listing for \" + src);\n+    } else {\n+\n+      for (int i = 0; i < items.length; i++) {\n+        if (!items[i].isDir()) {\n+          setFileReplication(items[i].getPath(), srcFs, newRep, waitingList);\n+        } else if (recursive) {\n+          setReplication(newRep, srcFs, items[i].getPath(), recursive, \n+                         waitingList);\n+        }\n+      }\n+    }\n+  }\n+    \n+  /**\n+   * Actually set the replication for this file\n+   * If it fails either throw IOException or print an error msg\n+   * @param file: a file/directory\n+   * @param newRep: new replication factor\n+   * @throws IOException\n+   */\n+  private void setFileReplication(Path file, FileSystem srcFs, short newRep, List<Path> waitList)\n+    throws IOException {\n+    if (srcFs.setReplication(file, newRep)) {\n+      if (waitList != null) {\n+        waitList.add(file);\n+      }\n+      System.out.println(\"Replication \" + newRep + \" set: \" + file);\n+    } else {\n+      System.err.println(\"Could not set replication for: \" + file);\n+    }\n+  }\n+    \n+    \n+  /**\n+   * Get a listing of all files in that match the file pattern <i>srcf</i>.\n+   * @param srcf a file pattern specifying source files\n+   * @param recursive if need to list files in subdirs\n+   * @throws IOException  \n+   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)\n+   */\n+  private int ls(String srcf, boolean recursive) throws IOException {\n+    Path srcPath = new Path(srcf);\n+    FileSystem srcFs = srcPath.getFileSystem(this.getConf());\n+    FileStatus[] srcs = srcFs.globStatus(srcPath);\n+    if (srcs==null || srcs.length==0) {\n+      throw new FileNotFoundException(\"Cannot access \" + srcf + \n+          \": No such file or directory.\");\n+    }\n+ \n+    boolean printHeader = (srcs.length == 1) ? true: false;\n+    int numOfErrors = 0;\n+    for(int i=0; i<srcs.length; i++) {\n+      numOfErrors += ls(srcs[i], srcFs, recursive, printHeader);\n+    }\n+    return numOfErrors == 0 ? 0 : -1;\n+  }\n+\n+  /* list all files under the directory <i>src</i>\n+   * ideally we should provide \"-l\" option, that lists like \"ls -l\".\n+   */\n+  private int ls(FileStatus src, FileSystem srcFs, boolean recursive,\n+      boolean printHeader) throws IOException {\n+    final String cmd = recursive? \"lsr\": \"ls\";\n+    final FileStatus[] items = shellListStatus(cmd, srcFs, src);\n+    if (items == null) {\n+      return 1;\n+    } else {\n+      int numOfErrors = 0;\n+      if (!recursive && printHeader) {\n+        if (items.length != 0) {\n+          System.out.println(\"Found \" + items.length + \" items\");\n+        }\n+      }\n+      \n+      int maxReplication = 3, maxLen = 10, maxOwner = 0,maxGroup = 0;\n+\n+      for(int i = 0; i < items.length; i++) {\n+        FileStatus stat = items[i];\n+        int replication = String.valueOf(stat.getReplication()).length();\n+        int len = String.valueOf(stat.getLen()).length();\n+        int owner = String.valueOf(stat.getOwner()).length();\n+        int group = String.valueOf(stat.getGroup()).length();\n+        \n+        if (replication > maxReplication) maxReplication = replication;\n+        if (len > maxLen) maxLen = len;\n+        if (owner > maxOwner)  maxOwner = owner;\n+        if (group > maxGroup)  maxGroup = group;\n+      }\n+      \n+      for (int i = 0; i < items.length; i++) {\n+        FileStatus stat = items[i];\n+        Path cur = stat.getPath();\n+        String mdate = dateForm.format(new Date(stat.getModificationTime()));\n+        \n+        System.out.print((stat.isDir() ? \"d\" : \"-\") + \n+          stat.getPermission() + \" \");\n+        System.out.printf(\"%\"+ maxReplication + \n+          \"s \", (!stat.isDir() ? stat.getReplication() : \"-\"));\n+        if (maxOwner > 0)\n+          System.out.printf(\"%-\"+ maxOwner + \"s \", stat.getOwner());\n+        if (maxGroup > 0)\n+          System.out.printf(\"%-\"+ maxGroup + \"s \", stat.getGroup());\n+        System.out.printf(\"%\"+ maxLen + \"d \", stat.getLen());\n+        System.out.print(mdate + \" \");\n+        System.out.println(cur.toUri().getPath());\n+        if (recursive && stat.isDir()) {\n+          numOfErrors += ls(stat,srcFs, recursive, printHeader);\n+        }\n+      }\n+      return numOfErrors;\n+    }\n+  }\n+\n+   /**\n+   * Show the size of a partition in the filesystem that contains\n+   * the specified <i>path</i>.\n+   * @param path a path specifying the source partition. null means /.\n+   * @throws IOException  \n+   */\n+  void df(String path) throws IOException {\n+    if (path == null) path = \"/\";\n+    final Path srcPath = new Path(path);\n+    final FileSystem srcFs = srcPath.getFileSystem(getConf());\n+    if (! srcFs.exists(srcPath)) {\n+      throw new FileNotFoundException(\"Cannot access \"+srcPath.toString());\n+    }\n+    final FsStatus stats = srcFs.getStatus(srcPath);\n+    final int PercentUsed = (int)(100.0f *  (float)stats.getUsed() / (float)stats.getCapacity());\n+    System.out.println(\"Filesystem\\t\\tSize\\tUsed\\tAvail\\tUse%\");\n+    System.out.printf(\"%s\\t\\t%d\\t%d\\t%d\\t%d%%\\n\",\n+      path, \n+      stats.getCapacity(), stats.getUsed(), stats.getRemaining(),\n+      PercentUsed);\n+  }\n+\n+  /**\n+   * Show the size of all files that match the file pattern <i>src</i>\n+   * @param src a file pattern specifying source files\n+   * @throws IOException  \n+   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)\n+   */\n+  void du(String src) throws IOException {\n+    Path srcPath = new Path(src);\n+    FileSystem srcFs = srcPath.getFileSystem(getConf());\n+    Path[] pathItems = FileUtil.stat2Paths(srcFs.globStatus(srcPath), \n+                                           srcPath);\n+    FileStatus items[] = srcFs.listStatus(pathItems);\n+    if ((items == null) || ((items.length == 0) && \n+        (!srcFs.exists(srcPath)))){\n+      throw new FileNotFoundException(\"Cannot access \" + src\n+            + \": No such file or directory.\");\n+    } else {\n+      System.out.println(\"Found \" + items.length + \" items\");\n+      int maxLength = 10;\n+      \n+      long length[] = new long[items.length];\n+      for (int i = 0; i < items.length; i++) {\n+        length[i] = items[i].isDir() ?\n+          srcFs.getContentSummary(items[i].getPath()).getLength() :\n+          items[i].getLen();\n+        int len = String.valueOf(length[i]).length();\n+        if (len > maxLength) maxLength = len;\n+      }\n+      for(int i = 0; i < items.length; i++) {\n+        System.out.printf(\"%-\"+ (maxLength + BORDER) +\"d\", length[i]);\n+        System.out.println(items[i].getPath());\n+      }\n+    }\n+  }\n+    \n+  /**\n+   * Show the summary disk usage of each dir/file \n+   * that matches the file pattern <i>src</i>\n+   * @param src a file pattern specifying source files\n+   * @throws IOException  \n+   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)\n+   */\n+  void dus(String src) throws IOException {\n+    Path srcPath = new Path(src);\n+    FileSystem srcFs = srcPath.getFileSystem(getConf());\n+    FileStatus status[] = srcFs.globStatus(new Path(src));\n+    if (status==null || status.length==0) {\n+      throw new FileNotFoundException(\"Cannot access \" + src + \n+          \": No such file or directory.\");\n+    }\n+    for(int i=0; i<status.length; i++) {\n+      long totalSize = srcFs.getContentSummary(status[i].getPath()).getLength();\n+      String pathStr = status[i].getPath().toString();\n+      System.out.println((\"\".equals(pathStr)?\".\":pathStr) + \"\\t\" + totalSize);\n+    }\n+  }\n+\n+  /**\n+   * Create the given dir\n+   */\n+  void mkdir(String src) throws IOException {\n+    Path f = new Path(src);\n+    FileSystem srcFs = f.getFileSystem(getConf());\n+    FileStatus fstatus = null;\n+    try {\n+      fstatus = srcFs.getFileStatus(f);\n+      if (fstatus.isDir()) {\n+        throw new IOException(\"cannot create directory \" \n+            + src + \": File exists\");\n+      }\n+      else {\n+        throw new IOException(src + \" exists but \" +\n+            \"is not a directory\");\n+      }\n+    } catch(FileNotFoundException e) {\n+        if (!srcFs.mkdirs(f)) {\n+          throw new IOException(\"failed to create \" + src);\n+        }\n+    }\n+  }\n+\n+  /**\n+   * (Re)create zero-length file at the specified path.\n+   * This will be replaced by a more UNIX-like touch when files may be\n+   * modified.\n+   */\n+  void touchz(String src) throws IOException {\n+    Path f = new Path(src);\n+    FileSystem srcFs = f.getFileSystem(getConf());\n+    FileStatus st;\n+    if (srcFs.exists(f)) {\n+      st = srcFs.getFileStatus(f);\n+      if (st.isDir()) {\n+        // TODO: handle this\n+        throw new IOException(src + \" is a directory\");\n+      } else if (st.getLen() != 0)\n+        throw new IOException(src + \" must be a zero-length file\");\n+    }\n+    FSDataOutputStream out = srcFs.create(f);\n+    out.close();\n+  }\n+\n+  /**\n+   * Check file types.\n+   */\n+  int test(String argv[], int i) throws IOException {\n+    if (!argv[i].startsWith(\"-\") || argv[i].length() > 2)\n+      throw new IOException(\"Not a flag: \" + argv[i]);\n+    char flag = argv[i].toCharArray()[1];\n+    Path f = new Path(argv[++i]);\n+    FileSystem srcFs = f.getFileSystem(getConf());\n+    switch(flag) {\n+      case 'e':\n+        return srcFs.exists(f) ? 0 : 1;\n+      case 'z':\n+        return srcFs.getFileStatus(f).getLen() == 0 ? 0 : 1;\n+      case 'd':\n+        return srcFs.getFileStatus(f).isDir() ? 0 : 1;\n+      default:\n+        throw new IOException(\"Unknown flag: \" + flag);\n+    }\n+  }\n+\n+  /**\n+   * Print statistics about path in specified format.\n+   * Format sequences:\n+   *   %b: Size of file in blocks\n+   *   %n: Filename\n+   *   %o: Block size\n+   *   %r: replication\n+   *   %y: UTC date as &quot;yyyy-MM-dd HH:mm:ss&quot;\n+   *   %Y: Milliseconds since January 1, 1970 UTC\n+   */\n+  void stat(char[] fmt, String src) throws IOException {\n+    Path srcPath = new Path(src);\n+    FileSystem srcFs = srcPath.getFileSystem(getConf());\n+    FileStatus glob[] = srcFs.globStatus(srcPath);\n+    if (null == glob)\n+      throw new IOException(\"cannot stat `\" + src + \"': No such file or directory\");\n+    for (FileStatus f : glob) {\n+      StringBuilder buf = new StringBuilder();\n+      for (int i = 0; i < fmt.length; ++i) {\n+        if (fmt[i] != '%') {\n+          buf.append(fmt[i]);\n+        } else {\n+          if (i + 1 == fmt.length) break;\n+          switch(fmt[++i]) {\n+            case 'b':\n+              buf.append(f.getLen());\n+              break;\n+            case 'F':\n+              buf.append(f.isDir() ? \"directory\" : \"regular file\");\n+              break;\n+            case 'n':\n+              buf.append(f.getPath().getName());\n+              break;\n+            case 'o':\n+              buf.append(f.getBlockSize());\n+              break;\n+            case 'r':\n+              buf.append(f.getReplication());\n+              break;\n+            case 'y':\n+              buf.append(modifFmt.format(new Date(f.getModificationTime())));\n+              break;\n+            case 'Y':\n+              buf.append(f.getModificationTime());\n+              break;\n+            default:\n+              buf.append(fmt[i]);\n+              break;\n+          }\n+        }\n+      }\n+      System.out.println(buf.toString());\n+    }\n+  }\n+\n+  /**\n+   * Move files that match the file pattern <i>srcf</i>\n+   * to a destination file.\n+   * When moving mutiple files, the destination must be a directory. \n+   * Otherwise, IOException is thrown.\n+   * @param srcf a file pattern specifying source files\n+   * @param dstf a destination local file/directory \n+   * @throws IOException  \n+   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)\n+   */\n+  void rename(String srcf, String dstf) throws IOException {\n+    Path srcPath = new Path(srcf);\n+    Path dstPath = new Path(dstf);\n+    FileSystem fs = srcPath.getFileSystem(getConf());\n+    URI srcURI = fs.getUri();\n+    URI dstURI = dstPath.getFileSystem(getConf()).getUri();\n+    if (srcURI.compareTo(dstURI) != 0) {\n+      throw new IOException(\"src and destination filesystems do not match.\");\n+    }\n+    Path[] srcs = FileUtil.stat2Paths(fs.globStatus(srcPath), srcPath);\n+    Path dst = new Path(dstf);\n+    if (srcs.length > 1 && !fs.isDirectory(dst)) {\n+      throw new IOException(\"When moving multiple files, \" \n+                            + \"destination should be a directory.\");\n+    }\n+    for(int i=0; i<srcs.length; i++) {\n+      if (!fs.rename(srcs[i], dst)) {\n+        FileStatus srcFstatus = null;\n+        FileStatus dstFstatus = null;\n+        try {\n+          srcFstatus = fs.getFileStatus(srcs[i]);\n+        } catch(FileNotFoundException e) {\n+          throw new FileNotFoundException(srcs[i] + \n+          \": No such file or directory\");\n+        }\n+        try {\n+          dstFstatus = fs.getFileStatus(dst);\n+        } catch(IOException e) {\n+        }\n+        if((srcFstatus!= null) && (dstFstatus!= null)) {\n+          if (srcFstatus.isDir()  && !dstFstatus.isDir()) {\n+            throw new IOException(\"cannot overwrite non directory \"\n+                + dst + \" with directory \" + srcs[i]);\n+          }\n+        }\n+        throw new IOException(\"Failed to rename \" + srcs[i] + \" to \" + dst);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Move/rename file(s) to a destination file. Multiple source\n+   * files can be specified. The destination is the last element of\n+   * the argvp[] array.\n+   * If multiple source files are specified, then the destination \n+   * must be a directory. Otherwise, IOException is thrown.\n+   * @exception: IOException  \n+   */\n+  private int rename(String argv[], Configuration conf) throws IOException {\n+    int i = 0;\n+    int exitCode = 0;\n+    String cmd = argv[i++];  \n+    String dest = argv[argv.length-1];\n+    //\n+    // If the user has specified multiple source files, then\n+    // the destination has to be a directory\n+    //\n+    if (argv.length > 3) {\n+      Path dst = new Path(dest);\n+      FileSystem dstFs = dst.getFileSystem(getConf());\n+      if (!dstFs.isDirectory(dst)) {\n+        throw new IOException(\"When moving multiple files, \" \n+                              + \"destination \" + dest + \" should be a directory.\");\n+      }\n+    }\n+    //\n+    // for each source file, issue the rename\n+    //\n+    for (; i < argv.length - 1; i++) {\n+      try {\n+        //\n+        // issue the rename to the fs\n+        //\n+        rename(argv[i], dest);\n+      } catch (RemoteException e) {\n+        //\n+        // This is a error returned by hadoop server. Print\n+        // out the first line of the error mesage.\n+        //\n+        exitCode = -1;\n+        try {\n+          String[] content;\n+          content = e.getLocalizedMessage().split(\"\\n\");\n+          System.err.println(cmd.substring(1) + \": \" + content[0]);\n+        } catch (Exception ex) {\n+          System.err.println(cmd.substring(1) + \": \" +\n+                             ex.getLocalizedMessage());\n+        }\n+      } catch (IOException e) {\n+        //\n+        // IO exception encountered locally.\n+        //\n+        exitCode = -1;\n+        System.err.println(cmd.substring(1) + \": \" +\n+                           e.getLocalizedMessage());\n+      }\n+    }\n+    return exitCode;\n+  }\n+\n+  /**\n+   * Copy files that match the file pattern <i>srcf</i>\n+   * to a destination file.\n+   * When copying mutiple files, the destination must be a directory. \n+   * Otherwise, IOException is thrown.\n+   * @param srcf a file pattern specifying source files\n+   * @param dstf a destination local file/directory \n+   * @throws IOException  \n+   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)\n+   */\n+  void copy(String srcf, String dstf, Configuration conf) throws IOException {\n+    Path srcPath = new Path(srcf);\n+    FileSystem srcFs = srcPath.getFileSystem(getConf());\n+    Path dstPath = new Path(dstf);\n+    FileSystem dstFs = dstPath.getFileSystem(getConf());\n+    Path [] srcs = FileUtil.stat2Paths(srcFs.globStatus(srcPath), srcPath);\n+    if (srcs.length > 1 && !dstFs.isDirectory(dstPath)) {\n+      throw new IOException(\"When copying multiple files, \" \n+                            + \"destination should be a directory.\");\n+    }\n+    for(int i=0; i<srcs.length; i++) {\n+      FileUtil.copy(srcFs, srcs[i], dstFs, dstPath, false, conf);\n+    }\n+  }\n+\n+  /**\n+   * Copy file(s) to a destination file. Multiple source\n+   * files can be specified. The destination is the last element of\n+   * the argvp[] array.\n+   * If multiple source files are specified, then the destination \n+   * must be a directory. Otherwise, IOException is thrown.\n+   * @exception: IOException  \n+   */\n+  private int copy(String argv[], Configuration conf) throws IOException {\n+    int i = 0;\n+    int exitCode = 0;\n+    String cmd = argv[i++];  \n+    String dest = argv[argv.length-1];\n+    //\n+    // If the user has specified multiple source files, then\n+    // the destination has to be a directory\n+    //\n+    if (argv.length > 3) {\n+      Path dst = new Path(dest);\n+      if (!fs.isDirectory(dst)) {\n+        throw new IOException(\"When copying multiple files, \" \n+                              + \"destination \" + dest + \" should be a directory.\");\n+      }\n+    }\n+    //\n+    // for each source file, issue the copy\n+    //\n+    for (; i < argv.length - 1; i++) {\n+      try {\n+        //\n+        // issue the copy to the fs\n+        //\n+        copy(argv[i], dest, conf);\n+      } catch (RemoteException e) {\n+        //\n+        // This is a error returned by hadoop server. Print\n+        // out the first line of the error mesage.\n+        //\n+        exitCode = -1;\n+        try {\n+          String[] content;\n+          content = e.getLocalizedMessage().split(\"\\n\");\n+          System.err.println(cmd.substring(1) + \": \" +\n+                             content[0]);\n+        } catch (Exception ex) {\n+          System.err.println(cmd.substring(1) + \": \" +\n+                             ex.getLocalizedMessage());\n+        }\n+      } catch (IOException e) {\n+        //\n+        // IO exception encountered locally.\n+        //\n+        exitCode = -1;\n+        System.err.println(cmd.substring(1) + \": \" +\n+                           e.getLocalizedMessage());\n+      }\n+    }\n+    return exitCode;\n+  }\n+\n+  /**\n+   * Delete all files that match the file pattern <i>srcf</i>.\n+   * @param srcf a file pattern specifying source files\n+   * @param recursive if need to delete subdirs\n+   * @throws IOException  \n+   * @see org.apache.hadoop.fs.FileSystem#globStatus(Path)\n+   */\n+  void delete(String srcf, final boolean recursive) throws IOException {\n+    //rm behavior in Linux\n+    //  [~/1207]$ ls ?.txt\n+    //  x.txt  z.txt\n+    //  [~/1207]$ rm x.txt y.txt z.txt \n+    //  rm: cannot remove `y.txt': No such file or directory\n+\n+    Path srcPattern = new Path(srcf);\n+    new DelayedExceptionThrowing() {\n+      @Override\n+      void process(Path p, FileSystem srcFs) throws IOException {\n+        delete(p, srcFs, recursive);\n+      }\n+    }.globAndProcess(srcPattern, srcPattern.getFileSystem(getConf()));\n+  }\n+    \n+  /* delete a file */\n+  private void delete(Path src, FileSystem srcFs, boolean recursive) throws IOException {\n+    if (srcFs.isDirectory(src) && !recursive) {\n+      throw new IOException(\"Cannot remove directory \\\"\" + src +\n+                            \"\\\", use -rmr instead\");\n+    }\n+    Trash trashTmp = new Trash(srcFs, getConf());\n+    if (trashTmp.moveToTrash(src)) {\n+      System.out.println(\"Moved to trash: \" + src);\n+      return;\n+    }\n+    if (srcFs.delete(src, true)) {\n+      System.out.println(\"Deleted \" + src);\n+    } else {\n+      if (!srcFs.exists(src)) {\n+        throw new FileNotFoundException(\"cannot remove \"\n+            + src + \": No such file or directory.\");\n+        }\n+      throw new IOException(\"Delete failed \" + src);\n+    }\n+  }\n+\n+  private void expunge() throws IOException {\n+    trash.expunge();\n+    trash.checkpoint();\n+  }\n+\n+  /**\n+   * Returns the Trash object associated with this shell.\n+   */\n+  public Path getCurrentTrashDir() {\n+    return trash.getCurrentTrashDir();\n+  }\n+\n+  /**\n+   * Parse the incoming command string\n+   * @param cmd\n+   * @param pos ignore anything before this pos in cmd\n+   * @throws IOException \n+   */\n+  private void tail(String[] cmd, int pos) throws IOException {\n+    CommandFormat c = new CommandFormat(\"tail\", 1, 1, \"f\");\n+    String src = null;\n+    Path path = null;\n+\n+    try {\n+      List<String> parameters = c.parse(cmd, pos);\n+      src = parameters.get(0);\n+    } catch(IllegalArgumentException iae) {\n+      System.err.println(\"Usage: java FsShell \" + TAIL_USAGE);\n+      throw iae;\n+    }\n+    boolean foption = c.getOpt(\"f\") ? true: false;\n+    path = new Path(src);\n+    FileSystem srcFs = path.getFileSystem(getConf());\n+    FileStatus fileStatus = srcFs.getFileStatus(path);\n+    if (fileStatus.isDir()) {\n+      throw new IOException(\"Source must be a file.\");\n+    }\n+\n+    long fileSize = fileStatus.getLen();\n+    long offset = (fileSize > 1024) ? fileSize - 1024: 0;\n+\n+    while (true) {\n+      FSDataInputStream in = srcFs.open(path);\n+      in.seek(offset);\n+      IOUtils.copyBytes(in, System.out, 1024, false);\n+      offset = in.getPos();\n+      in.close();\n+      if (!foption) {\n+        break;\n+      }\n+      fileSize = srcFs.getFileStatus(path).getLen();\n+      offset = (fileSize > offset) ? offset: fileSize;\n+      try {\n+        Thread.sleep(5000);\n+      } catch (InterruptedException e) {\n+        break;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * This class runs a command on a given FileStatus. This can be used for\n+   * running various commands like chmod, chown etc.\n+   */\n+  static abstract class CmdHandler {\n+    \n+    protected int errorCode = 0;\n+    protected boolean okToContinue = true;\n+    protected String cmdName;\n+    \n+    int getErrorCode() { return errorCode; }\n+    boolean okToContinue() { return okToContinue; }\n+    String getName() { return cmdName; }\n+    \n+    protected CmdHandler(String cmdName, FileSystem fs) {\n+      this.cmdName = cmdName;\n+    }\n+    \n+    public abstract void run(FileStatus file, FileSystem fs) throws IOException;\n+  }\n+  \n+  /** helper returns listStatus() */\n+  private static FileStatus[] shellListStatus(String cmd, \n+                                                   FileSystem srcFs,\n+                                                   FileStatus src) {\n+    if (!src.isDir()) {\n+      FileStatus[] files = { src };\n+      return files;\n+    }\n+    Path path = src.getPath();\n+    try {\n+      FileStatus[] files = srcFs.listStatus(path);\n+      if ( files == null ) {\n+        System.err.println(cmd + \n+                           \": could not get listing for '\" + path + \"'\");\n+      }\n+      return files;\n+    } catch (IOException e) {\n+      System.err.println(cmd + \n+                         \": could not get get listing for '\" + path + \"' : \" +\n+                         e.getMessage().split(\"\\n\")[0]);\n+    }\n+    return null;\n+  }\n+  \n+  \n+  /**\n+   * Runs the command on a given file with the command handler. \n+   * If recursive is set, command is run recursively.\n+   */                                       \n+  private static int runCmdHandler(CmdHandler handler, FileStatus stat, \n+                                   FileSystem srcFs, \n+                                   boolean recursive) throws IOException {\n+    int errors = 0;\n+    handler.run(stat, srcFs);\n+    if (recursive && stat.isDir() && handler.okToContinue()) {\n+      FileStatus[] files = shellListStatus(handler.getName(), srcFs, stat);\n+      if (files == null) {\n+        return 1;\n+      }\n+      for(FileStatus file : files ) {\n+        errors += runCmdHandler(handler, file, srcFs, recursive);\n+      }\n+    }\n+    return errors;\n+  }\n+\n+  ///top level runCmdHandler\n+  int runCmdHandler(CmdHandler handler, String[] args,\n+                                   int startIndex, boolean recursive) \n+                                   throws IOException {\n+    int errors = 0;\n+    \n+    for (int i=startIndex; i<args.length; i++) {\n+      Path srcPath = new Path(args[i]);\n+      FileSystem srcFs = srcPath.getFileSystem(getConf());\n+      Path[] paths = FileUtil.stat2Paths(srcFs.globStatus(srcPath), srcPath);\n+      for(Path path : paths) {\n+        try {\n+          FileStatus file = srcFs.getFileStatus(path);\n+          if (file == null) {\n+            System.err.println(handler.getName() + \n+                               \": could not get status for '\" + path + \"'\");\n+            errors++;\n+          } else {\n+            errors += runCmdHandler(handler, file, srcFs, recursive);\n+          }\n+        } catch (IOException e) {\n+          String msg = (e.getMessage() != null ? e.getLocalizedMessage() :\n+            (e.getCause().getMessage() != null ? \n+                e.getCause().getLocalizedMessage() : \"null\"));\n+          System.err.println(handler.getName() + \": could not get status for '\"\n+                                        + path + \"': \" + msg.split(\"\\n\")[0]);        \n+        }\n+      }\n+    }\n+    \n+    return (errors > 0 || handler.getErrorCode() != 0) ? 1 : 0;\n+  }\n+  \n+  /**\n+   * Return an abbreviated English-language desc of the byte length\n+   * @deprecated Consider using {@link org.apache.hadoop.util.StringUtils#byteDesc} instead.\n+   */\n+  @Deprecated\n+  public static String byteDesc(long len) {\n+    return StringUtils.byteDesc(len);\n+  }\n+\n+  /**\n+   * @deprecated Consider using {@link org.apache.hadoop.util.StringUtils#limitDecimalTo2} instead.\n+   */\n+  @Deprecated\n+  public static synchronized String limitDecimalTo2(double d) {\n+    return StringUtils.limitDecimalTo2(d);\n+  }\n+\n+  private void printHelp(String cmd) {\n+    String summary = \"hadoop fs is the command to execute fs commands. \" +\n+      \"The full syntax is: \\n\\n\" +\n+      \"hadoop fs [-fs <local | file system URI>] [-conf <configuration file>]\\n\\t\" +\n+      \"[-D <property=value>] [-ls <path>] [-lsr <path>] [-df [<path>]] [-du <path>]\\n\\t\" + \n+      \"[-dus <path>] [-mv <src> <dst>] [-cp <src> <dst>] [-rm <src>]\\n\\t\" + \n+      \"[-rmr <src>] [-put <localsrc> ... <dst>] [-copyFromLocal <localsrc> ... <dst>]\\n\\t\" +\n+      \"[-moveFromLocal <localsrc> ... <dst>] [\" + \n+      GET_SHORT_USAGE + \"\\n\\t\" +\n+      \"[-getmerge <src> <localdst> [addnl]] [-cat <src>]\\n\\t\" +\n+      \"[\" + COPYTOLOCAL_SHORT_USAGE + \"] [-moveToLocal <src> <localdst>]\\n\\t\" +\n+      \"[-mkdir <path>] [-report] [\" + SETREP_SHORT_USAGE + \"]\\n\\t\" +\n+      \"[-touchz <path>] [-test -[ezd] <path>] [-stat [format] <path>]\\n\\t\" +\n+      \"[-tail [-f] <path>] [-text <path>]\\n\\t\" +\n+      \"[\" + FsShellPermissions.CHMOD_USAGE + \"]\\n\\t\" +\n+      \"[\" + FsShellPermissions.CHOWN_USAGE + \"]\\n\\t\" +\n+      \"[\" + FsShellPermissions.CHGRP_USAGE + \"]\\n\\t\" +      \n+      \"[\" + Count.USAGE + \"]\\n\\t\" +      \n+      \"[-help [cmd]]\\n\";\n+\n+    String conf =\"-conf <configuration file>:  Specify an application configuration file.\";\n+ \n+    String D = \"-D <property=value>:  Use value for given property.\";\n+  \n+    String fs = \"-fs [local | <file system URI>]: \\tSpecify the file system to use.\\n\" + \n+      \"\\t\\tIf not specified, the current configuration is used, \\n\" +\n+      \"\\t\\ttaken from the following, in increasing precedence: \\n\" + \n+      \"\\t\\t\\tcore-default.xml inside the hadoop jar file \\n\" +\n+      \"\\t\\t\\tcore-site.xml in $HADOOP_CONF_DIR \\n\" +\n+      \"\\t\\t'local' means use the local file system as your DFS. \\n\" +\n+      \"\\t\\t<file system URI> specifies a particular file system to \\n\" +\n+      \"\\t\\tcontact. This argument is optional but if used must appear\\n\" +\n+      \"\\t\\tappear first on the command line.  Exactly one additional\\n\" +\n+      \"\\t\\targument must be specified. \\n\";\n+\n+        \n+    String ls = \"-ls <path>: \\tList the contents that match the specified file pattern. If\\n\" + \n+      \"\\t\\tpath is not specified, the contents of /user/<currentUser>\\n\" +\n+      \"\\t\\twill be listed. Directory entries are of the form \\n\" +\n+      \"\\t\\t\\tdirName (full path) <dir> \\n\" +\n+      \"\\t\\tand file entries are of the form \\n\" + \n+      \"\\t\\t\\tfileName(full path) <r n> size \\n\" +\n+      \"\\t\\twhere n is the number of replicas specified for the file \\n\" + \n+      \"\\t\\tand size is the size of the file, in bytes.\\n\";\n+\n+    String lsr = \"-lsr <path>: \\tRecursively list the contents that match the specified\\n\" +\n+      \"\\t\\tfile pattern.  Behaves very similarly to hadoop fs -ls,\\n\" + \n+      \"\\t\\texcept that the data is shown for all the entries in the\\n\" +\n+      \"\\t\\tsubtree.\\n\";\n+\n+    String df = \"-df [<path>]: \\tShows the capacity, free and used space of the filesystem.\\n\"+\n+      \"\\t\\tIf the filesystem has multiple partitions, and no path to a particular partition\\n\"+\n+      \"\\t\\tis specified, then the status of the root partitions will be shown.\\n\";\n+\n+    String du = \"-du <path>: \\tShow the amount of space, in bytes, used by the files that \\n\" +\n+      \"\\t\\tmatch the specified file pattern.  Equivalent to the unix\\n\" + \n+      \"\\t\\tcommand \\\"du -sb <path>/*\\\" in case of a directory, \\n\" +\n+      \"\\t\\tand to \\\"du -b <path>\\\" in case of a file.\\n\" +\n+      \"\\t\\tThe output is in the form \\n\" + \n+      \"\\t\\t\\tname(full path) size (in bytes)\\n\"; \n+\n+    String dus = \"-dus <path>: \\tShow the amount of space, in bytes, used by the files that \\n\" +\n+      \"\\t\\tmatch the specified file pattern.  Equivalent to the unix\\n\" + \n+      \"\\t\\tcommand \\\"du -sb\\\"  The output is in the form \\n\" + \n+      \"\\t\\t\\tname(full path) size (in bytes)\\n\"; \n+    \n+    String mv = \"-mv <src> <dst>:   Move files that match the specified file pattern <src>\\n\" +\n+      \"\\t\\tto a destination <dst>.  When moving multiple files, the \\n\" +\n+      \"\\t\\tdestination must be a directory. \\n\";\n+\n+    String cp = \"-cp <src> <dst>:   Copy files that match the file pattern <src> to a \\n\" +\n+      \"\\t\\tdestination.  When copying multiple files, the destination\\n\" +\n+      \"\\t\\tmust be a directory. \\n\";\n+\n+    String rm = \"-rm <src>: \\tDelete all files that match the specified file pattern.\\n\" +\n+      \"\\t\\tEquivlent to the Unix command \\\"rm <src>\\\"\\n\";\n+\n+    String rmr = \"-rmr <src>: \\tRemove all directories which match the specified file \\n\" +\n+      \"\\t\\tpattern. Equivlent to the Unix command \\\"rm -rf <src>\\\"\\n\";\n+\n+    String put = \"-put <localsrc> ... <dst>: \\tCopy files \" + \n+    \"from the local file system \\n\\t\\tinto fs. \\n\";\n+\n+    String copyFromLocal = \"-copyFromLocal <localsrc> ... <dst>:\" +\n+    \" Identical to the -put command.\\n\";\n+\n+    String moveFromLocal = \"-moveFromLocal <localsrc> ... <dst>:\" +\n+    \" Same as -put, except that the source is\\n\\t\\tdeleted after it's copied.\\n\"; \n+\n+    String get = GET_SHORT_USAGE\n+      + \":  Copy files that match the file pattern <src> \\n\" +\n+      \"\\t\\tto the local name.  <src> is kept.  When copying mutiple, \\n\" +\n+      \"\\t\\tfiles, the destination must be a directory. \\n\";\n+\n+    String getmerge = \"-getmerge <src> <localdst>:  Get all the files in the directories that \\n\" +\n+      \"\\t\\tmatch the source file pattern and merge and sort them to only\\n\" +\n+      \"\\t\\tone file on local fs. <src> is kept.\\n\";\n+\n+    String cat = \"-cat <src>: \\tFetch all files that match the file pattern <src> \\n\" +\n+      \"\\t\\tand display their content on stdout.\\n\";\n+\n+    \n+    String text = \"-text <src>: \\tTakes a source file and outputs the file in text format.\\n\" +\n+      \"\\t\\tThe allowed formats are zip and TextRecordInputStream.\\n\";\n+         \n+    \n+    String copyToLocal = COPYTOLOCAL_SHORT_USAGE\n+                         + \":  Identical to the -get command.\\n\";\n+\n+    String moveToLocal = \"-moveToLocal <src> <localdst>:  Not implemented yet \\n\";\n+        \n+    String mkdir = \"-mkdir <path>: \\tCreate a directory in specified location. \\n\";\n+\n+    String setrep = SETREP_SHORT_USAGE\n+      + \":  Set the replication level of a file. \\n\"\n+      + \"\\t\\tThe -R flag requests a recursive change of replication level \\n\"\n+      + \"\\t\\tfor an entire tree.\\n\";\n+\n+    String touchz = \"-touchz <path>: Write a timestamp in yyyy-MM-dd HH:mm:ss format\\n\" +\n+      \"\\t\\tin a file at <path>. An error is returned if the file exists with non-zero length\\n\";\n+\n+    String test = \"-test -[ezd] <path>: If file { exists, has zero length, is a directory\\n\" +\n+      \"\\t\\tthen return 0, else return 1.\\n\";\n+\n+    String stat = \"-stat [format] <path>: Print statistics about the file/directory at <path>\\n\" +\n+      \"\\t\\tin the specified format. Format accepts filesize in blocks (%b), filename (%n),\\n\" +\n+      \"\\t\\tblock size (%o), replication (%r), modification date (%y, %Y)\\n\";\n+\n+    String tail = TAIL_USAGE\n+      + \":  Show the last 1KB of the file. \\n\"\n+      + \"\\t\\tThe -f option shows apended data as the file grows. \\n\";\n+\n+    String chmod = FsShellPermissions.CHMOD_USAGE + \"\\n\" +\n+      \"\\t\\tChanges permissions of a file.\\n\" +\n+      \"\\t\\tThis works similar to shell's chmod with a few exceptions.\\n\\n\" +\n+      \"\\t-R\\tmodifies the files recursively. This is the only option\\n\" +\n+      \"\\t\\tcurrently supported.\\n\\n\" +\n+      \"\\tMODE\\tMode is same as mode used for chmod shell command.\\n\" +\n+      \"\\t\\tOnly letters recognized are 'rwxXt'. E.g. +t,a+r,g-w,+rwx,o=r\\n\\n\" +\n+      \"\\tOCTALMODE Mode specifed in 3 or 4 digits. If 4 digits, the first may\\n\" +\n+      \"\\tbe 1 or 0 to turn the sticky bit on or off, respectively.  Unlike \" +\n+      \"\\tshell command, it is not possible to specify only part of the mode\\n\" +\n+      \"\\t\\tE.g. 754 is same as u=rwx,g=rx,o=r\\n\\n\" +\n+      \"\\t\\tIf none of 'augo' is specified, 'a' is assumed and unlike\\n\" +\n+      \"\\t\\tshell command, no umask is applied.\\n\";\n+    \n+    String chown = FsShellPermissions.CHOWN_USAGE + \"\\n\" +\n+      \"\\t\\tChanges owner and group of a file.\\n\" +\n+      \"\\t\\tThis is similar to shell's chown with a few exceptions.\\n\\n\" +\n+      \"\\t-R\\tmodifies the files recursively. This is the only option\\n\" +\n+      \"\\t\\tcurrently supported.\\n\\n\" +\n+      \"\\t\\tIf only owner or group is specified then only owner or\\n\" +\n+      \"\\t\\tgroup is modified.\\n\\n\" +\n+      \"\\t\\tThe owner and group names may only cosists of digits, alphabet,\\n\"+\n+      \"\\t\\tand any of '-_.@/' i.e. [-_.@/a-zA-Z0-9]. The names are case\\n\" +\n+      \"\\t\\tsensitive.\\n\\n\" +\n+      \"\\t\\tWARNING: Avoid using '.' to separate user name and group though\\n\" +\n+      \"\\t\\tLinux allows it. If user names have dots in them and you are\\n\" +\n+      \"\\t\\tusing local file system, you might see surprising results since\\n\" +\n+      \"\\t\\tshell command 'chown' is used for local files.\\n\";\n+    \n+    String chgrp = FsShellPermissions.CHGRP_USAGE + \"\\n\" +\n+      \"\\t\\tThis is equivalent to -chown ... :GROUP ...\\n\";\n+    \n+    String help = \"-help [cmd]: \\tDisplays help for given command or all commands if none\\n\" +\n+      \"\\t\\tis specified.\\n\";\n+\n+    if (\"fs\".equals(cmd)) {\n+      System.out.println(fs);\n+    } else if (\"conf\".equals(cmd)) {\n+      System.out.println(conf);\n+    } else if (\"D\".equals(cmd)) {\n+      System.out.println(D);\n+    } else if (\"ls\".equals(cmd)) {\n+      System.out.println(ls);\n+    } else if (\"lsr\".equals(cmd)) {\n+      System.out.println(lsr);\n+    } else if (\"df\".equals(cmd)) {\n+      System.out.println(df);\n+    } else if (\"du\".equals(cmd)) {\n+      System.out.println(du);\n+    } else if (\"dus\".equals(cmd)) {\n+      System.out.println(dus);\n+    } else if (\"rm\".equals(cmd)) {\n+      System.out.println(rm);\n+    } else if (\"rmr\".equals(cmd)) {\n+      System.out.println(rmr);\n+    } else if (\"mkdir\".equals(cmd)) {\n+      System.out.println(mkdir);\n+    } else if (\"mv\".equals(cmd)) {\n+      System.out.println(mv);\n+    } else if (\"cp\".equals(cmd)) {\n+      System.out.println(cp);\n+    } else if (\"put\".equals(cmd)) {\n+      System.out.println(put);\n+    } else if (\"copyFromLocal\".equals(cmd)) {\n+      System.out.println(copyFromLocal);\n+    } else if (\"moveFromLocal\".equals(cmd)) {\n+      System.out.println(moveFromLocal);\n+    } else if (\"get\".equals(cmd)) {\n+      System.out.println(get);\n+    } else if (\"getmerge\".equals(cmd)) {\n+      System.out.println(getmerge);\n+    } else if (\"copyToLocal\".equals(cmd)) {\n+      System.out.println(copyToLocal);\n+    } else if (\"moveToLocal\".equals(cmd)) {\n+      System.out.println(moveToLocal);\n+    } else if (\"cat\".equals(cmd)) {\n+      System.out.println(cat);\n+    } else if (\"get\".equals(cmd)) {\n+      System.out.println(get);\n+    } else if (\"setrep\".equals(cmd)) {\n+      System.out.println(setrep);\n+    } else if (\"touchz\".equals(cmd)) {\n+      System.out.println(touchz);\n+    } else if (\"test\".equals(cmd)) {\n+      System.out.println(test);\n+    } else if (\"text\".equals(cmd)) {\n+      System.out.println(text);\n+    } else if (\"stat\".equals(cmd)) {\n+      System.out.println(stat);\n+    } else if (\"tail\".equals(cmd)) {\n+      System.out.println(tail);\n+    } else if (\"chmod\".equals(cmd)) {\n+      System.out.println(chmod);\n+    } else if (\"chown\".equals(cmd)) {\n+      System.out.println(chown);\n+    } else if (\"chgrp\".equals(cmd)) {\n+      System.out.println(chgrp);\n+    } else if (Count.matches(cmd)) {\n+      System.out.println(Count.DESCRIPTION);\n+    } else if (\"help\".equals(cmd)) {\n+      System.out.println(help);\n+    } else {\n+      System.out.println(summary);\n+      System.out.println(fs);\n+      System.out.println(ls);\n+      System.out.println(lsr);\n+      System.out.println(df);\n+      System.out.println(du);\n+      System.out.println(dus);\n+      System.out.println(mv);\n+      System.out.println(cp);\n+      System.out.println(rm);\n+      System.out.println(rmr);\n+      System.out.println(put);\n+      System.out.println(copyFromLocal);\n+      System.out.println(moveFromLocal);\n+      System.out.println(get);\n+      System.out.println(getmerge);\n+      System.out.println(cat);\n+      System.out.println(copyToLocal);\n+      System.out.println(moveToLocal);\n+      System.out.println(mkdir);\n+      System.out.println(setrep);\n+      System.out.println(tail);\n+      System.out.println(touchz);\n+      System.out.println(test);\n+      System.out.println(text);\n+      System.out.println(stat);\n+      System.out.println(chmod);\n+      System.out.println(chown);      \n+      System.out.println(chgrp);\n+      System.out.println(Count.DESCRIPTION);\n+      System.out.println(help);\n+    }        \n+\n+                           \n+  }\n+\n+  /**\n+   * Apply operation specified by 'cmd' on all parameters\n+   * starting from argv[startindex].\n+   */\n+  private int doall(String cmd, String argv[], int startindex) {\n+    int exitCode = 0;\n+    int i = startindex;\n+    //\n+    // for each source file, issue the command\n+    //\n+    for (; i < argv.length; i++) {\n+      try {\n+        //\n+        // issue the command to the fs\n+        //\n+        if (\"-cat\".equals(cmd)) {\n+          cat(argv[i], true);\n+        } else if (\"-mkdir\".equals(cmd)) {\n+          mkdir(argv[i]);\n+        } else if (\"-rm\".equals(cmd)) {\n+          delete(argv[i], false);\n+        } else if (\"-rmr\".equals(cmd)) {\n+          delete(argv[i], true);\n+        } else if (\"-df\".equals(cmd)) {\n+          df(argv[i]);\n+        } else if (\"-du\".equals(cmd)) {\n+          du(argv[i]);\n+        } else if (\"-dus\".equals(cmd)) {\n+          dus(argv[i]);\n+        } else if (Count.matches(cmd)) {\n+          new Count(argv, i, getConf()).runAll();\n+        } else if (\"-ls\".equals(cmd)) {\n+          exitCode = ls(argv[i], false);\n+        } else if (\"-lsr\".equals(cmd)) {\n+          exitCode = ls(argv[i], true);\n+        } else if (\"-touchz\".equals(cmd)) {\n+          touchz(argv[i]);\n+        } else if (\"-text\".equals(cmd)) {\n+          text(argv[i]);\n+        }\n+      } catch (RemoteException e) {\n+        //\n+        // This is a error returned by hadoop server. Print\n+        // out the first line of the error message.\n+        //\n+        exitCode = -1;\n+        try {\n+          String[] content;\n+          content = e.getLocalizedMessage().split(\"\\n\");\n+          System.err.println(cmd.substring(1) + \": \" +\n+                             content[0]);\n+        } catch (Exception ex) {\n+          System.err.println(cmd.substring(1) + \": \" +\n+                             ex.getLocalizedMessage());\n+        }\n+      } catch (IOException e) {\n+        //\n+        // IO exception encountered locally.\n+        //\n+        exitCode = -1;\n+        String content = e.getLocalizedMessage();\n+        if (content != null) {\n+          content = content.split(\"\\n\")[0];\n+        }\n+        System.err.println(cmd.substring(1) + \": \" +\n+                          content);\n+      }\n+    }\n+    return exitCode;\n+  }\n+\n+  /**\n+   * Displays format of commands.\n+   * \n+   */\n+  private static void printUsage(String cmd) {\n+    String prefix = \"Usage: java \" + FsShell.class.getSimpleName();\n+    if (\"-fs\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell\" + \n+                         \" [-fs <local | file system URI>]\");\n+    } else if (\"-conf\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell\" + \n+                         \" [-conf <configuration file>]\");\n+    } else if (\"-D\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell\" + \n+                         \" [-D <[property=value>]\");\n+    } else if (\"-ls\".equals(cmd) || \"-lsr\".equals(cmd) ||\n+               \"-du\".equals(cmd) || \"-dus\".equals(cmd) ||\n+               \"-rm\".equals(cmd) || \"-rmr\".equals(cmd) ||\n+               \"-touchz\".equals(cmd) || \"-mkdir\".equals(cmd) ||\n+               \"-text\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell\" + \n+                         \" [\" + cmd + \" <path>]\");\n+    } else if (\"-df\".equals(cmd) ) {\n+      System.err.println(\"Usage: java FsShell\" +\n+                         \" [\" + cmd + \" [<path>]]\");\n+    } else if (Count.matches(cmd)) {\n+      System.err.println(prefix + \" [\" + Count.USAGE + \"]\");\n+    } else if (\"-mv\".equals(cmd) || \"-cp\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell\" + \n+                         \" [\" + cmd + \" <src> <dst>]\");\n+    } else if (\"-put\".equals(cmd) || \"-copyFromLocal\".equals(cmd) ||\n+               \"-moveFromLocal\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell\" + \n+                         \" [\" + cmd + \" <localsrc> ... <dst>]\");\n+    } else if (\"-get\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell [\" + GET_SHORT_USAGE + \"]\"); \n+    } else if (\"-copyToLocal\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell [\" + COPYTOLOCAL_SHORT_USAGE+ \"]\"); \n+    } else if (\"-moveToLocal\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell\" + \n+                         \" [\" + cmd + \" [-crc] <src> <localdst>]\");\n+    } else if (\"-cat\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell\" + \n+                         \" [\" + cmd + \" <src>]\");\n+    } else if (\"-setrep\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell [\" + SETREP_SHORT_USAGE + \"]\");\n+    } else if (\"-test\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell\" +\n+                         \" [-test -[ezd] <path>]\");\n+    } else if (\"-stat\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell\" +\n+                         \" [-stat [format] <path>]\");\n+    } else if (\"-tail\".equals(cmd)) {\n+      System.err.println(\"Usage: java FsShell [\" + TAIL_USAGE + \"]\");\n+    } else {\n+      System.err.println(\"Usage: java FsShell\");\n+      System.err.println(\"           [-ls <path>]\");\n+      System.err.println(\"           [-lsr <path>]\");\n+      System.err.println(\"           [-df [<path>]]\");\n+      System.err.println(\"           [-du <path>]\");\n+      System.err.println(\"           [-dus <path>]\");\n+      System.err.println(\"           [\" + Count.USAGE + \"]\");\n+      System.err.println(\"           [-mv <src> <dst>]\");\n+      System.err.println(\"           [-cp <src> <dst>]\");\n+      System.err.println(\"           [-rm <path>]\");\n+      System.err.println(\"           [-rmr <path>]\");\n+      System.err.println(\"           [-expunge]\");\n+      System.err.println(\"           [-put <localsrc> ... <dst>]\");\n+      System.err.println(\"           [-copyFromLocal <localsrc> ... <dst>]\");\n+      System.err.println(\"           [-moveFromLocal <localsrc> ... <dst>]\");\n+      System.err.println(\"           [\" + GET_SHORT_USAGE + \"]\");\n+      System.err.println(\"           [-getmerge <src> <localdst> [addnl]]\");\n+      System.err.println(\"           [-cat <src>]\");\n+      System.err.println(\"           [-text <src>]\");\n+      System.err.println(\"           [\" + COPYTOLOCAL_SHORT_USAGE + \"]\");\n+      System.err.println(\"           [-moveToLocal [-crc] <src> <localdst>]\");\n+      System.err.println(\"           [-mkdir <path>]\");\n+      System.err.println(\"           [\" + SETREP_SHORT_USAGE + \"]\");\n+      System.err.println(\"           [-touchz <path>]\");\n+      System.err.println(\"           [-test -[ezd] <path>]\");\n+      System.err.println(\"           [-stat [format] <path>]\");\n+      System.err.println(\"           [\" + TAIL_USAGE + \"]\");\n+      System.err.println(\"           [\" + FsShellPermissions.CHMOD_USAGE + \"]\");      \n+      System.err.println(\"           [\" + FsShellPermissions.CHOWN_USAGE + \"]\");\n+      System.err.println(\"           [\" + FsShellPermissions.CHGRP_USAGE + \"]\");\n+      System.err.println(\"           [-help [cmd]]\");\n+      System.err.println();\n+      ToolRunner.printGenericCommandUsage(System.err);\n+    }\n+  }\n+\n+  /**\n+   * run\n+   */\n+  public int run(String argv[]) throws Exception {\n+\n+    if (argv.length < 1) {\n+      printUsage(\"\"); \n+      return -1;\n+    }\n+\n+    int exitCode = -1;\n+    int i = 0;\n+    String cmd = argv[i++];\n+    //\n+    // verify that we have enough command line parameters\n+    //\n+    if (\"-put\".equals(cmd) || \"-test\".equals(cmd) ||\n+        \"-copyFromLocal\".equals(cmd) || \"-moveFromLocal\".equals(cmd)) {\n+      if (argv.length < 3) {\n+        printUsage(cmd);\n+        return exitCode;\n+      }\n+    } else if (\"-get\".equals(cmd) || \n+               \"-copyToLocal\".equals(cmd) || \"-moveToLocal\".equals(cmd)) {\n+      if (argv.length < 3) {\n+        printUsage(cmd);\n+        return exitCode;\n+      }\n+    } else if (\"-mv\".equals(cmd) || \"-cp\".equals(cmd)) {\n+      if (argv.length < 3) {\n+        printUsage(cmd);\n+        return exitCode;\n+      }\n+    } else if (\"-rm\".equals(cmd) || \"-rmr\".equals(cmd) ||\n+               \"-cat\".equals(cmd) || \"-mkdir\".equals(cmd) ||\n+               \"-touchz\".equals(cmd) || \"-stat\".equals(cmd) ||\n+               \"-text\".equals(cmd)) {\n+      if (argv.length < 2) {\n+        printUsage(cmd);\n+        return exitCode;\n+      }\n+    }\n+    // initialize FsShell\n+    try {\n+      init();\n+    } catch (RPC.VersionMismatch v) { \n+      System.err.println(\"Version Mismatch between client and server\" +\n+                         \"... command aborted.\");\n+      return exitCode;\n+    } catch (IOException e) {\n+      System.err.println(\"Bad connection to FS. command aborted.\");\n+      return exitCode;\n+    }\n+\n+    exitCode = 0;\n+    try {\n+      if (\"-put\".equals(cmd) || \"-copyFromLocal\".equals(cmd)) {\n+        Path[] srcs = new Path[argv.length-2];\n+        for (int j=0 ; i < argv.length-1 ;) \n+          srcs[j++] = new Path(argv[i++]);\n+        copyFromLocal(srcs, argv[i++]);\n+      } else if (\"-moveFromLocal\".equals(cmd)) {\n+        Path[] srcs = new Path[argv.length-2];\n+        for (int j=0 ; i < argv.length-1 ;) \n+          srcs[j++] = new Path(argv[i++]);\n+        moveFromLocal(srcs, argv[i++]);\n+      } else if (\"-get\".equals(cmd) || \"-copyToLocal\".equals(cmd)) {\n+        copyToLocal(argv, i);\n+      } else if (\"-getmerge\".equals(cmd)) {\n+        if (argv.length>i+2)\n+          copyMergeToLocal(argv[i++], new Path(argv[i++]), Boolean.parseBoolean(argv[i++]));\n+        else\n+          copyMergeToLocal(argv[i++], new Path(argv[i++]));\n+      } else if (\"-cat\".equals(cmd)) {\n+        exitCode = doall(cmd, argv, i);\n+      } else if (\"-text\".equals(cmd)) {\n+        exitCode = doall(cmd, argv, i);\n+      } else if (\"-moveToLocal\".equals(cmd)) {\n+        moveToLocal(argv[i++], new Path(argv[i++]));\n+      } else if (\"-setrep\".equals(cmd)) {\n+        setReplication(argv, i);           \n+      } else if (\"-chmod\".equals(cmd) || \n+                 \"-chown\".equals(cmd) ||\n+                 \"-chgrp\".equals(cmd)) {\n+        FsShellPermissions.changePermissions(fs, cmd, argv, i, this);\n+      } else if (\"-ls\".equals(cmd)) {\n+        if (i < argv.length) {\n+          exitCode = doall(cmd, argv, i);\n+        } else {\n+          exitCode = ls(Path.CUR_DIR, false);\n+        } \n+      } else if (\"-lsr\".equals(cmd)) {\n+        if (i < argv.length) {\n+          exitCode = doall(cmd, argv, i);\n+        } else {\n+          exitCode = ls(Path.CUR_DIR, true);\n+        } \n+      } else if (\"-mv\".equals(cmd)) {\n+        exitCode = rename(argv, getConf());\n+      } else if (\"-cp\".equals(cmd)) {\n+        exitCode = copy(argv, getConf());\n+      } else if (\"-rm\".equals(cmd)) {\n+        exitCode = doall(cmd, argv, i);\n+      } else if (\"-rmr\".equals(cmd)) {\n+        exitCode = doall(cmd, argv, i);\n+      } else if (\"-expunge\".equals(cmd)) {\n+        expunge();\n+      } else if (\"-df\".equals(cmd)) {\n+        if (argv.length-1 > 0) {\n+          exitCode = doall(cmd, argv, i);\n+        } else {\n+          df(null);\n+        }\n+      } else if (\"-du\".equals(cmd)) {\n+        if (i < argv.length) {\n+          exitCode = doall(cmd, argv, i);\n+        } else {\n+          du(\".\");\n+        }\n+      } else if (\"-dus\".equals(cmd)) {\n+        if (i < argv.length) {\n+          exitCode = doall(cmd, argv, i);\n+        } else {\n+          dus(\".\");\n+        }         \n+      } else if (Count.matches(cmd)) {\n+        exitCode = new Count(argv, i, getConf()).runAll();\n+      } else if (\"-mkdir\".equals(cmd)) {\n+        exitCode = doall(cmd, argv, i);\n+      } else if (\"-touchz\".equals(cmd)) {\n+        exitCode = doall(cmd, argv, i);\n+      } else if (\"-test\".equals(cmd)) {\n+        exitCode = test(argv, i);\n+      } else if (\"-stat\".equals(cmd)) {\n+        if (i + 1 < argv.length) {\n+          stat(argv[i++].toCharArray(), argv[i++]);\n+        } else {\n+          stat(\"%y\".toCharArray(), argv[i]);\n+        }\n+      } else if (\"-help\".equals(cmd)) {\n+        if (i < argv.length) {\n+          printHelp(argv[i]);\n+        } else {\n+          printHelp(\"\");\n+        }\n+      } else if (\"-tail\".equals(cmd)) {\n+        tail(argv, i);           \n+      } else {\n+        exitCode = -1;\n+        System.err.println(cmd.substring(1) + \": Unknown command\");\n+        printUsage(\"\");\n+      }\n+    } catch (IllegalArgumentException arge) {\n+      exitCode = -1;\n+      System.err.println(cmd.substring(1) + \": \" + arge.getLocalizedMessage());\n+      printUsage(cmd);\n+    } catch (RemoteException e) {\n+      //\n+      // This is a error returned by hadoop server. Print\n+      // out the first line of the error mesage, ignore the stack trace.\n+      exitCode = -1;\n+      try {\n+        String[] content;\n+        content = e.getLocalizedMessage().split(\"\\n\");\n+        System.err.println(cmd.substring(1) + \": \" + \n+                           content[0]);\n+      } catch (Exception ex) {\n+        System.err.println(cmd.substring(1) + \": \" + \n+                           ex.getLocalizedMessage());  \n+      }\n+    } catch (IOException e) {\n+      //\n+      // IO exception encountered locally.\n+      // \n+      exitCode = -1;\n+      System.err.println(cmd.substring(1) + \": \" + \n+                         e.getLocalizedMessage());  \n+    } catch (Exception re) {\n+      exitCode = -1;\n+      System.err.println(cmd.substring(1) + \": \" + re.getLocalizedMessage());  \n+    } finally {\n+    }\n+    return exitCode;\n+  }\n+\n+  public void close() throws IOException {\n+    if (fs != null) {\n+      fs.close();\n+      fs = null;\n+    }\n+  }\n+\n+  /**\n+   * main() has some simple utility methods\n+   */\n+  public static void main(String argv[]) throws Exception {\n+    FsShell shell = new FsShell();\n+    int res;\n+    try {\n+      res = ToolRunner.run(shell, argv);\n+    } finally {\n+      shell.close();\n+    }\n+    System.exit(res);\n+  }\n+\n+  /**\n+   * Accumulate exceptions if there is any.  Throw them at last.\n+   */\n+  private abstract class DelayedExceptionThrowing {\n+    abstract void process(Path p, FileSystem srcFs) throws IOException;\n+\n+    final void globAndProcess(Path srcPattern, FileSystem srcFs\n+        ) throws IOException {\n+      List<IOException> exceptions = new ArrayList<IOException>();\n+      for(Path p : FileUtil.stat2Paths(srcFs.globStatus(srcPattern), \n+                                       srcPattern))\n+        try { process(p, srcFs); } \n+        catch(IOException ioe) { exceptions.add(ioe); }\n+    \n+      if (!exceptions.isEmpty())\n+        if (exceptions.size() == 1)\n+          throw exceptions.get(0);\n+        else \n+          throw new IOException(\"Multiple IOExceptions: \" + exceptions);\n+    }\n+  }\n+}"
        },
        {
            "sha": "27997c7e7a8af7bd64f7aca3c798876f19d70ade",
            "filename": "src/java/org/apache/hadoop/fs/FsShellPermissions.java",
            "status": "added",
            "additions": 315,
            "deletions": 0,
            "changes": 315,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsShellPermissions.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsShellPermissions.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsShellPermissions.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,315 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.IOException;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import org.apache.hadoop.fs.FsShell.CmdHandler;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+\n+\n+/**\n+ * This class is the home for file permissions related commands.\n+ * Moved to this separate class since FsShell is getting too large.\n+ */\n+class FsShellPermissions {\n+  \n+  /*========== chmod ==========*/\n+\n+  /*\n+   * The pattern is almost as flexible as mode allowed by chmod shell command.\n+   * The main restriction is that we recognize only rwxXt. To reduce errors we\n+   * also enforce octal mode specifications of either 3 digits without a sticky\n+   * bit setting or four digits with a sticky bit setting.\n+   */\n+  private static Pattern chmodNormalPattern =\n+   Pattern.compile(\"\\\\G\\\\s*([ugoa]*)([+=-]+)([rwxXt]+)([,\\\\s]*)\\\\s*\");\n+  private static Pattern chmodOctalPattern =\n+            Pattern.compile(\"^\\\\s*[+]?([01]?)([0-7]{3})\\\\s*$\");\n+\n+  static String CHMOD_USAGE =\n+                            \"-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...\";\n+\n+  private static class ChmodHandler extends CmdHandler {\n+\n+    private short userMode;\n+    private short groupMode;\n+    private short othersMode;\n+    private short stickyMode;\n+    private char userType = '+';\n+    private char groupType = '+';\n+    private char othersType = '+';\n+    private char stickyBitType = '+';\n+\n+    private void applyNormalPattern(String modeStr, Matcher matcher)\n+                                    throws IOException {\n+      // Are there multiple permissions stored in one chmod?\n+      boolean commaSeperated = false;\n+\n+      for(int i=0; i < 1 || matcher.end() < modeStr.length(); i++) {\n+        if (i>0 && (!commaSeperated || !matcher.find())) {\n+          patternError(modeStr);\n+        }\n+\n+        /* groups : 1 : [ugoa]*\n+         *          2 : [+-=]\n+         *          3 : [rwxXt]+\n+         *          4 : [,\\s]*\n+         */\n+\n+        String str = matcher.group(2);\n+        char type = str.charAt(str.length() - 1);\n+\n+        boolean user, group, others, stickyBit;\n+        user = group = others = stickyBit = false;\n+\n+        for(char c : matcher.group(1).toCharArray()) {\n+          switch (c) {\n+          case 'u' : user = true; break;\n+          case 'g' : group = true; break;\n+          case 'o' : others = true; break;\n+          case 'a' : break;\n+          default  : throw new RuntimeException(\"Unexpected\");          \n+          }\n+        }\n+\n+        if (!(user || group || others)) { // same as specifying 'a'\n+          user = group = others = true;\n+        }\n+\n+        short mode = 0;\n+\n+        for(char c : matcher.group(3).toCharArray()) {\n+          switch (c) {\n+          case 'r' : mode |= 4; break;\n+          case 'w' : mode |= 2; break;\n+          case 'x' : mode |= 1; break;\n+          case 'X' : mode |= 8; break;\n+          case 't' : stickyBit = true; break;\n+          default  : throw new RuntimeException(\"Unexpected\");\n+          }\n+        }\n+\n+        if ( user ) {\n+          userMode = mode;\n+          userType = type;\n+        }\n+\n+        if ( group ) {\n+          groupMode = mode;\n+          groupType = type;\n+        }\n+\n+        if ( others ) {\n+          othersMode = mode;\n+          othersType = type;\n+          \n+          stickyMode = (short) (stickyBit ? 1 : 0);\n+          stickyBitType = type;\n+        }\n+\n+        commaSeperated = matcher.group(4).contains(\",\");\n+      }\n+    }\n+\n+    private void applyOctalPattern(String modeStr, Matcher matcher) {\n+      userType = groupType = othersType = '=';\n+\n+      // Check if sticky bit is specified\n+      String sb = matcher.group(1);\n+      if(!sb.isEmpty()) {\n+        stickyMode = Short.valueOf(sb.substring(0, 1));\n+        stickyBitType = '=';\n+      }\n+\n+      String str = matcher.group(2);\n+      userMode = Short.valueOf(str.substring(0, 1));\n+      groupMode = Short.valueOf(str.substring(1, 2));\n+      othersMode = Short.valueOf(str.substring(2, 3));      \n+    }\n+\n+    private void patternError(String mode) throws IOException {\n+      throw new IOException(\"chmod : mode '\" + mode + \n+                            \"' does not match the expected pattern.\");      \n+    }\n+\n+    ChmodHandler(FileSystem fs, String modeStr) throws IOException {\n+      super(\"chmod\", fs);\n+      Matcher matcher = null;\n+\n+      if ((matcher = chmodNormalPattern.matcher(modeStr)).find()) {\n+        applyNormalPattern(modeStr, matcher);\n+      } else if ((matcher = chmodOctalPattern.matcher(modeStr)).matches()) {\n+        applyOctalPattern(modeStr, matcher);\n+      } else {\n+        patternError(modeStr);\n+      }\n+    }\n+\n+    private int applyChmod(char type, int mode, int existing, boolean exeOk) {\n+      boolean capX = false;\n+\n+      if ((mode&8) != 0) { // convert X to x;\n+        capX = true;\n+        mode &= ~8;\n+        mode |= 1;\n+      }\n+\n+      switch (type) {\n+      case '+' : mode = mode | existing; break;\n+      case '-' : mode = (~mode) & existing; break;\n+      case '=' : break;\n+      default  : throw new RuntimeException(\"Unexpected\");      \n+      }\n+\n+      // if X is specified add 'x' only if exeOk or x was already set.\n+      if (capX && !exeOk && (mode&1) != 0 && (existing&1) == 0) {\n+        mode &= ~1; // remove x\n+      }\n+\n+      return mode;\n+    }\n+\n+    @Override\n+    public void run(FileStatus file, FileSystem srcFs) throws IOException {\n+      FsPermission perms = file.getPermission();\n+      int existing = perms.toShort();\n+      boolean exeOk = file.isDir() || (existing & 0111) != 0;\n+      int newperms = ( applyChmod(stickyBitType, stickyMode,\n+                             (existing>>>9), false) << 9 |\n+                       applyChmod(userType, userMode,\n+                             (existing>>>6)&7, exeOk) << 6 |\n+                       applyChmod(groupType, groupMode,\n+                             (existing>>>3)&7, exeOk) << 3 |\n+                       applyChmod(othersType, othersMode, existing&7, exeOk));\n+\n+      if (existing != newperms) {\n+        try {\n+          srcFs.setPermission(file.getPath(), \n+                                new FsPermission((short)newperms));\n+        } catch (IOException e) {\n+          System.err.println(getName() + \": changing permissions of '\" + \n+                             file.getPath() + \"':\" + e.getMessage());\n+        }\n+      }\n+    }\n+  }\n+\n+  /*========== chown ==========*/\n+  \n+  static private String allowedChars = \"[-_./@a-zA-Z0-9]\";\n+  ///allows only \"allowedChars\" above in names for owner and group\n+  static private Pattern chownPattern = \n+         Pattern.compile(\"^\\\\s*(\" + allowedChars + \"+)?\" +\n+                          \"([:](\" + allowedChars + \"*))?\\\\s*$\");\n+  static private Pattern chgrpPattern = \n+         Pattern.compile(\"^\\\\s*(\" + allowedChars + \"+)\\\\s*$\");\n+  \n+  static String CHOWN_USAGE = \"-chown [-R] [OWNER][:[GROUP]] PATH...\";\n+  static String CHGRP_USAGE = \"-chgrp [-R] GROUP PATH...\";  \n+\n+  private static class ChownHandler extends CmdHandler {\n+    protected String owner = null;\n+    protected String group = null;\n+\n+    protected ChownHandler(String cmd, FileSystem fs) { //for chgrp\n+      super(cmd, fs);\n+    }\n+\n+    ChownHandler(FileSystem fs, String ownerStr) throws IOException {\n+      super(\"chown\", fs);\n+      Matcher matcher = chownPattern.matcher(ownerStr);\n+      if (!matcher.matches()) {\n+        throw new IOException(\"'\" + ownerStr + \"' does not match \" +\n+                              \"expected pattern for [owner][:group].\");\n+      }\n+      owner = matcher.group(1);\n+      group = matcher.group(3);\n+      if (group != null && group.length() == 0) {\n+        group = null;\n+      }\n+      if (owner == null && group == null) {\n+        throw new IOException(\"'\" + ownerStr + \"' does not specify \" +\n+                              \" onwer or group.\");\n+      }\n+    }\n+\n+    @Override\n+    public void run(FileStatus file, FileSystem srcFs) throws IOException {\n+      //Should we do case insensitive match?  \n+      String newOwner = (owner == null || owner.equals(file.getOwner())) ?\n+                        null : owner;\n+      String newGroup = (group == null || group.equals(file.getGroup())) ?\n+                        null : group;\n+\n+      if (newOwner != null || newGroup != null) {\n+        try {\n+          srcFs.setOwner(file.getPath(), newOwner, newGroup);\n+        } catch (IOException e) {\n+          System.err.println(getName() + \": changing ownership of '\" + \n+                             file.getPath() + \"':\" + e.getMessage());\n+\n+        }\n+      }\n+    }\n+  }\n+\n+  /*========== chgrp ==========*/    \n+  \n+  private static class ChgrpHandler extends ChownHandler {\n+    ChgrpHandler(FileSystem fs, String groupStr) throws IOException {\n+      super(\"chgrp\", fs);\n+\n+      Matcher matcher = chgrpPattern.matcher(groupStr);\n+      if (!matcher.matches()) {\n+        throw new IOException(\"'\" + groupStr + \"' does not match \" +\n+        \"expected pattern for group\");\n+      }\n+      group = matcher.group(1);\n+    }\n+  }\n+\n+  static void changePermissions(FileSystem fs, String cmd, \n+                                String argv[], int startIndex, FsShell shell)\n+                                throws IOException {\n+    CmdHandler handler = null;\n+    boolean recursive = false;\n+\n+    // handle common arguments, currently only \"-R\" \n+    for (; startIndex < argv.length && argv[startIndex].equals(\"-R\"); \n+    startIndex++) {\n+      recursive = true;\n+    }\n+\n+    if ( startIndex >= argv.length ) {\n+      throw new IOException(\"Not enough arguments for the command\");\n+    }\n+\n+    if (cmd.equals(\"-chmod\")) {\n+      handler = new ChmodHandler(fs, argv[startIndex++]);\n+    } else if (cmd.equals(\"-chown\")) {\n+      handler = new ChownHandler(fs, argv[startIndex++]);\n+    } else if (cmd.equals(\"-chgrp\")) {\n+      handler = new ChgrpHandler(fs, argv[startIndex++]);\n+    }\n+\n+    shell.runCmdHandler(handler, argv, startIndex, recursive);\n+  } \n+}"
        },
        {
            "sha": "0c7a5ac57472f15c83469e9232ab028e8e9ef8db",
            "filename": "src/java/org/apache/hadoop/fs/FsStatus.java",
            "status": "added",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsStatus.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsStatus.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsStatus.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,70 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.io.Writable;\n+\n+/** This class is used to represent the capacity, free and used space on a\n+  * {@link FileSystem}.\n+  */\n+public class FsStatus implements Writable {\n+  private long capacity;\n+  private long used;\n+  private long remaining;\n+\n+  /** Construct a FsStatus object, using the specified statistics */\n+  public FsStatus(long capacity, long used, long remaining) {\n+    this.capacity = capacity;\n+    this.used = used;\n+    this.remaining = remaining;\n+  }\n+\n+  /** Return the capacity in bytes of the file system */\n+  public long getCapacity() {\n+    return capacity;\n+  }\n+\n+  /** Return the number of bytes used on the file system */\n+  public long getUsed() {\n+    return used;\n+  }\n+\n+  /** Return the number of remaining bytes on the file system */\n+  public long getRemaining() {\n+    return remaining;\n+  }\n+\n+  //////////////////////////////////////////////////\n+  // Writable\n+  //////////////////////////////////////////////////\n+  public void write(DataOutput out) throws IOException {\n+    out.writeLong(capacity);\n+    out.writeLong(used);\n+    out.writeLong(remaining);\n+  }\n+\n+  public void readFields(DataInput in) throws IOException {\n+    capacity = in.readLong();\n+    used = in.readLong();\n+    remaining = in.readLong();\n+  }\n+}"
        },
        {
            "sha": "c919b8b4047569d25e1c5537bc86089d9facf8e6",
            "filename": "src/java/org/apache/hadoop/fs/FsUrlConnection.java",
            "status": "added",
            "additions": 61,
            "deletions": 0,
            "changes": 61,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsUrlConnection.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsUrlConnection.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsUrlConnection.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,61 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URISyntaxException;\n+import java.net.URL;\n+import java.net.URLConnection;\n+\n+import org.apache.hadoop.conf.Configuration;\n+\n+/**\n+ * Representation of a URL connection to open InputStreams.\n+ */\n+class FsUrlConnection extends URLConnection {\n+\n+  private Configuration conf;\n+\n+  private InputStream is;\n+\n+  FsUrlConnection(Configuration conf, URL url) {\n+    super(url);\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public void connect() throws IOException {\n+    try {\n+      FileSystem fs = FileSystem.get(url.toURI(), conf);\n+      is = fs.open(new Path(url.getPath()));\n+    } catch (URISyntaxException e) {\n+      throw new IOException(e.toString());\n+    }\n+  }\n+\n+  /* @inheritDoc */\n+  @Override\n+  public InputStream getInputStream() throws IOException {\n+    if (is == null) {\n+      connect();\n+    }\n+    return is;\n+  }\n+\n+}"
        },
        {
            "sha": "37c6fcf4807f3df78db3942cbdf190a6cbd3ec5a",
            "filename": "src/java/org/apache/hadoop/fs/FsUrlStreamHandler.java",
            "status": "added",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsUrlStreamHandler.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsUrlStreamHandler.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsUrlStreamHandler.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,47 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.IOException;\n+import java.net.URL;\n+import java.net.URLStreamHandler;\n+\n+import org.apache.hadoop.conf.Configuration;\n+\n+/**\n+ * URLStream handler relying on FileSystem and on a given Configuration to\n+ * handle URL protocols.\n+ */\n+class FsUrlStreamHandler extends URLStreamHandler {\n+\n+  private Configuration conf;\n+\n+  FsUrlStreamHandler(Configuration conf) {\n+    this.conf = conf;\n+  }\n+\n+  FsUrlStreamHandler() {\n+    this.conf = new Configuration();\n+  }\n+\n+  @Override\n+  protected FsUrlConnection openConnection(URL url) throws IOException {\n+    return new FsUrlConnection(conf, url);\n+  }\n+\n+}"
        },
        {
            "sha": "624d7050b9346cc60fd7ba27560d91f8a0cfa2de",
            "filename": "src/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java",
            "status": "added",
            "additions": 78,
            "deletions": 0,
            "changes": 78,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsUrlStreamHandlerFactory.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsUrlStreamHandlerFactory.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FFsUrlStreamHandlerFactory.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,78 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.net.URLStreamHandlerFactory;\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.apache.hadoop.conf.Configuration;\n+\n+/**\n+ * Factory for URL stream handlers.\n+ * \n+ * There is only one handler whose job is to create UrlConnections. A\n+ * FsUrlConnection relies on FileSystem to choose the appropriate FS\n+ * implementation.\n+ * \n+ * Before returning our handler, we make sure that FileSystem knows an\n+ * implementation for the requested scheme/protocol.\n+ */\n+public class FsUrlStreamHandlerFactory implements\n+    URLStreamHandlerFactory {\n+\n+  // The configuration holds supported FS implementation class names.\n+  private Configuration conf;\n+\n+  // This map stores whether a protocol is know or not by FileSystem\n+  private Map<String, Boolean> protocols = new HashMap<String, Boolean>();\n+\n+  // The URL Stream handler\n+  private java.net.URLStreamHandler handler;\n+\n+  public FsUrlStreamHandlerFactory() {\n+    this.conf = new Configuration();\n+    // force the resolution of the configuration files\n+    // this is required if we want the factory to be able to handle\n+    // file:// URLs\n+    this.conf.getClass(\"fs.file.impl\", null);\n+    this.handler = new FsUrlStreamHandler(this.conf);\n+  }\n+\n+  public FsUrlStreamHandlerFactory(Configuration conf) {\n+    this.conf = new Configuration(conf);\n+    // force the resolution of the configuration files\n+    this.conf.getClass(\"fs.file.impl\", null);\n+    this.handler = new FsUrlStreamHandler(this.conf);\n+  }\n+\n+  public java.net.URLStreamHandler createURLStreamHandler(String protocol) {\n+    if (!protocols.containsKey(protocol)) {\n+      boolean known =\n+          (conf.getClass(\"fs.\" + protocol + \".impl\", null) != null);\n+      protocols.put(protocol, known);\n+    }\n+    if (protocols.get(protocol)) {\n+      return handler;\n+    } else {\n+      // FileSystem does not know the protocol, let the VM handle this\n+      return null;\n+    }\n+  }\n+\n+}"
        },
        {
            "sha": "bc9b27674e0b1b0a5152f18672119845db59c0f8",
            "filename": "src/java/org/apache/hadoop/fs/GlobExpander.java",
            "status": "added",
            "additions": 166,
            "deletions": 0,
            "changes": 166,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FGlobExpander.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FGlobExpander.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FGlobExpander.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,166 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+class GlobExpander {\n+  \n+  static class StringWithOffset {\n+    String string;\n+    int offset;\n+    public StringWithOffset(String string, int offset) {\n+      super();\n+      this.string = string;\n+      this.offset = offset;\n+    }\n+  }\n+  \n+  /**\n+   * Expand globs in the given <code>filePattern</code> into a collection of \n+   * file patterns so that in the expanded set no file pattern has a\n+   * slash character (\"/\") in a curly bracket pair.\n+   * @param filePattern\n+   * @return expanded file patterns\n+   * @throws IOException \n+   */\n+  public static List<String> expand(String filePattern) throws IOException {\n+    List<String> fullyExpanded = new ArrayList<String>();\n+    List<StringWithOffset> toExpand = new ArrayList<StringWithOffset>();\n+    toExpand.add(new StringWithOffset(filePattern, 0));\n+    while (!toExpand.isEmpty()) {\n+      StringWithOffset path = toExpand.remove(0);\n+      List<StringWithOffset> expanded = expandLeftmost(path);\n+      if (expanded == null) {\n+        fullyExpanded.add(path.string);\n+      } else {\n+        toExpand.addAll(0, expanded);\n+      }\n+    }\n+    return fullyExpanded;\n+  }\n+  \n+  /**\n+   * Expand the leftmost outer curly bracket pair containing a\n+   * slash character (\"/\") in <code>filePattern</code>.\n+   * @param filePattern\n+   * @return expanded file patterns\n+   * @throws IOException \n+   */\n+  private static List<StringWithOffset> expandLeftmost(StringWithOffset\n+      filePatternWithOffset) throws IOException {\n+    \n+    String filePattern = filePatternWithOffset.string;\n+    int leftmost = leftmostOuterCurlyContainingSlash(filePattern,\n+        filePatternWithOffset.offset);\n+    if (leftmost == -1) {\n+      return null;\n+    }\n+    int curlyOpen = 0;\n+    StringBuilder prefix = new StringBuilder(filePattern.substring(0, leftmost));\n+    StringBuilder suffix = new StringBuilder();\n+    List<String> alts = new ArrayList<String>();\n+    StringBuilder alt = new StringBuilder();\n+    StringBuilder cur = prefix;\n+    for (int i = leftmost; i < filePattern.length(); i++) {\n+      char c = filePattern.charAt(i);\n+      if (cur == suffix) {\n+        cur.append(c);\n+      } else if (c == '\\\\') {\n+        i++;\n+        if (i >= filePattern.length()) {\n+          throw new IOException(\"Illegal file pattern: \"\n+              + \"An escaped character does not present for glob \"\n+              + filePattern + \" at \" + i);\n+        }\n+        c = filePattern.charAt(i);\n+        cur.append(c);\n+      } else if (c == '{') {\n+        if (curlyOpen++ == 0) {\n+          alt.setLength(0);\n+          cur = alt;\n+        } else {\n+          cur.append(c);\n+        }\n+\n+      } else if (c == '}' && curlyOpen > 0) {\n+        if (--curlyOpen == 0) {\n+          alts.add(alt.toString());\n+          alt.setLength(0);\n+          cur = suffix;\n+        } else {\n+          cur.append(c);\n+        }\n+      } else if (c == ',') {\n+        if (curlyOpen == 1) {\n+          alts.add(alt.toString());\n+          alt.setLength(0);\n+        } else {\n+          cur.append(c);\n+        }\n+      } else {\n+        cur.append(c);\n+      }\n+    }\n+    List<StringWithOffset> exp = new ArrayList<StringWithOffset>();\n+    for (String string : alts) {\n+      exp.add(new StringWithOffset(prefix + string + suffix, prefix.length()));\n+    }\n+    return exp;\n+  }\n+  \n+  /**\n+   * Finds the index of the leftmost opening curly bracket containing a\n+   * slash character (\"/\") in <code>filePattern</code>.\n+   * @param filePattern\n+   * @return the index of the leftmost opening curly bracket containing a\n+   * slash character (\"/\"), or -1 if there is no such bracket\n+   * @throws IOException \n+   */\n+  private static int leftmostOuterCurlyContainingSlash(String filePattern,\n+      int offset) throws IOException {\n+    int curlyOpen = 0;\n+    int leftmost = -1;\n+    boolean seenSlash = false;\n+    for (int i = offset; i < filePattern.length(); i++) {\n+      char c = filePattern.charAt(i);\n+      if (c == '\\\\') {\n+        i++;\n+        if (i >= filePattern.length()) {\n+          throw new IOException(\"Illegal file pattern: \"\n+              + \"An escaped character does not present for glob \"\n+              + filePattern + \" at \" + i);\n+        }\n+      } else if (c == '{') {\n+        if (curlyOpen++ == 0) {\n+          leftmost = i;\n+        }\n+      } else if (c == '}' && curlyOpen > 0) {\n+        if (--curlyOpen == 0 && leftmost != -1 && seenSlash) {\n+          return leftmost;\n+        }\n+      } else if (c == '/' && curlyOpen > 0) {\n+        seenSlash = true;\n+      }\n+    }\n+    return -1;\n+  }\n+\n+}"
        },
        {
            "sha": "bcec4b660f1742663335a4db61ef59fdc859d1cb",
            "filename": "src/java/org/apache/hadoop/fs/HarFileSystem.java",
            "status": "added",
            "additions": 892,
            "deletions": 0,
            "changes": 892,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FHarFileSystem.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FHarFileSystem.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FHarFileSystem.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,892 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.util.LineReader;\n+import org.apache.hadoop.util.Progressable;\n+\n+/**\n+ * This is an implementation of the Hadoop Archive \n+ * Filesystem. This archive Filesystem has index files\n+ * of the form _index* and has contents of the form\n+ * part-*. The index files store the indexes of the \n+ * real files. The index files are of the form _masterindex\n+ * and _index. The master index is a level of indirection \n+ * in to the index file to make the look ups faster. the index\n+ * file is sorted with hash code of the paths that it contains \n+ * and the master index contains pointers to the positions in \n+ * index for ranges of hashcodes.\n+ */\n+\n+public class HarFileSystem extends FilterFileSystem {\n+  public static final int VERSION = 1;\n+  // uri representation of this Har filesystem\n+  private URI uri;\n+  // the version of this har filesystem\n+  private int version;\n+  // underlying uri \n+  private URI underLyingURI;\n+  // the top level path of the archive\n+  // in the underlying file system\n+  private Path archivePath;\n+  // the masterIndex of the archive\n+  private Path masterIndex;\n+  // the index file \n+  private Path archiveIndex;\n+  // the har auth\n+  private String harAuth;\n+  \n+  /**\n+   * public construction of harfilesystem\n+   *\n+   */\n+  public HarFileSystem() {\n+  }\n+  \n+  /**\n+   * Constructor to create a HarFileSystem with an\n+   * underlying filesystem.\n+   * @param fs\n+   */\n+  public HarFileSystem(FileSystem fs) {\n+    super(fs);\n+  }\n+  \n+  /**\n+   * Initialize a Har filesystem per har archive. The \n+   * archive home directory is the top level directory\n+   * in the filesystem that contains the HAR archive.\n+   * Be careful with this method, you do not want to go \n+   * on creating new Filesystem instances per call to \n+   * path.getFileSystem().\n+   * the uri of Har is \n+   * har://underlyingfsscheme-host:port/archivepath.\n+   * or \n+   * har:///archivepath. This assumes the underlying filesystem\n+   * to be used in case not specified.\n+   */\n+  public void initialize(URI name, Configuration conf) throws IOException {\n+    //decode the name\n+    underLyingURI = decodeHarURI(name, conf);\n+    //  we got the right har Path- now check if this is \n+    //truly a har filesystem\n+    Path harPath = archivePath(new Path(name.toString()));\n+    if (harPath == null) { \n+      throw new IOException(\"Invalid path for the Har Filesystem. \" + \n+                           name.toString());\n+    }\n+    if (fs == null) {\n+      fs = FileSystem.get(underLyingURI, conf);\n+    }\n+    this.uri = harPath.toUri();\n+    this.archivePath = new Path(this.uri.getPath());\n+    this.harAuth = getHarAuth(this.underLyingURI);\n+    //check for the underlying fs containing\n+    // the index file\n+    this.masterIndex = new Path(archivePath, \"_masterindex\");\n+    this.archiveIndex = new Path(archivePath, \"_index\");\n+    if (!fs.exists(masterIndex) || !fs.exists(archiveIndex)) {\n+      throw new IOException(\"Invalid path for the Har Filesystem. \" +\n+          \"No index file in \" + harPath);\n+    }\n+    try{ \n+      this.version = getHarVersion();\n+    } catch(IOException io) {\n+      throw new IOException(\"Unable to \" +\n+          \"read the version of the Har file system: \" + this.archivePath);\n+    }\n+    if (this.version != HarFileSystem.VERSION) {\n+      throw new IOException(\"Invalid version \" + \n+          this.version + \" expected \" + HarFileSystem.VERSION);\n+    }\n+  }\n+  \n+  // get the version of the filesystem from the masterindex file\n+  // the version is currently not useful since its the first version \n+  // of archives\n+  public int getHarVersion() throws IOException { \n+    FSDataInputStream masterIn = fs.open(masterIndex);\n+    LineReader lmaster = new LineReader(masterIn, getConf());\n+    Text line = new Text();\n+    lmaster.readLine(line);\n+    try {\n+      masterIn.close();\n+    } catch(IOException e){\n+      //disregard it.\n+      // its a read.\n+    }\n+    String versionLine = line.toString();\n+    String[] arr = versionLine.split(\" \");\n+    int version = Integer.parseInt(arr[0]);\n+    return version;\n+  }\n+  \n+  /*\n+   * find the parent path that is the \n+   * archive path in the path. The last\n+   * path segment that ends with .har is \n+   * the path that will be returned.\n+   */\n+  private Path archivePath(Path p) {\n+    Path retPath = null;\n+    Path tmp = p;\n+    for (int i=0; i< p.depth(); i++) {\n+      if (tmp.toString().endsWith(\".har\")) {\n+        retPath = tmp;\n+        break;\n+      }\n+      tmp = tmp.getParent();\n+    }\n+    return retPath;\n+  }\n+\n+  /**\n+   * decode the raw URI to get the underlying URI\n+   * @param rawURI raw Har URI\n+   * @return filtered URI of the underlying fileSystem\n+   */\n+  private URI decodeHarURI(URI rawURI, Configuration conf) throws IOException {\n+    String tmpAuth = rawURI.getAuthority();\n+    //we are using the default file\n+    //system in the config \n+    //so create a underlying uri and \n+    //return it\n+    if (tmpAuth == null) {\n+      //create a path \n+      return FileSystem.getDefaultUri(conf);\n+    }\n+    String host = rawURI.getHost();\n+    String[] str = host.split(\"-\", 2);\n+    if (str[0] == null) {\n+      throw new IOException(\"URI: \" + rawURI + \" is an invalid Har URI.\");\n+    }\n+    String underLyingScheme = str[0];\n+    String underLyingHost = (str.length > 1)? str[1]:null;\n+    int underLyingPort = rawURI.getPort();\n+    String auth = (underLyingHost == null && underLyingPort == -1)?\n+                  null:(underLyingHost+\":\"+underLyingPort);\n+    URI tmp = null;\n+    if (rawURI.getQuery() != null) {\n+      // query component not allowed\n+      throw new IOException(\"query component in Path not supported  \" + rawURI);\n+    }\n+    try {\n+      tmp = new URI(underLyingScheme, auth, rawURI.getPath(), \n+            rawURI.getQuery(), rawURI.getFragment());\n+    } catch (URISyntaxException e) {\n+        // do nothing should not happen\n+    }\n+    return tmp;\n+  }\n+  \n+  /**\n+   * return the top level archive.\n+   */\n+  public Path getWorkingDirectory() {\n+    return new Path(uri.toString());\n+  }\n+  \n+  /**\n+   * Create a har specific auth \n+   * har-underlyingfs:port\n+   * @param underLyingURI the uri of underlying\n+   * filesystem\n+   * @return har specific auth\n+   */\n+  private String getHarAuth(URI underLyingUri) {\n+    String auth = underLyingUri.getScheme() + \"-\";\n+    if (underLyingUri.getHost() != null) {\n+      auth += underLyingUri.getHost() + \":\";\n+      if (underLyingUri.getPort() != -1) {\n+        auth +=  underLyingUri.getPort();\n+      }\n+    }\n+    else {\n+      auth += \":\";\n+    }\n+    return auth;\n+  }\n+  \n+  /**\n+   * Returns the uri of this filesystem.\n+   * The uri is of the form \n+   * har://underlyingfsschema-host:port/pathintheunderlyingfs\n+   */\n+  @Override\n+  public URI getUri() {\n+    return this.uri;\n+  }\n+  \n+  /**\n+   * this method returns the path \n+   * inside the har filesystem.\n+   * this is relative path inside \n+   * the har filesystem.\n+   * @param path the fully qualified path in the har filesystem.\n+   * @return relative path in the filesystem.\n+   */\n+  private Path getPathInHar(Path path) {\n+    Path harPath = new Path(path.toUri().getPath());\n+    if (archivePath.compareTo(harPath) == 0)\n+      return new Path(Path.SEPARATOR);\n+    Path tmp = new Path(harPath.getName());\n+    Path parent = harPath.getParent();\n+    while (!(parent.compareTo(archivePath) == 0)) {\n+      if (parent.toString().equals(Path.SEPARATOR)) {\n+        tmp = null;\n+        break;\n+      }\n+      tmp = new Path(parent.getName(), tmp);\n+      parent = parent.getParent();\n+    }\n+    if (tmp != null) \n+      tmp = new Path(Path.SEPARATOR, tmp);\n+    return tmp;\n+  }\n+  \n+  //the relative path of p. basically \n+  // getting rid of /. Parsing and doing \n+  // string manipulation is not good - so\n+  // just use the path api to do it.\n+  private Path makeRelative(String initial, Path p) {\n+    Path root = new Path(Path.SEPARATOR);\n+    if (root.compareTo(p) == 0)\n+      return new Path(initial);\n+    Path retPath = new Path(p.getName());\n+    Path parent = p.getParent();\n+    for (int i=0; i < p.depth()-1; i++) {\n+      retPath = new Path(parent.getName(), retPath);\n+      parent = parent.getParent();\n+    }\n+    return new Path(initial, retPath.toString());\n+  }\n+  \n+  /* this makes a path qualified in the har filesystem\n+   * (non-Javadoc)\n+   * @see org.apache.hadoop.fs.FilterFileSystem#makeQualified(\n+   * org.apache.hadoop.fs.Path)\n+   */\n+  @Override\n+  public Path makeQualified(Path path) {\n+    // make sure that we just get the \n+    // path component \n+    Path fsPath = path;\n+    if (!path.isAbsolute()) {\n+      fsPath = new Path(archivePath, path);\n+    }\n+\n+    URI tmpURI = fsPath.toUri();\n+    fsPath = new Path(tmpURI.getPath());\n+    //change this to Har uri \n+    URI tmp = null;\n+    try {\n+      tmp = new URI(uri.getScheme(), harAuth, fsPath.toString(),\n+                    tmpURI.getQuery(), tmpURI.getFragment());\n+    } catch(URISyntaxException ue) {\n+      LOG.error(\"Error in URI \", ue);\n+    }\n+    if (tmp != null) {\n+      return new Path(tmp.toString());\n+    }\n+    return null;\n+  }\n+  \n+  /**\n+   * get block locations from the underlying fs\n+   * @param file the input filestatus to get block locations\n+   * @param start the start in the file\n+   * @param len the length in the file\n+   * @return block locations for this segment of file\n+   * @throws IOException\n+   */\n+  @Override\n+  public BlockLocation[] getFileBlockLocations(FileStatus file, long start,\n+      long len) throws IOException {\n+    // need to look up the file in the underlying fs\n+    // look up the index \n+    \n+    // make sure this is a prt of this har filesystem\n+    Path p = makeQualified(file.getPath());\n+    Path harPath = getPathInHar(p);\n+    String line = fileStatusInIndex(harPath);\n+    if (line == null)  {\n+      throw new FileNotFoundException(\"File \" + file.getPath() + \" not found\");\n+    }\n+    HarStatus harStatus = new HarStatus(line);\n+    if (harStatus.isDir()) {\n+      return new BlockLocation[0];\n+    }\n+    FileStatus fsFile = fs.getFileStatus(new Path(archivePath,\n+        harStatus.getPartName()));\n+    BlockLocation[] rawBlocks = fs.getFileBlockLocations(fsFile, \n+        harStatus.getStartIndex() + start, len);\n+    return fakeBlockLocations(rawBlocks, harStatus.getStartIndex());\n+  }\n+  \n+  /**\n+   * fake the rawblocks since map reduce uses the block offsets to \n+   * fo some computations regarding the blocks\n+   * @param rawBlocks the raw blocks returned by the filesystem\n+   * @return faked blocks with changed offsets.\n+   */\n+  private BlockLocation[] fakeBlockLocations(BlockLocation[] rawBlocks, \n+\t\t  long startIndex) {\n+\tfor (BlockLocation block : rawBlocks) {\n+\t\tlong rawOffset = block.getOffset();\n+\t\tblock.setOffset(rawOffset - startIndex);\n+\t}\n+\treturn rawBlocks;\n+  }\n+  \n+  /**\n+   * the hash of the path p inside iniside\n+   * the filesystem\n+   * @param p the path in the harfilesystem\n+   * @return the hash code of the path.\n+   */\n+  public static int getHarHash(Path p) {\n+    return (p.toString().hashCode() & 0x7fffffff);\n+  }\n+  \n+  static class Store {\n+    public Store() {\n+      begin = end = startHash = endHash = 0;\n+    }\n+    public Store(long begin, long end, int startHash, int endHash) {\n+      this.begin = begin;\n+      this.end = end;\n+      this.startHash = startHash;\n+      this.endHash = endHash;\n+    }\n+    public long begin;\n+    public long end;\n+    public int startHash;\n+    public int endHash;\n+  }\n+  \n+  // make sure that this harPath is relative to the har filesystem\n+  // this only works for relative paths. This returns the line matching\n+  // the file in the index. Returns a null if there is not matching \n+  // filename in the index file.\n+  private String fileStatusInIndex(Path harPath) throws IOException {\n+    // read the index file \n+    int hashCode = getHarHash(harPath);\n+    // get the master index to find the pos \n+    // in the index file\n+    FSDataInputStream in = fs.open(masterIndex);\n+    FileStatus masterStat = fs.getFileStatus(masterIndex);\n+    LineReader lin = new LineReader(in, getConf());\n+    Text line = new Text();\n+    long read = lin.readLine(line);\n+   //ignore the first line. this is the header of the index files\n+    String[] readStr = null;\n+    List<Store> stores = new ArrayList<Store>();\n+    while(read < masterStat.getLen()) {\n+      int b = lin.readLine(line);\n+      read += b;\n+      readStr = line.toString().split(\" \");\n+      int startHash = Integer.parseInt(readStr[0]);\n+      int endHash  = Integer.parseInt(readStr[1]);\n+      if (startHash <= hashCode && hashCode <= endHash) {\n+        stores.add(new Store(Long.parseLong(readStr[2]), \n+            Long.parseLong(readStr[3]), startHash,\n+            endHash));\n+      }\n+      line.clear();\n+    }\n+    try {\n+      lin.close();\n+    } catch(IOException io){\n+      // do nothing just a read.\n+    }\n+    FSDataInputStream aIn = fs.open(archiveIndex);\n+    LineReader aLin = new LineReader(aIn, getConf());\n+    String retStr = null;\n+    // now start reading the real index file\n+     read = 0;\n+    for (Store s: stores) {\n+      aIn.seek(s.begin);\n+      while (read + s.begin < s.end) {\n+        int tmp = aLin.readLine(line);\n+        read += tmp;\n+        String lineFeed = line.toString();\n+        String[] parsed = lineFeed.split(\" \");\n+        if (harPath.compareTo(new Path(parsed[0])) == 0) {\n+          // bingo!\n+          retStr = lineFeed;\n+          break;\n+        }\n+        line.clear();\n+      }\n+      if (retStr != null)\n+        break;\n+    }\n+    try {\n+      aIn.close();\n+    } catch(IOException io) {\n+      //do nothing\n+    }\n+    return retStr;\n+  }\n+  \n+  // a single line parser for hadoop archives status \n+  // stored in a single line in the index files \n+  // the format is of the form \n+  // filename \"dir\"/\"file\" partFileName startIndex length \n+  // <space seperated children>\n+  private static class HarStatus {\n+    boolean isDir;\n+    String name;\n+    List<String> children;\n+    String partName;\n+    long startIndex;\n+    long length;\n+    public HarStatus(String harString) {\n+      String[] splits = harString.split(\" \");\n+      this.name = splits[0];\n+      this.isDir = \"dir\".equals(splits[1]) ? true: false;\n+      // this is equal to \"none\" if its a directory\n+      this.partName = splits[2];\n+      this.startIndex = Long.parseLong(splits[3]);\n+      this.length = Long.parseLong(splits[4]);\n+      if (isDir) {\n+        children = new ArrayList<String>();\n+        for (int i = 5; i < splits.length; i++) {\n+          children.add(splits[i]);\n+        }\n+      }\n+    }\n+    public boolean isDir() {\n+      return isDir;\n+    }\n+    \n+    public String getName() {\n+      return name;\n+    }\n+    \n+    public List<String> getChildren() {\n+      return children;\n+    }\n+    public String getFileName() {\n+      return name;\n+    }\n+    public String getPartName() {\n+      return partName;\n+    }\n+    public long getStartIndex() {\n+      return startIndex;\n+    }\n+    public long getLength() {\n+      return length;\n+    }\n+  }\n+  \n+  /**\n+   * return the filestatus of files in har archive.\n+   * The permission returned are that of the archive\n+   * index files. The permissions are not persisted \n+   * while creating a hadoop archive.\n+   * @param f the path in har filesystem\n+   * @return filestatus.\n+   * @throws IOException\n+   */\n+  @Override\n+  public FileStatus getFileStatus(Path f) throws IOException {\n+    FileStatus archiveStatus = fs.getFileStatus(archiveIndex);\n+    // get the fs DataInputStream for the underlying file\n+    // look up the index.\n+    Path p = makeQualified(f);\n+    Path harPath = getPathInHar(p);\n+    if (harPath == null) {\n+      throw new IOException(\"Invalid file name: \" + f + \" in \" + uri);\n+    }\n+    String readStr = fileStatusInIndex(harPath);\n+    if (readStr == null) {\n+      throw new FileNotFoundException(\"File: \" +  f + \" does not exist in \" + uri);\n+    }\n+    HarStatus hstatus = null;\n+    hstatus = new HarStatus(readStr);\n+    return new FileStatus(hstatus.isDir()?0:hstatus.getLength(), hstatus.isDir(),\n+        (int)archiveStatus.getReplication(), archiveStatus.getBlockSize(),\n+        archiveStatus.getModificationTime(), archiveStatus.getAccessTime(),\n+        new FsPermission(\n+        archiveStatus.getPermission()), archiveStatus.getOwner(), \n+        archiveStatus.getGroup(), \n+            makeRelative(this.uri.toString(), new Path(hstatus.name)));\n+  }\n+\n+  /**\n+   * Returns a har input stream which fakes end of \n+   * file. It reads the index files to get the part \n+   * file name and the size and start of the file.\n+   */\n+  @Override\n+  public FSDataInputStream open(Path f, int bufferSize) throws IOException {\n+    // get the fs DataInputStream for the underlying file\n+    // look up the index.\n+    Path p = makeQualified(f);\n+    Path harPath = getPathInHar(p);\n+    if (harPath == null) {\n+      throw new IOException(\"Invalid file name: \" + f + \" in \" + uri);\n+    }\n+    String readStr = fileStatusInIndex(harPath);\n+    if (readStr == null) {\n+      throw new FileNotFoundException(f + \": not found in \" + archivePath);\n+    }\n+    HarStatus hstatus = new HarStatus(readStr); \n+    // we got it.. woo hooo!!! \n+    if (hstatus.isDir()) {\n+      throw new FileNotFoundException(f + \" : not a file in \" +\n+                archivePath);\n+    }\n+    return new HarFSDataInputStream(fs, new Path(archivePath, \n+        hstatus.getPartName()),\n+        hstatus.getStartIndex(), hstatus.getLength(), bufferSize);\n+  }\n+ \n+  /*\n+   * create throws an exception in Har filesystem.\n+   * The archive once created cannot be changed.\n+   */\n+  public FSDataOutputStream create(Path f, int bufferSize) \n+                                    throws IOException {\n+    throw new IOException(\"Har: Create not allowed\");\n+  }\n+  \n+  public FSDataOutputStream create(Path f,\n+      FsPermission permission,\n+      boolean overwrite,\n+      int bufferSize,\n+      short replication,\n+      long blockSize,\n+      Progressable progress) throws IOException {\n+    throw new IOException(\"Har: create not allowed.\");\n+  }\n+  \n+  @Override\n+  public void close() throws IOException {\n+    if (fs != null) {\n+      try {\n+        fs.close();\n+      } catch(IOException ie) {\n+        //this might already be closed\n+        // ignore\n+      }\n+    }\n+  }\n+  \n+  /**\n+   * Not implemented.\n+   */\n+  @Override\n+  public boolean setReplication(Path src, short replication) throws IOException{\n+    throw new IOException(\"Har: setreplication not allowed\");\n+  }\n+  \n+  /**\n+   * Not implemented.\n+   */\n+  @Override\n+  public boolean delete(Path f, boolean recursive) throws IOException { \n+    throw new IOException(\"Har: delete not allowed\");\n+  }\n+  \n+  /**\n+   * liststatus returns the children of a directory \n+   * after looking up the index files.\n+   */\n+  @Override\n+  public FileStatus[] listStatus(Path f) throws IOException {\n+    //need to see if the file is an index in file\n+    //get the filestatus of the archive directory\n+    // we will create fake filestatuses to return\n+    // to the client\n+    List<FileStatus> statuses = new ArrayList<FileStatus>();\n+    FileStatus archiveStatus = fs.getFileStatus(archiveIndex);\n+    Path tmpPath = makeQualified(f);\n+    Path harPath = getPathInHar(tmpPath);\n+    String readStr = fileStatusInIndex(harPath);\n+    if (readStr == null) {\n+      throw new FileNotFoundException(\"File \" + f + \" not found in \" + archivePath);\n+    }\n+    HarStatus hstatus = new HarStatus(readStr);\n+    if (!hstatus.isDir()) \n+        statuses.add(new FileStatus(hstatus.getLength(), \n+            hstatus.isDir(),\n+            archiveStatus.getReplication(), archiveStatus.getBlockSize(),\n+            archiveStatus.getModificationTime(), archiveStatus.getAccessTime(),\n+            new FsPermission(archiveStatus.getPermission()),\n+            archiveStatus.getOwner(), archiveStatus.getGroup(), \n+            makeRelative(this.uri.toString(), new Path(hstatus.name))));\n+    else \n+      for (String child: hstatus.children) {\n+        FileStatus tmp = getFileStatus(new Path(tmpPath, child));\n+        statuses.add(tmp);\n+      }\n+    return statuses.toArray(new FileStatus[statuses.size()]);\n+  }\n+  \n+  /**\n+   * return the top level archive path.\n+   */\n+  public Path getHomeDirectory() {\n+    return new Path(uri.toString());\n+  }\n+  \n+  public void setWorkingDirectory(Path newDir) {\n+    //does nothing.\n+  }\n+  \n+  /**\n+   * not implemented.\n+   */\n+  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n+    throw new IOException(\"Har: mkdirs not allowed\");\n+  }\n+  \n+  /**\n+   * not implemented.\n+   */\n+  public void copyFromLocalFile(boolean delSrc, Path src, Path dst) throws \n+        IOException {\n+    throw new IOException(\"Har: copyfromlocalfile not allowed\");\n+  }\n+  \n+  /**\n+   * copies the file in the har filesystem to a local file.\n+   */\n+  public void copyToLocalFile(boolean delSrc, Path src, Path dst) \n+    throws IOException {\n+    FileUtil.copy(this, src, getLocal(getConf()), dst, false, getConf());\n+  }\n+  \n+  /**\n+   * not implemented.\n+   */\n+  public Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile) \n+    throws IOException {\n+    throw new IOException(\"Har: startLocalOutput not allowed\");\n+  }\n+  \n+  /**\n+   * not implemented.\n+   */\n+  public void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile) \n+    throws IOException {\n+    throw new IOException(\"Har: completeLocalOutput not allowed\");\n+  }\n+  \n+  /**\n+   * not implemented.\n+   */\n+  public void setOwner(Path p, String username, String groupname)\n+    throws IOException {\n+    throw new IOException(\"Har: setowner not allowed\");\n+  }\n+\n+  /**\n+   * Not implemented.\n+   */\n+  public void setPermission(Path p, FsPermission permisssion) \n+    throws IOException {\n+    throw new IOException(\"Har: setPermission not allowed\");\n+  }\n+  \n+  /**\n+   * Hadoop archives input stream. This input stream fakes EOF \n+   * since archive files are part of bigger part files.\n+   */\n+  private static class HarFSDataInputStream extends FSDataInputStream {\n+    /**\n+     * Create an input stream that fakes all the reads/positions/seeking.\n+     */\n+    private static class HarFsInputStream extends FSInputStream {\n+      private long position, start, end;\n+      //The underlying data input stream that the\n+      // underlying filesystem will return.\n+      private FSDataInputStream underLyingStream;\n+      //one byte buffer\n+      private byte[] oneBytebuff = new byte[1];\n+      HarFsInputStream(FileSystem fs, Path path, long start,\n+          long length, int bufferSize) throws IOException {\n+        underLyingStream = fs.open(path, bufferSize);\n+        underLyingStream.seek(start);\n+        // the start of this file in the part file\n+        this.start = start;\n+        // the position pointer in the part file\n+        this.position = start;\n+        // the end pointer in the part file\n+        this.end = start + length;\n+      }\n+      \n+      public synchronized int available() throws IOException {\n+        long remaining = end - underLyingStream.getPos();\n+        if (remaining > (long)Integer.MAX_VALUE) {\n+          return Integer.MAX_VALUE;\n+        }\n+        return (int) remaining;\n+      }\n+      \n+      public synchronized  void close() throws IOException {\n+        underLyingStream.close();\n+        super.close();\n+      }\n+      \n+      //not implemented\n+      @Override\n+      public void mark(int readLimit) {\n+        // do nothing \n+      }\n+      \n+      /**\n+       * reset is not implemented\n+       */\n+      public void reset() throws IOException {\n+        throw new IOException(\"reset not implemented.\");\n+      }\n+      \n+      public synchronized int read() throws IOException {\n+        int ret = read(oneBytebuff, 0, 1);\n+        return (ret <= 0) ? -1: (oneBytebuff[0] & 0xff);\n+      }\n+      \n+      public synchronized int read(byte[] b) throws IOException {\n+        int ret = read(b, 0, b.length);\n+        if (ret != -1) {\n+          position += ret;\n+        }\n+        return ret;\n+      }\n+      \n+      /**\n+       * \n+       */\n+      public synchronized int read(byte[] b, int offset, int len) \n+        throws IOException {\n+        int newlen = len;\n+        int ret = -1;\n+        if (position + len > end) {\n+          newlen = (int) (end - position);\n+        }\n+        // end case\n+        if (newlen == 0) \n+          return ret;\n+        ret = underLyingStream.read(b, offset, newlen);\n+        position += ret;\n+        return ret;\n+      }\n+      \n+      public synchronized long skip(long n) throws IOException {\n+        long tmpN = n;\n+        if (tmpN > 0) {\n+          if (position + tmpN > end) {\n+            tmpN = end - position;\n+          }\n+          underLyingStream.seek(tmpN + position);\n+          position += tmpN;\n+          return tmpN;\n+        }\n+        return (tmpN < 0)? -1 : 0;\n+      }\n+      \n+      public synchronized long getPos() throws IOException {\n+        return (position - start);\n+      }\n+      \n+      public synchronized void seek(long pos) throws IOException {\n+        if (pos < 0 || (start + pos > end)) {\n+          throw new IOException(\"Failed to seek: EOF\");\n+        }\n+        position = start + pos;\n+        underLyingStream.seek(position);\n+      }\n+\n+      public boolean seekToNewSource(long targetPos) throws IOException {\n+        //do not need to implement this\n+        // hdfs in itself does seektonewsource \n+        // while reading.\n+        return false;\n+      }\n+      \n+      /**\n+       * implementing position readable. \n+       */\n+      public int read(long pos, byte[] b, int offset, int length) \n+      throws IOException {\n+        int nlength = length;\n+        if (start + nlength + pos > end) {\n+          nlength = (int) (end - (start + pos));\n+        }\n+        return underLyingStream.read(pos + start , b, offset, nlength);\n+      }\n+      \n+      /**\n+       * position readable again.\n+       */\n+      public void readFully(long pos, byte[] b, int offset, int length) \n+      throws IOException {\n+        if (start + length + pos > end) {\n+          throw new IOException(\"Not enough bytes to read.\");\n+        }\n+        underLyingStream.readFully(pos + start, b, offset, length);\n+      }\n+      \n+      public void readFully(long pos, byte[] b) throws IOException {\n+          readFully(pos, b, 0, b.length);\n+      }\n+      \n+    }\n+  \n+    /**\n+     * constructors for har input stream.\n+     * @param fs the underlying filesystem\n+     * @param p The path in the underlying filesystem\n+     * @param start the start position in the part file\n+     * @param length the length of valid data in the part file\n+     * @param bufsize the buffer size\n+     * @throws IOException\n+     */\n+    public HarFSDataInputStream(FileSystem fs, Path  p, long start, \n+        long length, int bufsize) throws IOException {\n+        super(new HarFsInputStream(fs, p, start, length, bufsize));\n+    }\n+\n+    /**\n+     * constructor for har input stream.\n+     * @param fs the underlying filesystem\n+     * @param p the path in the underlying file system\n+     * @param start the start position in the part file\n+     * @param length the length of valid data in the part file.\n+     * @throws IOException\n+     */\n+    public HarFSDataInputStream(FileSystem fs, Path  p, long start, long length)\n+      throws IOException {\n+        super(new HarFsInputStream(fs, p, start, length, 0));\n+    }\n+  }\n+}"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "src/java/org/apache/hadoop/fs/LengthFileChecksum.java",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FLengthFileChecksum.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FLengthFileChecksum.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FLengthFileChecksum.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "5d04d280da33f223eba198c467931cc48c665d72",
            "filename": "src/java/org/apache/hadoop/fs/LocalDirAllocator.java",
            "status": "added",
            "additions": 418,
            "deletions": 0,
            "changes": 418,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FLocalDirAllocator.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FLocalDirAllocator.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FLocalDirAllocator.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,418 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs;\n+\n+import java.io.*;\n+import java.util.*;\n+\n+import org.apache.commons.logging.*;\n+\n+import org.apache.hadoop.util.*;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.util.DiskChecker.DiskErrorException;\n+import org.apache.hadoop.conf.Configuration; \n+\n+/** An implementation of a round-robin scheme for disk allocation for creating\n+ * files. The way it works is that it is kept track what disk was last\n+ * allocated for a file write. For the current request, the next disk from\n+ * the set of disks would be allocated if the free space on the disk is \n+ * sufficient enough to accomodate the file that is being considered for\n+ * creation. If the space requirements cannot be met, the next disk in order\n+ * would be tried and so on till a disk is found with sufficient capacity.\n+ * Once a disk with sufficient space is identified, a check is done to make\n+ * sure that the disk is writable. Also, there is an API provided that doesn't\n+ * take the space requirements into consideration but just checks whether the\n+ * disk under consideration is writable (this should be used for cases where\n+ * the file size is not known apriori). An API is provided to read a path that\n+ * was created earlier. That API works by doing a scan of all the disks for the\n+ * input pathname.\n+ * This implementation also provides the functionality of having multiple \n+ * allocators per JVM (one for each unique functionality or context, like \n+ * mapred, dfs-client, etc.). It ensures that there is only one instance of\n+ * an allocator per context per JVM.\n+ * Note:\n+ * 1. The contexts referred above are actually the configuration items defined\n+ * in the Configuration class like \"mapred.local.dir\" (for which we want to \n+ * control the dir allocations). The context-strings are exactly those \n+ * configuration items.\n+ * 2. This implementation does not take into consideration cases where\n+ * a disk becomes read-only or goes out of space while a file is being written\n+ * to (disks are shared between multiple processes, and so the latter situation\n+ * is probable).\n+ * 3. In the class implementation, \"Disk\" is referred to as \"Dir\", which\n+ * actually points to the configured directory on the Disk which will be the\n+ * parent for all file write/read allocations.\n+ */\n+public class LocalDirAllocator {\n+  \n+  //A Map from the config item names like \"mapred.local.dir\", \n+  //\"dfs.client.buffer.dir\" to the instance of the AllocatorPerContext. This\n+  //is a static object to make sure there exists exactly one instance per JVM\n+  private static Map <String, AllocatorPerContext> contexts = \n+                 new TreeMap<String, AllocatorPerContext>();\n+  private String contextCfgItemName;\n+\n+  /**Create an allocator object\n+   * @param contextCfgItemName\n+   */\n+  public LocalDirAllocator(String contextCfgItemName) {\n+    this.contextCfgItemName = contextCfgItemName;\n+  }\n+  \n+  /** This method must be used to obtain the dir allocation context for a \n+   * particular value of the context name. The context name must be an item\n+   * defined in the Configuration object for which we want to control the \n+   * dir allocations (e.g., <code>mapred.local.dir</code>). The method will\n+   * create a context for that name if it doesn't already exist.\n+   */\n+  private AllocatorPerContext obtainContext(String contextCfgItemName) {\n+    synchronized (contexts) {\n+      AllocatorPerContext l = contexts.get(contextCfgItemName);\n+      if (l == null) {\n+        contexts.put(contextCfgItemName, \n+                    (l = new AllocatorPerContext(contextCfgItemName)));\n+      }\n+      return l;\n+    }\n+  }\n+  \n+  /** Get a path from the local FS. This method should be used if the size of \n+   *  the file is not known apriori. We go round-robin over the set of disks\n+   *  (via the configured dirs) and return the first complete path where\n+   *  we could create the parent directory of the passed path. \n+   *  @param pathStr the requested path (this will be created on the first \n+   *  available disk)\n+   *  @param conf the Configuration object\n+   *  @return the complete path to the file on a local disk\n+   *  @throws IOException\n+   */\n+  public Path getLocalPathForWrite(String pathStr, \n+      Configuration conf) throws IOException {\n+    return getLocalPathForWrite(pathStr, -1, conf);\n+  }\n+  \n+  /** Get a path from the local FS. Pass size as -1 if not known apriori. We\n+   *  round-robin over the set of disks (via the configured dirs) and return\n+   *  the first complete path which has enough space \n+   *  @param pathStr the requested path (this will be created on the first \n+   *  available disk)\n+   *  @param size the size of the file that is going to be written\n+   *  @param conf the Configuration object\n+   *  @return the complete path to the file on a local disk\n+   *  @throws IOException\n+   */\n+  public Path getLocalPathForWrite(String pathStr, long size, \n+      Configuration conf) throws IOException {\n+    AllocatorPerContext context = obtainContext(contextCfgItemName);\n+    return context.getLocalPathForWrite(pathStr, size, conf);\n+  }\n+  \n+  /** Get a path from the local FS for reading. We search through all the\n+   *  configured dirs for the file's existence and return the complete\n+   *  path to the file when we find one \n+   *  @param pathStr the requested file (this will be searched)\n+   *  @param conf the Configuration object\n+   *  @return the complete path to the file on a local disk\n+   *  @throws IOException\n+   */\n+  public Path getLocalPathToRead(String pathStr, \n+      Configuration conf) throws IOException {\n+    AllocatorPerContext context = obtainContext(contextCfgItemName);\n+    return context.getLocalPathToRead(pathStr, conf);\n+  }\n+\n+  /** Creates a temporary file in the local FS. Pass size as -1 if not known \n+   *  apriori. We round-robin over the set of disks (via the configured dirs) \n+   *  and select the first complete path which has enough space. A file is\n+   *  created on this directory. The file is guaranteed to go away when the\n+   *  JVM exits.\n+   *  @param pathStr prefix for the temporary file\n+   *  @param size the size of the file that is going to be written\n+   *  @param conf the Configuration object\n+   *  @return a unique temporary file\n+   *  @throws IOException\n+   */\n+  public File createTmpFileForWrite(String pathStr, long size, \n+      Configuration conf) throws IOException {\n+    AllocatorPerContext context = obtainContext(contextCfgItemName);\n+    return context.createTmpFileForWrite(pathStr, size, conf);\n+  }\n+  \n+  /** Method to check whether a context is valid\n+   * @param contextCfgItemName\n+   * @return true/false\n+   */\n+  public static boolean isContextValid(String contextCfgItemName) {\n+    synchronized (contexts) {\n+      return contexts.containsKey(contextCfgItemName);\n+    }\n+  }\n+    \n+  /** We search through all the configured dirs for the file's existence\n+   *  and return true when we find  \n+   *  @param pathStr the requested file (this will be searched)\n+   *  @param conf the Configuration object\n+   *  @return true if files exist. false otherwise\n+   *  @throws IOException\n+   */\n+  public boolean ifExists(String pathStr,Configuration conf) {\n+    AllocatorPerContext context = obtainContext(contextCfgItemName);\n+    return context.ifExists(pathStr, conf);\n+  }\n+\n+  /**\n+   * Get the current directory index for the given configuration item.\n+   * @return the current directory index for the given configuration item.\n+   */\n+  int getCurrentDirectoryIndex() {\n+    AllocatorPerContext context = obtainContext(contextCfgItemName);\n+    return context.getCurrentDirectoryIndex();\n+  }\n+  \n+  private static class AllocatorPerContext {\n+\n+    private final Log LOG =\n+      LogFactory.getLog(AllocatorPerContext.class);\n+\n+    private int dirNumLastAccessed;\n+    private Random dirIndexRandomizer = new Random();\n+    private FileSystem localFS;\n+    private DF[] dirDF;\n+    private String contextCfgItemName;\n+    private String[] localDirs;\n+    private String savedLocalDirs = \"\";\n+\n+    public AllocatorPerContext(String contextCfgItemName) {\n+      this.contextCfgItemName = contextCfgItemName;\n+    }\n+\n+    /** This method gets called everytime before any read/write to make sure\n+     * that any change to localDirs is reflected immediately.\n+     */\n+    private void confChanged(Configuration conf) throws IOException {\n+      String newLocalDirs = conf.get(contextCfgItemName);\n+      if (!newLocalDirs.equals(savedLocalDirs)) {\n+        localDirs = conf.getStrings(contextCfgItemName);\n+        localFS = FileSystem.getLocal(conf);\n+        int numDirs = localDirs.length;\n+        ArrayList<String> dirs = new ArrayList<String>(numDirs);\n+        ArrayList<DF> dfList = new ArrayList<DF>(numDirs);\n+        for (int i = 0; i < numDirs; i++) {\n+          try {\n+            // filter problematic directories\n+            Path tmpDir = new Path(localDirs[i]);\n+            if(localFS.mkdirs(tmpDir)|| localFS.exists(tmpDir)) {\n+              try {\n+                DiskChecker.checkDir(new File(localDirs[i]));\n+                dirs.add(localDirs[i]);\n+                dfList.add(new DF(new File(localDirs[i]), 30000));\n+              } catch (DiskErrorException de) {\n+                LOG.warn( localDirs[i] + \"is not writable\\n\" +\n+                    StringUtils.stringifyException(de));\n+              }\n+            } else {\n+              LOG.warn( \"Failed to create \" + localDirs[i]);\n+            }\n+          } catch (IOException ie) { \n+            LOG.warn( \"Failed to create \" + localDirs[i] + \": \" +\n+                ie.getMessage() + \"\\n\" + StringUtils.stringifyException(ie));\n+          } //ignore\n+        }\n+        localDirs = dirs.toArray(new String[dirs.size()]);\n+        dirDF = dfList.toArray(new DF[dirs.size()]);\n+        savedLocalDirs = newLocalDirs;\n+        \n+        // randomize the first disk picked in the round-robin selection \n+        dirNumLastAccessed = dirIndexRandomizer.nextInt(dirs.size());\n+      }\n+    }\n+\n+    private Path createPath(String path) throws IOException {\n+      Path file = new Path(new Path(localDirs[dirNumLastAccessed]),\n+                                    path);\n+      //check whether we are able to create a directory here. If the disk\n+      //happens to be RDONLY we will fail\n+      try {\n+        DiskChecker.checkDir(new File(file.getParent().toUri().getPath()));\n+        return file;\n+      } catch (DiskErrorException d) {\n+        LOG.warn(StringUtils.stringifyException(d));\n+        return null;\n+      }\n+    }\n+\n+    /**\n+     * Get the current directory index.\n+     * @return the current directory index.\n+     */\n+    int getCurrentDirectoryIndex() {\n+      return dirNumLastAccessed;\n+    }\n+    \n+    /** Get a path from the local FS. This method should be used if the size of \n+     *  the file is not known a priori. \n+     *  \n+     *  It will use roulette selection, picking directories\n+     *  with probability proportional to their available space. \n+     */\n+    public synchronized Path getLocalPathForWrite(String path, \n+        Configuration conf) throws IOException {\n+      return getLocalPathForWrite(path, -1, conf);\n+    }\n+\n+    /** Get a path from the local FS. If size is known, we go\n+     *  round-robin over the set of disks (via the configured dirs) and return\n+     *  the first complete path which has enough space.\n+     *  \n+     *  If size is not known, use roulette selection -- pick directories\n+     *  with probability proportional to their available space.\n+     */\n+    public synchronized Path getLocalPathForWrite(String pathStr, long size, \n+        Configuration conf) throws IOException {\n+      confChanged(conf);\n+      int numDirs = localDirs.length;\n+      int numDirsSearched = 0;\n+      //remove the leading slash from the path (to make sure that the uri\n+      //resolution results in a valid path on the dir being checked)\n+      if (pathStr.startsWith(\"/\")) {\n+        pathStr = pathStr.substring(1);\n+      }\n+      Path returnPath = null;\n+      \n+      if(size == -1) {  //do roulette selection: pick dir with probability \n+                    //proportional to available size\n+        long[] availableOnDisk = new long[dirDF.length];\n+        long totalAvailable = 0;\n+        \n+            //build the \"roulette wheel\"\n+        for(int i =0; i < dirDF.length; ++i) {\n+          availableOnDisk[i] = dirDF[i].getAvailable();\n+          totalAvailable += availableOnDisk[i];\n+        }\n+\n+        // Keep rolling the wheel till we get a valid path\n+        Random r = new java.util.Random();\n+        while (numDirsSearched < numDirs && returnPath == null) {\n+          long randomPosition = Math.abs(r.nextLong()) % totalAvailable;\n+          int dir = 0;\n+          while (randomPosition > availableOnDisk[dir]) {\n+            randomPosition -= availableOnDisk[dir];\n+            dir++;\n+          }\n+          dirNumLastAccessed = dir;\n+          returnPath = createPath(pathStr);\n+          if (returnPath == null) {\n+            totalAvailable -= availableOnDisk[dir];\n+            availableOnDisk[dir] = 0; // skip this disk\n+            numDirsSearched++;\n+          }\n+        }\n+      } else {\n+        while (numDirsSearched < numDirs && returnPath == null) {\n+          long capacity = dirDF[dirNumLastAccessed].getAvailable();\n+          if (capacity > size) {\n+            returnPath = createPath(pathStr);\n+          }\n+          dirNumLastAccessed++;\n+          dirNumLastAccessed = dirNumLastAccessed % numDirs; \n+          numDirsSearched++;\n+        } \n+      }\n+      if (returnPath != null) {\n+        return returnPath;\n+      }\n+      \n+      //no path found\n+      throw new DiskErrorException(\"Could not find any valid local \" +\n+          \"directory for \" + pathStr);\n+    }\n+\n+    /** Creates a file on the local FS. Pass size as -1 if not known apriori. We\n+     *  round-robin over the set of disks (via the configured dirs) and return\n+     *  a file on the first path which has enough space. The file is guaranteed\n+     *  to go away when the JVM exits.\n+     */\n+    public File createTmpFileForWrite(String pathStr, long size, \n+        Configuration conf) throws IOException {\n+\n+      // find an appropriate directory\n+      Path path = getLocalPathForWrite(pathStr, size, conf);\n+      File dir = new File(path.getParent().toUri().getPath());\n+      String prefix = path.getName();\n+\n+      // create a temp file on this directory\n+      File result = File.createTempFile(prefix, null, dir);\n+      result.deleteOnExit();\n+      return result;\n+    }\n+\n+    /** Get a path from the local FS for reading. We search through all the\n+     *  configured dirs for the file's existence and return the complete\n+     *  path to the file when we find one \n+     */\n+    public synchronized Path getLocalPathToRead(String pathStr, \n+        Configuration conf) throws IOException {\n+      confChanged(conf);\n+      int numDirs = localDirs.length;\n+      int numDirsSearched = 0;\n+      //remove the leading slash from the path (to make sure that the uri\n+      //resolution results in a valid path on the dir being checked)\n+      if (pathStr.startsWith(\"/\")) {\n+        pathStr = pathStr.substring(1);\n+      }\n+      while (numDirsSearched < numDirs) {\n+        Path file = new Path(localDirs[numDirsSearched], pathStr);\n+        if (localFS.exists(file)) {\n+          return file;\n+        }\n+        numDirsSearched++;\n+      }\n+\n+      //no path found\n+      throw new DiskErrorException (\"Could not find \" + pathStr +\" in any of\" +\n+      \" the configured local directories\");\n+    }\n+\n+    /** We search through all the configured dirs for the file's existence\n+     *  and return true when we find one \n+     */\n+    public synchronized boolean ifExists(String pathStr,Configuration conf) {\n+      try {\n+        int numDirs = localDirs.length;\n+        int numDirsSearched = 0;\n+        //remove the leading slash from the path (to make sure that the uri\n+        //resolution results in a valid path on the dir being checked)\n+        if (pathStr.startsWith(\"/\")) {\n+          pathStr = pathStr.substring(1);\n+        }\n+        while (numDirsSearched < numDirs) {\n+          Path file = new Path(localDirs[numDirsSearched], pathStr);\n+          if (localFS.exists(file)) {\n+            return true;\n+          }\n+          numDirsSearched++;\n+        }\n+      } catch (IOException e) {\n+        // IGNORE and try again\n+      }\n+      return false;\n+    }\n+  }\n+}"
        },
        {
            "sha": "199c773f5e42ec426829a7079273809ad2753df6",
            "filename": "src/java/org/apache/hadoop/fs/LocalFileSystem.java",
            "status": "added",
            "additions": 115,
            "deletions": 0,
            "changes": 115,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FLocalFileSystem.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FLocalFileSystem.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FLocalFileSystem.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,115 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs;\n+\n+import java.io.*;\n+import java.net.URI;\n+import java.util.*;\n+\n+/****************************************************************\n+ * Implement the FileSystem API for the checksumed local filesystem.\n+ *\n+ *****************************************************************/\n+public class LocalFileSystem extends ChecksumFileSystem {\n+  static final URI NAME = URI.create(\"file:///\");\n+  static private Random rand = new Random();\n+  FileSystem rfs;\n+  \n+  public LocalFileSystem() {\n+    this(new RawLocalFileSystem());\n+  }\n+  \n+  public FileSystem getRaw() {\n+    return rfs;\n+  }\n+    \n+  public LocalFileSystem(FileSystem rawLocalFileSystem) {\n+    super(rawLocalFileSystem);\n+    rfs = rawLocalFileSystem;\n+  }\n+    \n+  /** Convert a path to a File. */\n+  public File pathToFile(Path path) {\n+    return ((RawLocalFileSystem)fs).pathToFile(path);\n+  }\n+\n+  @Override\n+  public void copyFromLocalFile(boolean delSrc, Path src, Path dst)\n+    throws IOException {\n+    FileUtil.copy(this, src, this, dst, delSrc, getConf());\n+  }\n+\n+  @Override\n+  public void copyToLocalFile(boolean delSrc, Path src, Path dst)\n+    throws IOException {\n+    FileUtil.copy(this, src, this, dst, delSrc, getConf());\n+  }\n+\n+  /**\n+   * Moves files to a bad file directory on the same device, so that their\n+   * storage will not be reused.\n+   */\n+  public boolean reportChecksumFailure(Path p, FSDataInputStream in,\n+                                       long inPos,\n+                                       FSDataInputStream sums, long sumsPos) {\n+    try {\n+      // canonicalize f\n+      File f = ((RawLocalFileSystem)fs).pathToFile(p).getCanonicalFile();\n+      \n+      // find highest writable parent dir of f on the same device\n+      String device = new DF(f, getConf()).getMount();\n+      File parent = f.getParentFile();\n+      File dir = null;\n+      while (parent!=null && parent.canWrite() && parent.toString().startsWith(device)) {\n+        dir = parent;\n+        parent = parent.getParentFile();\n+      }\n+\n+      if (dir==null) {\n+        throw new IOException(\n+                              \"not able to find the highest writable parent dir\");\n+      }\n+        \n+      // move the file there\n+      File badDir = new File(dir, \"bad_files\");\n+      if (!badDir.mkdirs()) {\n+        if (!badDir.isDirectory()) {\n+          throw new IOException(\"Mkdirs failed to create \" + badDir.toString());\n+        }\n+      }\n+      String suffix = \".\" + rand.nextInt();\n+      File badFile = new File(badDir, f.getName()+suffix);\n+      LOG.warn(\"Moving bad file \" + f + \" to \" + badFile);\n+      in.close();                               // close it first\n+      boolean b = f.renameTo(badFile);                      // rename it\n+      if (!b) {\n+        LOG.warn(\"Ignoring failure of renameTo\");\n+      }\n+      // move checksum file too\n+      File checkFile = ((RawLocalFileSystem)fs).pathToFile(getChecksumFile(p));\n+      b = checkFile.renameTo(new File(badDir, checkFile.getName()+suffix));\n+      if (!b) {\n+          LOG.warn(\"Ignoring failure of renameTo\");\n+        }\n+    } catch (IOException e) {\n+      LOG.warn(\"Error moving bad file \" + p + \": \" + e);\n+    }\n+    return false;\n+  }\n+}"
        },
        {
            "sha": "c20b3d31d5d0a11ec6b0537c9674a44870d16e24",
            "filename": "src/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java",
            "status": "added",
            "additions": 113,
            "deletions": 0,
            "changes": 113,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FMD5MD5CRC32FileChecksum.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FMD5MD5CRC32FileChecksum.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FMD5MD5CRC32FileChecksum.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,113 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+\n+import org.apache.hadoop.io.MD5Hash;\n+import org.apache.hadoop.io.WritableUtils;\n+import org.xml.sax.Attributes;\n+import org.xml.sax.SAXException;\n+import org.znerd.xmlenc.XMLOutputter;\n+\n+/** MD5 of MD5 of CRC32. */\n+public class MD5MD5CRC32FileChecksum extends FileChecksum {\n+  public static final int LENGTH = MD5Hash.MD5_LEN\n+      + (Integer.SIZE + Long.SIZE)/Byte.SIZE;\n+\n+  private int bytesPerCRC;\n+  private long crcPerBlock;\n+  private MD5Hash md5;\n+\n+  /** Same as this(0, 0, null) */\n+  public MD5MD5CRC32FileChecksum() {\n+    this(0, 0, null);\n+  }\n+\n+  /** Create a MD5FileChecksum */\n+  public MD5MD5CRC32FileChecksum(int bytesPerCRC, long crcPerBlock, MD5Hash md5) {\n+    this.bytesPerCRC = bytesPerCRC;\n+    this.crcPerBlock = crcPerBlock;\n+    this.md5 = md5;\n+  }\n+  \n+  /** {@inheritDoc} */ \n+  public String getAlgorithmName() {\n+    return \"MD5-of-\" + crcPerBlock + \"MD5-of-\" + bytesPerCRC + \"CRC32\";\n+  }\n+\n+  /** {@inheritDoc} */ \n+  public int getLength() {return LENGTH;}\n+\n+  /** {@inheritDoc} */ \n+  public byte[] getBytes() {\n+    return WritableUtils.toByteArray(this);\n+  }\n+\n+  /** {@inheritDoc} */ \n+  public void readFields(DataInput in) throws IOException {\n+    bytesPerCRC = in.readInt();\n+    crcPerBlock = in.readLong();\n+    md5 = MD5Hash.read(in);\n+  }\n+\n+  /** {@inheritDoc} */ \n+  public void write(DataOutput out) throws IOException {\n+    out.writeInt(bytesPerCRC);\n+    out.writeLong(crcPerBlock);\n+    md5.write(out);    \n+  }\n+\n+  /** Write that object to xml output. */\n+  public static void write(XMLOutputter xml, MD5MD5CRC32FileChecksum that\n+      ) throws IOException {\n+    xml.startTag(MD5MD5CRC32FileChecksum.class.getName());\n+    if (that != null) {\n+      xml.attribute(\"bytesPerCRC\", \"\" + that.bytesPerCRC);\n+      xml.attribute(\"crcPerBlock\", \"\" + that.crcPerBlock);\n+      xml.attribute(\"md5\", \"\" + that.md5);\n+    }\n+    xml.endTag();\n+  }\n+\n+  /** Return the object represented in the attributes. */\n+  public static MD5MD5CRC32FileChecksum valueOf(Attributes attrs\n+      ) throws SAXException {\n+    final String bytesPerCRC = attrs.getValue(\"bytesPerCRC\");\n+    final String crcPerBlock = attrs.getValue(\"crcPerBlock\");\n+    final String md5 = attrs.getValue(\"md5\");\n+    if (bytesPerCRC == null || crcPerBlock == null || md5 == null) {\n+      return null;\n+    }\n+\n+    try {\n+      return new MD5MD5CRC32FileChecksum(Integer.valueOf(bytesPerCRC),\n+          Integer.valueOf(crcPerBlock), new MD5Hash(md5));\n+    } catch(Exception e) {\n+      throw new SAXException(\"Invalid attributes: bytesPerCRC=\" + bytesPerCRC\n+          + \", crcPerBlock=\" + crcPerBlock + \", md5=\" + md5, e);\n+    }\n+  }\n+\n+  /** {@inheritDoc} */ \n+  public String toString() {\n+    return getAlgorithmName() + \":\" + md5;\n+  }\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "cf96bf245150f9ab8971b891ee9735508e0db490",
            "filename": "src/java/org/apache/hadoop/fs/Path.java",
            "status": "added",
            "additions": 298,
            "deletions": 0,
            "changes": 298,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FPath.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FPath.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FPath.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,298 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs;\n+\n+import java.net.*;\n+import java.io.*;\n+\n+import org.apache.hadoop.conf.Configuration;\n+\n+/** Names a file or directory in a {@link FileSystem}.\n+ * Path strings use slash as the directory separator.  A path string is\n+ * absolute if it begins with a slash.\n+ */\n+public class Path implements Comparable {\n+\n+  /** The directory separator, a slash. */\n+  public static final String SEPARATOR = \"/\";\n+  public static final char SEPARATOR_CHAR = '/';\n+  \n+  public static final String CUR_DIR = \".\";\n+  \n+  static final boolean WINDOWS\n+    = System.getProperty(\"os.name\").startsWith(\"Windows\");\n+\n+  private URI uri;                                // a hierarchical uri\n+\n+  /** Resolve a child path against a parent path. */\n+  public Path(String parent, String child) {\n+    this(new Path(parent), new Path(child));\n+  }\n+\n+  /** Resolve a child path against a parent path. */\n+  public Path(Path parent, String child) {\n+    this(parent, new Path(child));\n+  }\n+\n+  /** Resolve a child path against a parent path. */\n+  public Path(String parent, Path child) {\n+    this(new Path(parent), child);\n+  }\n+\n+  /** Resolve a child path against a parent path. */\n+  public Path(Path parent, Path child) {\n+    // Add a slash to parent's path so resolution is compatible with URI's\n+    URI parentUri = parent.uri;\n+    String parentPath = parentUri.getPath();\n+    if (!(parentPath.equals(\"/\") || parentPath.equals(\"\")))\n+      try {\n+        parentUri = new URI(parentUri.getScheme(), parentUri.getAuthority(),\n+                            parentUri.getPath()+\"/\", null, null);\n+      } catch (URISyntaxException e) {\n+        throw new IllegalArgumentException(e);\n+      }\n+    URI resolved = parentUri.resolve(child.uri);\n+    initialize(resolved.getScheme(), resolved.getAuthority(),\n+               normalizePath(resolved.getPath()));\n+  }\n+\n+  private void checkPathArg( String path ) {\n+    // disallow construction of a Path from an empty string\n+    if ( path == null ) {\n+      throw new IllegalArgumentException(\n+          \"Can not create a Path from a null string\");\n+    }\n+    if( path.length() == 0 ) {\n+       throw new IllegalArgumentException(\n+           \"Can not create a Path from an empty string\");\n+    }   \n+  }\n+  \n+  /** Construct a path from a String.  Path strings are URIs, but with\n+   * unescaped elements and some additional normalization. */\n+  public Path(String pathString) {\n+    checkPathArg( pathString );\n+    \n+    // We can't use 'new URI(String)' directly, since it assumes things are\n+    // escaped, which we don't require of Paths. \n+    \n+    // add a slash in front of paths with Windows drive letters\n+    if (hasWindowsDrive(pathString, false))\n+      pathString = \"/\"+pathString;\n+\n+    // parse uri components\n+    String scheme = null;\n+    String authority = null;\n+\n+    int start = 0;\n+\n+    // parse uri scheme, if any\n+    int colon = pathString.indexOf(':');\n+    int slash = pathString.indexOf('/');\n+    if ((colon != -1) &&\n+        ((slash == -1) || (colon < slash))) {     // has a scheme\n+      scheme = pathString.substring(0, colon);\n+      start = colon+1;\n+    }\n+\n+    // parse uri authority, if any\n+    if (pathString.startsWith(\"//\", start) &&\n+        (pathString.length()-start > 2)) {       // has authority\n+      int nextSlash = pathString.indexOf('/', start+2);\n+      int authEnd = nextSlash > 0 ? nextSlash : pathString.length();\n+      authority = pathString.substring(start+2, authEnd);\n+      start = authEnd;\n+    }\n+\n+    // uri path is the rest of the string -- query & fragment not supported\n+    String path = pathString.substring(start, pathString.length());\n+\n+    initialize(scheme, authority, path);\n+  }\n+\n+  /** Construct a Path from components. */\n+  public Path(String scheme, String authority, String path) {\n+    checkPathArg( path );\n+    initialize(scheme, authority, path);\n+  }\n+\n+  private void initialize(String scheme, String authority, String path) {\n+    try {\n+      this.uri = new URI(scheme, authority, normalizePath(path), null, null)\n+        .normalize();\n+    } catch (URISyntaxException e) {\n+      throw new IllegalArgumentException(e);\n+    }\n+  }\n+\n+  private String normalizePath(String path) {\n+    // remove double slashes & backslashes\n+    path = path.replace(\"//\", \"/\");\n+    path = path.replace(\"\\\\\", \"/\");\n+    \n+    // trim trailing slash from non-root path (ignoring windows drive)\n+    int minLength = hasWindowsDrive(path, true) ? 4 : 1;\n+    if (path.length() > minLength && path.endsWith(\"/\")) {\n+      path = path.substring(0, path.length()-1);\n+    }\n+    \n+    return path;\n+  }\n+\n+  private boolean hasWindowsDrive(String path, boolean slashed) {\n+    if (!WINDOWS) return false;\n+    int start = slashed ? 1 : 0;\n+    return\n+      path.length() >= start+2 &&\n+      (slashed ? path.charAt(0) == '/' : true) &&\n+      path.charAt(start+1) == ':' &&\n+      ((path.charAt(start) >= 'A' && path.charAt(start) <= 'Z') ||\n+       (path.charAt(start) >= 'a' && path.charAt(start) <= 'z'));\n+  }\n+\n+\n+  /** Convert this to a URI. */\n+  public URI toUri() { return uri; }\n+\n+  /** Return the FileSystem that owns this Path. */\n+  public FileSystem getFileSystem(Configuration conf) throws IOException {\n+    return FileSystem.get(this.toUri(), conf);\n+  }\n+\n+  /** True if the directory of this path is absolute. */\n+  public boolean isAbsolute() {\n+    int start = hasWindowsDrive(uri.getPath(), true) ? 3 : 0;\n+    return uri.getPath().startsWith(SEPARATOR, start);\n+  }\n+\n+  /** Returns the final component of this path.*/\n+  public String getName() {\n+    String path = uri.getPath();\n+    int slash = path.lastIndexOf(SEPARATOR);\n+    return path.substring(slash+1);\n+  }\n+\n+  /** Returns the parent of a path or null if at root. */\n+  public Path getParent() {\n+    String path = uri.getPath();\n+    int lastSlash = path.lastIndexOf('/');\n+    int start = hasWindowsDrive(path, true) ? 3 : 0;\n+    if ((path.length() == start) ||               // empty path\n+        (lastSlash == start && path.length() == start+1)) { // at root\n+      return null;\n+    }\n+    String parent;\n+    if (lastSlash==-1) {\n+      parent = CUR_DIR;\n+    } else {\n+      int end = hasWindowsDrive(path, true) ? 3 : 0;\n+      parent = path.substring(0, lastSlash==end?end+1:lastSlash);\n+    }\n+    return new Path(uri.getScheme(), uri.getAuthority(), parent);\n+  }\n+\n+  /** Adds a suffix to the final name in the path.*/\n+  public Path suffix(String suffix) {\n+    return new Path(getParent(), getName()+suffix);\n+  }\n+\n+  public String toString() {\n+    // we can't use uri.toString(), which escapes everything, because we want\n+    // illegal characters unescaped in the string, for glob processing, etc.\n+    StringBuffer buffer = new StringBuffer();\n+    if (uri.getScheme() != null) {\n+      buffer.append(uri.getScheme());\n+      buffer.append(\":\");\n+    }\n+    if (uri.getAuthority() != null) {\n+      buffer.append(\"//\");\n+      buffer.append(uri.getAuthority());\n+    }\n+    if (uri.getPath() != null) {\n+      String path = uri.getPath();\n+      if (path.indexOf('/')==0 &&\n+          hasWindowsDrive(path, true) &&          // has windows drive\n+          uri.getScheme() == null &&              // but no scheme\n+          uri.getAuthority() == null)             // or authority\n+        path = path.substring(1);                 // remove slash before drive\n+      buffer.append(path);\n+    }\n+    return buffer.toString();\n+  }\n+\n+  public boolean equals(Object o) {\n+    if (!(o instanceof Path)) {\n+      return false;\n+    }\n+    Path that = (Path)o;\n+    return this.uri.equals(that.uri);\n+  }\n+\n+  public int hashCode() {\n+    return uri.hashCode();\n+  }\n+\n+  public int compareTo(Object o) {\n+    Path that = (Path)o;\n+    return this.uri.compareTo(that.uri);\n+  }\n+  \n+  /** Return the number of elements in this path. */\n+  public int depth() {\n+    String path = uri.getPath();\n+    int depth = 0;\n+    int slash = path.length()==1 && path.charAt(0)=='/' ? -1 : 0;\n+    while (slash != -1) {\n+      depth++;\n+      slash = path.indexOf(SEPARATOR, slash+1);\n+    }\n+    return depth;\n+  }\n+\n+  /** Returns a qualified path object. */\n+  public Path makeQualified(FileSystem fs) {\n+    Path path = this;\n+    if (!isAbsolute()) {\n+      path = new Path(fs.getWorkingDirectory(), this);\n+    }\n+\n+    URI pathUri = path.toUri();\n+    URI fsUri = fs.getUri();\n+      \n+    String scheme = pathUri.getScheme();\n+    String authority = pathUri.getAuthority();\n+\n+    if (scheme != null &&\n+        (authority != null || fsUri.getAuthority() == null))\n+      return path;\n+\n+    if (scheme == null) {\n+      scheme = fsUri.getScheme();\n+    }\n+\n+    if (authority == null) {\n+      authority = fsUri.getAuthority();\n+      if (authority == null) {\n+        authority = \"\";\n+      }\n+    }\n+\n+    return new Path(scheme+\":\"+\"//\"+authority + pathUri.getPath());\n+  }\n+}"
        },
        {
            "sha": "bcb7658943aee0cf30fc2eeaffcbf14165964b59",
            "filename": "src/java/org/apache/hadoop/fs/PathFilter.java",
            "status": "added",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FPathFilter.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FPathFilter.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FPathFilter.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,32 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+public interface PathFilter {\n+  /**\n+   * Tests whether or not the specified abstract pathname should be\n+   * included in a pathname list.\n+   *\n+   * @param  path  The abstract pathname to be tested\n+   * @return  <code>true</code> if and only if <code>pathname</code>\n+   *          should be included\n+   */\n+  boolean accept(Path path);\n+}\n+\n+"
        },
        {
            "sha": "d5af64e53e0215bbd4368678d12adb6a8b9414e4",
            "filename": "src/java/org/apache/hadoop/fs/PositionedReadable.java",
            "status": "added",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FPositionedReadable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FPositionedReadable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FPositionedReadable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,47 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.*;\n+import org.apache.hadoop.fs.*;\n+\n+/** Stream that permits positional reading. */\n+public interface PositionedReadable {\n+  /**\n+   * Read upto the specified number of bytes, from a given\n+   * position within a file, and return the number of bytes read. This does not\n+   * change the current offset of a file, and is thread-safe.\n+   */\n+  public int read(long position, byte[] buffer, int offset, int length)\n+    throws IOException;\n+  \n+  /**\n+   * Read the specified number of bytes, from a given\n+   * position within a file. This does not\n+   * change the current offset of a file, and is thread-safe.\n+   */\n+  public void readFully(long position, byte[] buffer, int offset, int length)\n+    throws IOException;\n+  \n+  /**\n+   * Read number of bytes equalt to the length of the buffer, from a given\n+   * position within a file. This does not\n+   * change the current offset of a file, and is thread-safe.\n+   */\n+  public void readFully(long position, byte[] buffer) throws IOException;\n+}"
        },
        {
            "sha": "4587136e8afa99de16842e32286d0efa561d62cc",
            "filename": "src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
            "status": "added",
            "additions": 496,
            "deletions": 0,
            "changes": 496,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FRawLocalFileSystem.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FRawLocalFileSystem.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FRawLocalFileSystem.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,496 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs;\n+\n+import java.io.BufferedOutputStream;\n+import java.io.DataOutput;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.nio.ByteBuffer;\n+import java.util.StringTokenizer;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.util.Progressable;\n+import org.apache.hadoop.util.Shell;\n+import org.apache.hadoop.util.StringUtils;\n+\n+/****************************************************************\n+ * Implement the FileSystem API for the raw local filesystem.\n+ *\n+ *****************************************************************/\n+public class RawLocalFileSystem extends FileSystem {\n+  static final URI NAME = URI.create(\"file:///\");\n+  private Path workingDir;\n+  \n+  public RawLocalFileSystem() {\n+    workingDir = new Path(System.getProperty(\"user.dir\")).makeQualified(this);\n+  }\n+  \n+  /** Convert a path to a File. */\n+  public File pathToFile(Path path) {\n+    checkPath(path);\n+    if (!path.isAbsolute()) {\n+      path = new Path(getWorkingDirectory(), path);\n+    }\n+    return new File(path.toUri().getPath());\n+  }\n+\n+  public URI getUri() { return NAME; }\n+  \n+  public void initialize(URI uri, Configuration conf) throws IOException {\n+    super.initialize(uri, conf);\n+    setConf(conf);\n+  }\n+  \n+  class TrackingFileInputStream extends FileInputStream {\n+    public TrackingFileInputStream(File f) throws IOException {\n+      super(f);\n+    }\n+    \n+    public int read() throws IOException {\n+      int result = super.read();\n+      if (result != -1) {\n+        statistics.incrementBytesRead(1);\n+      }\n+      return result;\n+    }\n+    \n+    public int read(byte[] data) throws IOException {\n+      int result = super.read(data);\n+      if (result != -1) {\n+        statistics.incrementBytesRead(result);\n+      }\n+      return result;\n+    }\n+    \n+    public int read(byte[] data, int offset, int length) throws IOException {\n+      int result = super.read(data, offset, length);\n+      if (result != -1) {\n+        statistics.incrementBytesRead(result);\n+      }\n+      return result;\n+    }\n+  }\n+\n+  /*******************************************************\n+   * For open()'s FSInputStream\n+   *******************************************************/\n+  class LocalFSFileInputStream extends FSInputStream {\n+    FileInputStream fis;\n+    private long position;\n+\n+    public LocalFSFileInputStream(Path f) throws IOException {\n+      this.fis = new TrackingFileInputStream(pathToFile(f));\n+    }\n+    \n+    public void seek(long pos) throws IOException {\n+      fis.getChannel().position(pos);\n+      this.position = pos;\n+    }\n+    \n+    public long getPos() throws IOException {\n+      return this.position;\n+    }\n+    \n+    public boolean seekToNewSource(long targetPos) throws IOException {\n+      return false;\n+    }\n+    \n+    /*\n+     * Just forward to the fis\n+     */\n+    public int available() throws IOException { return fis.available(); }\n+    public void close() throws IOException { fis.close(); }\n+    public boolean markSupport() { return false; }\n+    \n+    public int read() throws IOException {\n+      try {\n+        int value = fis.read();\n+        if (value >= 0) {\n+          this.position++;\n+        }\n+        return value;\n+      } catch (IOException e) {                 // unexpected exception\n+        throw new FSError(e);                   // assume native fs error\n+      }\n+    }\n+    \n+    public int read(byte[] b, int off, int len) throws IOException {\n+      try {\n+        int value = fis.read(b, off, len);\n+        if (value > 0) {\n+          this.position += value;\n+        }\n+        return value;\n+      } catch (IOException e) {                 // unexpected exception\n+        throw new FSError(e);                   // assume native fs error\n+      }\n+    }\n+    \n+    public int read(long position, byte[] b, int off, int len)\n+      throws IOException {\n+      ByteBuffer bb = ByteBuffer.wrap(b, off, len);\n+      try {\n+        return fis.getChannel().read(bb, position);\n+      } catch (IOException e) {\n+        throw new FSError(e);\n+      }\n+    }\n+    \n+    public long skip(long n) throws IOException {\n+      long value = fis.skip(n);\n+      if (value > 0) {\n+        this.position += value;\n+      }\n+      return value;\n+    }\n+  }\n+  \n+  public FSDataInputStream open(Path f, int bufferSize) throws IOException {\n+    if (!exists(f)) {\n+      throw new FileNotFoundException(f.toString());\n+    }\n+    return new FSDataInputStream(new BufferedFSInputStream(\n+        new LocalFSFileInputStream(f), bufferSize));\n+  }\n+  \n+  /*********************************************************\n+   * For create()'s FSOutputStream.\n+   *********************************************************/\n+  class LocalFSFileOutputStream extends OutputStream implements Syncable {\n+    FileOutputStream fos;\n+    \n+    private LocalFSFileOutputStream(Path f, boolean append) throws IOException {\n+      this.fos = new FileOutputStream(pathToFile(f), append);\n+    }\n+    \n+    /*\n+     * Just forward to the fos\n+     */\n+    public void close() throws IOException { fos.close(); }\n+    public void flush() throws IOException { fos.flush(); }\n+    public void write(byte[] b, int off, int len) throws IOException {\n+      try {\n+        fos.write(b, off, len);\n+      } catch (IOException e) {                // unexpected exception\n+        throw new FSError(e);                  // assume native fs error\n+      }\n+    }\n+    \n+    public void write(int b) throws IOException {\n+      try {\n+        fos.write(b);\n+      } catch (IOException e) {              // unexpected exception\n+        throw new FSError(e);                // assume native fs error\n+      }\n+    }\n+\n+    /** {@inheritDoc} */\n+    public void sync() throws IOException {\n+      fos.getFD().sync();      \n+    }\n+  }\n+  \n+  /** {@inheritDoc} */\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    if (!exists(f)) {\n+      throw new FileNotFoundException(\"File \" + f + \" not found.\");\n+    }\n+    if (getFileStatus(f).isDir()) {\n+      throw new IOException(\"Cannot append to a diretory (=\" + f + \" ).\");\n+    }\n+    return new FSDataOutputStream(new BufferedOutputStream(\n+        new LocalFSFileOutputStream(f, true), bufferSize), statistics);\n+  }\n+\n+  /** {@inheritDoc} */\n+  public FSDataOutputStream create(Path f, boolean overwrite, int bufferSize,\n+                                   short replication, long blockSize, Progressable progress)\n+    throws IOException {\n+    if (exists(f) && !overwrite) {\n+      throw new IOException(\"File already exists:\"+f);\n+    }\n+    Path parent = f.getParent();\n+    if (parent != null && !mkdirs(parent)) {\n+      throw new IOException(\"Mkdirs failed to create \" + parent.toString());\n+    }\n+    return new FSDataOutputStream(new BufferedOutputStream(\n+        new LocalFSFileOutputStream(f, false), bufferSize), statistics);\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize, short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    FSDataOutputStream out = create(f,\n+        overwrite, bufferSize, replication, blockSize, progress);\n+    setPermission(f, permission);\n+    return out;\n+  }\n+  \n+  public boolean rename(Path src, Path dst) throws IOException {\n+    if (pathToFile(src).renameTo(pathToFile(dst))) {\n+      return true;\n+    }\n+    return FileUtil.copy(this, src, this, dst, true, getConf());\n+  }\n+  \n+  public boolean delete(Path p, boolean recursive) throws IOException {\n+    File f = pathToFile(p);\n+    if (f.isFile()) {\n+      return f.delete();\n+    } else if ((!recursive) && f.isDirectory() && \n+        (f.listFiles().length != 0)) {\n+      throw new IOException(\"Directory \" + f.toString() + \" is not empty\");\n+    }\n+    return FileUtil.fullyDelete(f);\n+  }\n+ \n+  public FileStatus[] listStatus(Path f) throws IOException {\n+    File localf = pathToFile(f);\n+    FileStatus[] results;\n+\n+    if (!localf.exists()) {\n+      return null;\n+    }\n+    if (localf.isFile()) {\n+      return new FileStatus[] {\n+          new RawLocalFileStatus(localf, getDefaultBlockSize(), this) };\n+    }\n+\n+    String[] names = localf.list();\n+    if (names == null) {\n+      return null;\n+    }\n+    results = new FileStatus[names.length];\n+    for (int i = 0; i < names.length; i++) {\n+      results[i] = getFileStatus(new Path(f, names[i]));\n+    }\n+    return results;\n+  }\n+\n+  /**\n+   * Creates the specified directory hierarchy. Does not\n+   * treat existence as an error.\n+   */\n+  public boolean mkdirs(Path f) throws IOException {\n+    Path parent = f.getParent();\n+    File p2f = pathToFile(f);\n+    return (parent == null || mkdirs(parent)) &&\n+      (p2f.mkdir() || p2f.isDirectory());\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n+    boolean b = mkdirs(f);\n+    setPermission(f, permission);\n+    return b;\n+  }\n+  \n+  @Override\n+  public Path getHomeDirectory() {\n+    return new Path(System.getProperty(\"user.home\")).makeQualified(this);\n+  }\n+\n+  /**\n+   * Set the working directory to the given directory.\n+   */\n+  @Override\n+  public void setWorkingDirectory(Path newDir) {\n+    workingDir = newDir;\n+  }\n+  \n+  @Override\n+  public Path getWorkingDirectory() {\n+    return workingDir;\n+  }\n+\n+  /** {@inheritDoc} */\n+  @Override\n+  public FsStatus getStatus(Path p) throws IOException {\n+    File partition = pathToFile(p == null ? new Path(\"/\") : p);\n+    //File provides getUsableSpace() and getFreeSpace()\n+    //File provides no API to obtain used space, assume used = total - free\n+    return new FsStatus(partition.getTotalSpace(), \n+      partition.getTotalSpace() - partition.getFreeSpace(),\n+      partition.getFreeSpace());\n+  }\n+  \n+  // In the case of the local filesystem, we can just rename the file.\n+  public void moveFromLocalFile(Path src, Path dst) throws IOException {\n+    rename(src, dst);\n+  }\n+  \n+  // We can write output directly to the final location\n+  public Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile)\n+    throws IOException {\n+    return fsOutputFile;\n+  }\n+  \n+  // It's in the right place - nothing to do.\n+  public void completeLocalOutput(Path fsWorkingFile, Path tmpLocalFile)\n+    throws IOException {\n+  }\n+  \n+  public void close() throws IOException {\n+    super.close();\n+  }\n+  \n+  public String toString() {\n+    return \"LocalFS\";\n+  }\n+  \n+  public FileStatus getFileStatus(Path f) throws IOException {\n+    File path = pathToFile(f);\n+    if (path.exists()) {\n+      return new RawLocalFileStatus(pathToFile(f), getDefaultBlockSize(), this);\n+    } else {\n+      throw new FileNotFoundException( \"File \" + f + \" does not exist.\");\n+    }\n+  }\n+\n+  static class RawLocalFileStatus extends FileStatus {\n+    /* We can add extra fields here. It breaks at least CopyFiles.FilePair().\n+     * We recognize if the information is already loaded by check if\n+     * onwer.equals(\"\").\n+     */\n+    private boolean isPermissionLoaded() {\n+      return !super.getOwner().equals(\"\"); \n+    }\n+    \n+    RawLocalFileStatus(File f, long defaultBlockSize, FileSystem fs) {\n+      super(f.length(), f.isDirectory(), 1, defaultBlockSize,\n+            f.lastModified(), new Path(f.getPath()).makeQualified(fs));\n+    }\n+    \n+    @Override\n+    public FsPermission getPermission() {\n+      if (!isPermissionLoaded()) {\n+        loadPermissionInfo();\n+      }\n+      return super.getPermission();\n+    }\n+\n+    @Override\n+    public String getOwner() {\n+      if (!isPermissionLoaded()) {\n+        loadPermissionInfo();\n+      }\n+      return super.getOwner();\n+    }\n+\n+    @Override\n+    public String getGroup() {\n+      if (!isPermissionLoaded()) {\n+        loadPermissionInfo();\n+      }\n+      return super.getGroup();\n+    }\n+\n+    /// loads permissions, owner, and group from `ls -ld`\n+    private void loadPermissionInfo() {\n+      IOException e = null;\n+      try {\n+        StringTokenizer t = new StringTokenizer(\n+            execCommand(new File(getPath().toUri()), \n+                        Shell.getGET_PERMISSION_COMMAND()));\n+        //expected format\n+        //-rw-------    1 username groupname ...\n+        String permission = t.nextToken();\n+        if (permission.length() > 10) { //files with ACLs might have a '+'\n+          permission = permission.substring(0, 10);\n+        }\n+        setPermission(FsPermission.valueOf(permission));\n+        t.nextToken();\n+        setOwner(t.nextToken());\n+        setGroup(t.nextToken());\n+      } catch (Shell.ExitCodeException ioe) {\n+        if (ioe.getExitCode() != 1) {\n+          e = ioe;\n+        } else {\n+          setPermission(null);\n+          setOwner(null);\n+          setGroup(null);\n+        }\n+      } catch (IOException ioe) {\n+        e = ioe;\n+      } finally {\n+        if (e != null) {\n+          throw new RuntimeException(\"Error while running command to get \" +\n+                                     \"file permissions : \" + \n+                                     StringUtils.stringifyException(e));\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public void write(DataOutput out) throws IOException {\n+      if (!isPermissionLoaded()) {\n+        loadPermissionInfo();\n+      }\n+      super.write(out);\n+    }\n+  }\n+\n+  /**\n+   * Use the command chown to set owner.\n+   */\n+  @Override\n+  public void setOwner(Path p, String username, String groupname\n+      ) throws IOException {\n+    if (username == null && groupname == null) {\n+      throw new IOException(\"username == null && groupname == null\");\n+    }\n+\n+    if (username == null) {\n+      execCommand(pathToFile(p), Shell.SET_GROUP_COMMAND, groupname); \n+    } else {\n+      //OWNER[:[GROUP]]\n+      String s = username + (groupname == null? \"\": \":\" + groupname);\n+      execCommand(pathToFile(p), Shell.SET_OWNER_COMMAND, s);\n+    }\n+  }\n+\n+  /**\n+   * Use the command chmod to set permission.\n+   */\n+  @Override\n+  public void setPermission(Path p, FsPermission permission\n+      ) throws IOException {\n+    execCommand(pathToFile(p), Shell.SET_PERMISSION_COMMAND,\n+        String.format(\"%05o\", permission.toShort()));\n+  }\n+\n+  private static String execCommand(File f, String... cmd) throws IOException {\n+    String[] args = new String[cmd.length + 1];\n+    System.arraycopy(cmd, 0, args, 0, cmd.length);\n+    args[cmd.length] = f.getCanonicalPath();\n+    String output = Shell.execCommand(args);\n+    return output;\n+  }\n+}"
        },
        {
            "sha": "20e7508851424f0bef3ac71e4a7df513ae5b6f7a",
            "filename": "src/java/org/apache/hadoop/fs/Seekable.java",
            "status": "added",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FSeekable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FSeekable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FSeekable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,41 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.io.*;\n+\n+/** Stream that permits seeking. */\n+public interface Seekable {\n+  /**\n+   * Seek to the given offset from the start of the file.\n+   * The next read() will be from that location.  Can't\n+   * seek past the end of the file.\n+   */\n+  void seek(long pos) throws IOException;\n+  \n+  /**\n+   * Return the current offset from the start of the file\n+   */\n+  long getPos() throws IOException;\n+\n+  /**\n+   * Seeks a different copy of the data.  Returns true if \n+   * found a new source, false otherwise.\n+   */\n+  boolean seekToNewSource(long targetPos) throws IOException;\n+}"
        },
        {
            "sha": "650d224e3e999edcb62e9f88e4f8f428844e3b8c",
            "filename": "src/java/org/apache/hadoop/fs/Syncable.java",
            "status": "added",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FSyncable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FSyncable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FSyncable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,30 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs;\n+\n+import java.io.IOException;\n+\n+/** This interface declare the sync() operation. */\n+public interface Syncable {\n+  /**\n+   * Synchronize all buffer with the underlying devices.\n+   * @throws IOException\n+   */\n+  public void sync() throws IOException;\n+}"
        },
        {
            "sha": "5b062a1ece5a4a1d99724b550040a9cef3ba86ed",
            "filename": "src/java/org/apache/hadoop/fs/Trash.java",
            "status": "added",
            "additions": 291,
            "deletions": 0,
            "changes": 291,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FTrash.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FTrash.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2FTrash.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,291 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs;\n+\n+import java.text.*;\n+import java.io.*;\n+import java.util.Date;\n+\n+import org.apache.commons.logging.*;\n+\n+import org.apache.hadoop.conf.*;\n+import org.apache.hadoop.fs.permission.*;\n+import org.apache.hadoop.util.StringUtils;\n+\n+/** Provides a <i>trash</i> feature.  Files are moved to a user's trash\n+ * directory, a subdirectory of their home directory named \".Trash\".  Files are\n+ * initially moved to a <i>current</i> sub-directory of the trash directory.\n+ * Within that sub-directory their original path is preserved.  Periodically\n+ * one may checkpoint the current trash and remove older checkpoints.  (This\n+ * design permits trash management without enumeration of the full trash\n+ * content, without date support in the filesystem, and without clock\n+ * synchronization.)\n+ */\n+public class Trash extends Configured {\n+  private static final Log LOG =\n+    LogFactory.getLog(Trash.class);\n+\n+  private static final Path CURRENT = new Path(\"Current\");\n+  private static final Path TRASH = new Path(\".Trash/\");\n+  private static final Path HOMES = new Path(\"/user/\");\n+\n+  private static final FsPermission PERMISSION =\n+    new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE);\n+\n+  private static final DateFormat CHECKPOINT = new SimpleDateFormat(\"yyMMddHHmm\");\n+  private static final int MSECS_PER_MINUTE = 60*1000;\n+\n+  private final FileSystem fs;\n+  private final Path trash;\n+  private final Path current;\n+  private final long interval;\n+\n+  /** Construct a trash can accessor.\n+   * @param conf a Configuration\n+   */\n+  public Trash(Configuration conf) throws IOException {\n+    this(FileSystem.get(conf), conf);\n+  }\n+\n+  /**\n+   * Construct a trash can accessor for the FileSystem provided.\n+   */\n+  public Trash(FileSystem fs, Configuration conf) throws IOException {\n+    super(conf);\n+    this.fs = fs;\n+    this.trash = new Path(fs.getHomeDirectory(), TRASH);\n+    this.current = new Path(trash, CURRENT);\n+    this.interval = conf.getLong(\"fs.trash.interval\", 60) * MSECS_PER_MINUTE;\n+  }\n+\n+  private Trash(Path home, Configuration conf) throws IOException {\n+    super(conf);\n+    this.fs = home.getFileSystem(conf);\n+    this.trash = new Path(home, TRASH);\n+    this.current = new Path(trash, CURRENT);\n+    this.interval = conf.getLong(\"fs.trash.interval\", 60) * MSECS_PER_MINUTE;\n+  }\n+  \n+  private Path makeTrashRelativePath(Path basePath, Path rmFilePath) {\n+    return new Path(basePath + rmFilePath.toUri().getPath());\n+  }\n+\n+  /** Move a file or directory to the current trash directory.\n+   * @return false if the item is already in the trash or trash is disabled\n+   */ \n+  public boolean moveToTrash(Path path) throws IOException {\n+    if (interval == 0)\n+      return false;\n+\n+    if (!path.isAbsolute())                       // make path absolute\n+      path = new Path(fs.getWorkingDirectory(), path);\n+\n+    if (!fs.exists(path))                         // check that path exists\n+      throw new FileNotFoundException(path.toString());\n+\n+    String qpath = path.makeQualified(fs).toString();\n+\n+    if (qpath.startsWith(trash.toString())) {\n+      return false;                               // already in trash\n+    }\n+\n+    if (trash.getParent().toString().startsWith(qpath)) {\n+      throw new IOException(\"Cannot move \\\"\" + path +\n+                            \"\\\" to the trash, as it contains the trash\");\n+    }\n+\n+    Path trashPath = makeTrashRelativePath(current, path);\n+    Path baseTrashPath = makeTrashRelativePath(current, path.getParent());\n+    \n+    IOException cause = null;\n+\n+    // try twice, in case checkpoint between the mkdirs() & rename()\n+    for (int i = 0; i < 2; i++) {\n+      try {\n+        if (!fs.mkdirs(baseTrashPath, PERMISSION)) {      // create current\n+          LOG.warn(\"Can't create trash directory: \"+baseTrashPath);\n+          return false;\n+        }\n+      } catch (IOException e) {\n+        LOG.warn(\"Can't create trash directory: \"+baseTrashPath);\n+        return false;\n+      }\n+      try {\n+        //\n+        // if the target path in Trash already exists, then append with \n+        // a number. Start from 1.\n+        //\n+        String orig = trashPath.toString();\n+        for (int j = 1; fs.exists(trashPath); j++) {\n+          trashPath = new Path(orig + \".\" + j);\n+        }\n+        if (fs.rename(path, trashPath))           // move to current trash\n+          return true;\n+      } catch (IOException e) {\n+        cause = e;\n+      }\n+    }\n+    throw (IOException)\n+      new IOException(\"Failed to move to trash: \"+path).initCause(cause);\n+  }\n+\n+  /** Create a trash checkpoint. */\n+  public void checkpoint() throws IOException {\n+    if (!fs.exists(current))                      // no trash, no checkpoint\n+      return;\n+\n+    Path checkpoint;\n+    synchronized (CHECKPOINT) {\n+      checkpoint = new Path(trash, CHECKPOINT.format(new Date()));\n+    }\n+\n+    if (fs.rename(current, checkpoint)) {\n+      LOG.info(\"Created trash checkpoint: \"+checkpoint.toUri().getPath());\n+    } else {\n+      throw new IOException(\"Failed to checkpoint trash: \"+checkpoint);\n+    }\n+  }\n+\n+  /** Delete old checkpoints. */\n+  public void expunge() throws IOException {\n+    FileStatus[] dirs = fs.listStatus(trash);            // scan trash sub-directories\n+    if( dirs == null){\n+      return;\n+    }\n+    long now = System.currentTimeMillis();\n+    for (int i = 0; i < dirs.length; i++) {\n+      Path path = dirs[i].getPath();\n+      String dir = path.toUri().getPath();\n+      String name = path.getName();\n+      if (name.equals(CURRENT.getName()))         // skip current\n+        continue;\n+\n+      long time;\n+      try {\n+        synchronized (CHECKPOINT) {\n+          time = CHECKPOINT.parse(name).getTime();\n+        }\n+      } catch (ParseException e) {\n+        LOG.warn(\"Unexpected item in trash: \"+dir+\". Ignoring.\");\n+        continue;\n+      }\n+\n+      if ((now - interval) > time) {\n+        if (fs.delete(path, true)) {\n+          LOG.info(\"Deleted trash checkpoint: \"+dir);\n+        } else {\n+          LOG.warn(\"Couldn't delete checkpoint: \"+dir+\" Ignoring.\");\n+        }\n+      }\n+    }\n+  }\n+\n+  //\n+  // get the current working directory\n+  //\n+  Path getCurrentTrashDir() {\n+    return current;\n+  }\n+\n+  /** Return a {@link Runnable} that periodically empties the trash of all\n+   * users, intended to be run by the superuser.  Only one checkpoint is kept\n+   * at a time.\n+   */\n+  public Runnable getEmptier() throws IOException {\n+    return new Emptier(getConf());\n+  }\n+\n+  private class Emptier implements Runnable {\n+\n+    private Configuration conf;\n+    private long interval;\n+\n+    Emptier(Configuration conf) throws IOException {\n+      this.conf = conf;\n+      this.interval = conf.getLong(\"fs.trash.interval\", 0) * MSECS_PER_MINUTE;\n+    }\n+\n+    public void run() {\n+      if (interval == 0)\n+        return;                                   // trash disabled\n+\n+      long now = System.currentTimeMillis();\n+      long end;\n+      while (true) {\n+        end = ceiling(now, interval);\n+        try {                                     // sleep for interval\n+          Thread.sleep(end - now);\n+        } catch (InterruptedException e) {\n+          break;                                  // exit on interrupt\n+        }\n+          \n+        try {\n+          now = System.currentTimeMillis();\n+          if (now >= end) {\n+\n+            FileStatus[] homes = null;\n+            try {\n+              homes = fs.listStatus(HOMES);         // list all home dirs\n+            } catch (IOException e) {\n+              LOG.warn(\"Trash can't list homes: \"+e+\" Sleeping.\");\n+              continue;\n+            }\n+\n+            if (homes == null)\n+              continue;\n+\n+            for (FileStatus home : homes) {         // dump each trash\n+              if (!home.isDir())\n+                continue;\n+              try {\n+                Trash trash = new Trash(home.getPath(), conf);\n+                trash.expunge();\n+                trash.checkpoint();\n+              } catch (IOException e) {\n+                LOG.warn(\"Trash caught: \"+e+\". Skipping \"+home.getPath()+\".\");\n+              } \n+            }\n+          }\n+        } catch (Exception e) {\n+          LOG.warn(\"RuntimeException during Trash.Emptier.run() \" + \n+                   StringUtils.stringifyException(e));\n+        }\n+      }\n+      try {\n+        fs.close();\n+      } catch(IOException e) {\n+        LOG.warn(\"Trash cannot close FileSystem. \" +\n+            StringUtils.stringifyException(e));\n+      }\n+    }\n+\n+    private long ceiling(long time, long interval) {\n+      return floor(time, interval) + interval;\n+    }\n+    private long floor(long time, long interval) {\n+      return (time / interval) * interval;\n+    }\n+\n+  }\n+\n+  /** Run an emptier.*/\n+  public static void main(String[] args) throws Exception {\n+    new Trash(new Configuration()).getEmptier().run();\n+  }\n+\n+}"
        },
        {
            "sha": "c76cb57f3c81b09440b0599c7d2aa818315195d2",
            "filename": "src/java/org/apache/hadoop/fs/ftp/FTPException.java",
            "status": "added",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fftp%2FFTPException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fftp%2FFTPException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fftp%2FFTPException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,38 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ftp;\n+\n+/**\n+ * A class to wrap a {@link Throwable} into a Runtime Exception.\n+ */\n+public class FTPException extends RuntimeException {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  public FTPException(String message) {\n+    super(message);\n+  }\n+\n+  public FTPException(Throwable t) {\n+    super(t);\n+  }\n+\n+  public FTPException(String message, Throwable t) {\n+    super(message, t);\n+  }\n+}"
        },
        {
            "sha": "ee91f1c899f704c1e4c8ed4ede345623fa6201ed",
            "filename": "src/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java",
            "status": "added",
            "additions": 576,
            "deletions": 0,
            "changes": 576,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fftp%2FFTPFileSystem.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fftp%2FFTPFileSystem.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fftp%2FFTPFileSystem.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,576 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ftp;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.commons.net.ftp.FTP;\n+import org.apache.commons.net.ftp.FTPClient;\n+import org.apache.commons.net.ftp.FTPFile;\n+import org.apache.commons.net.ftp.FTPReply;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.util.Progressable;\n+\n+/**\n+ * <p>\n+ * A {@link FileSystem} backed by an FTP client provided by <a\n+ * href=\"http://commons.apache.org/net/\">Apache Commons Net</a>.\n+ * </p>\n+ */\n+public class FTPFileSystem extends FileSystem {\n+\n+  public static final Log LOG = LogFactory\n+      .getLog(FTPFileSystem.class);\n+\n+  public static final int DEFAULT_BUFFER_SIZE = 1024 * 1024;\n+\n+  public static final int DEFAULT_BLOCK_SIZE = 4 * 1024;\n+\n+  private URI uri;\n+\n+  @Override\n+  public void initialize(URI uri, Configuration conf) throws IOException { // get\n+    super.initialize(uri, conf);\n+    // get host information from uri (overrides info in conf)\n+    String host = uri.getHost();\n+    host = (host == null) ? conf.get(\"fs.ftp.host\", null) : host;\n+    if (host == null) {\n+      throw new IOException(\"Invalid host specified\");\n+    }\n+    conf.set(\"fs.ftp.host\", host);\n+\n+    // get port information from uri, (overrides info in conf)\n+    int port = uri.getPort();\n+    port = (port == -1) ? FTP.DEFAULT_PORT : port;\n+    conf.setInt(\"fs.ftp.host.port\", port);\n+\n+    // get user/password information from URI (overrides info in conf)\n+    String userAndPassword = uri.getUserInfo();\n+    if (userAndPassword == null) {\n+      userAndPassword = (conf.get(\"fs.ftp.user.\" + host, null) + \":\" + conf\n+          .get(\"fs.ftp.password.\" + host, null));\n+      if (userAndPassword == null) {\n+        throw new IOException(\"Invalid user/passsword specified\");\n+      }\n+    }\n+    String[] userPasswdInfo = userAndPassword.split(\":\");\n+    conf.set(\"fs.ftp.user.\" + host, userPasswdInfo[0]);\n+    if (userPasswdInfo.length > 1) {\n+      conf.set(\"fs.ftp.password.\" + host, userPasswdInfo[1]);\n+    } else {\n+      conf.set(\"fs.ftp.password.\" + host, null);\n+    }\n+    setConf(conf);\n+    this.uri = uri;\n+  }\n+\n+  /**\n+   * Connect to the FTP server using configuration parameters *\n+   * \n+   * @return An FTPClient instance\n+   * @throws IOException\n+   */\n+  private FTPClient connect() throws IOException {\n+    FTPClient client = null;\n+    Configuration conf = getConf();\n+    String host = conf.get(\"fs.ftp.host\");\n+    int port = conf.getInt(\"fs.ftp.host.port\", FTP.DEFAULT_PORT);\n+    String user = conf.get(\"fs.ftp.user.\" + host);\n+    String password = conf.get(\"fs.ftp.password.\" + host);\n+    client = new FTPClient();\n+    client.connect(host, port);\n+    int reply = client.getReplyCode();\n+    if (!FTPReply.isPositiveCompletion(reply)) {\n+      throw new IOException(\"Server - \" + host\n+          + \" refused connection on port - \" + port);\n+    } else if (client.login(user, password)) {\n+      client.setFileTransferMode(FTP.BLOCK_TRANSFER_MODE);\n+      client.setFileType(FTP.BINARY_FILE_TYPE);\n+      client.setBufferSize(DEFAULT_BUFFER_SIZE);\n+    } else {\n+      throw new IOException(\"Login failed on server - \" + host + \", port - \"\n+          + port);\n+    }\n+\n+    return client;\n+  }\n+\n+  /**\n+   * Logout and disconnect the given FTPClient. *\n+   * \n+   * @param client\n+   * @throws IOException\n+   */\n+  private void disconnect(FTPClient client) throws IOException {\n+    if (client != null) {\n+      if (!client.isConnected()) {\n+        throw new FTPException(\"Client not connected\");\n+      }\n+      boolean logoutSuccess = client.logout();\n+      client.disconnect();\n+      if (!logoutSuccess) {\n+        LOG.warn(\"Logout failed while disconnecting, error code - \"\n+            + client.getReplyCode());\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Resolve against given working directory. *\n+   * \n+   * @param workDir\n+   * @param path\n+   * @return\n+   */\n+  private Path makeAbsolute(Path workDir, Path path) {\n+    if (path.isAbsolute()) {\n+      return path;\n+    }\n+    return new Path(workDir, path);\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path file, int bufferSize) throws IOException {\n+    FTPClient client = connect();\n+    Path workDir = new Path(client.printWorkingDirectory());\n+    Path absolute = makeAbsolute(workDir, file);\n+    FileStatus fileStat = getFileStatus(client, absolute);\n+    if (fileStat.isDir()) {\n+      disconnect(client);\n+      throw new IOException(\"Path \" + file + \" is a directory.\");\n+    }\n+    client.allocate(bufferSize);\n+    Path parent = absolute.getParent();\n+    // Change to parent directory on the\n+    // server. Only then can we read the\n+    // file\n+    // on the server by opening up an InputStream. As a side effect the working\n+    // directory on the server is changed to the parent directory of the file.\n+    // The FTP client connection is closed when close() is called on the\n+    // FSDataInputStream.\n+    client.changeWorkingDirectory(parent.toUri().getPath());\n+    InputStream is = client.retrieveFileStream(file.getName());\n+    FSDataInputStream fis = new FSDataInputStream(new FTPInputStream(is,\n+        client, statistics));\n+    if (!FTPReply.isPositivePreliminary(client.getReplyCode())) {\n+      // The ftpClient is an inconsistent state. Must close the stream\n+      // which in turn will logout and disconnect from FTP server\n+      fis.close();\n+      throw new IOException(\"Unable to open file: \" + file + \", Aborting\");\n+    }\n+    return fis;\n+  }\n+\n+  /**\n+   * A stream obtained via this call must be closed before using other APIs of\n+   * this class or else the invocation will block.\n+   */\n+  @Override\n+  public FSDataOutputStream create(Path file, FsPermission permission,\n+      boolean overwrite, int bufferSize, short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+    final FTPClient client = connect();\n+    Path workDir = new Path(client.printWorkingDirectory());\n+    Path absolute = makeAbsolute(workDir, file);\n+    if (exists(client, file)) {\n+      if (overwrite) {\n+        delete(client, file);\n+      } else {\n+        disconnect(client);\n+        throw new IOException(\"File already exists: \" + file);\n+      }\n+    }\n+    Path parent = absolute.getParent();\n+    if (parent == null || !mkdirs(client, parent, FsPermission.getDefault())) {\n+      parent = (parent == null) ? new Path(\"/\") : parent;\n+      disconnect(client);\n+      throw new IOException(\"create(): Mkdirs failed to create: \" + parent);\n+    }\n+    client.allocate(bufferSize);\n+    // Change to parent directory on the server. Only then can we write to the\n+    // file on the server by opening up an OutputStream. As a side effect the\n+    // working directory on the server is changed to the parent directory of the\n+    // file. The FTP client connection is closed when close() is called on the\n+    // FSDataOutputStream.\n+    client.changeWorkingDirectory(parent.toUri().getPath());\n+    FSDataOutputStream fos = new FSDataOutputStream(client.storeFileStream(file\n+        .getName()), statistics) {\n+      @Override\n+      public void close() throws IOException {\n+        super.close();\n+        if (!client.isConnected()) {\n+          throw new FTPException(\"Client not connected\");\n+        }\n+        boolean cmdCompleted = client.completePendingCommand();\n+        disconnect(client);\n+        if (!cmdCompleted) {\n+          throw new FTPException(\"Could not complete transfer, Reply Code - \"\n+              + client.getReplyCode());\n+        }\n+      }\n+    };\n+    if (!FTPReply.isPositivePreliminary(client.getReplyCode())) {\n+      // The ftpClient is an inconsistent state. Must close the stream\n+      // which in turn will logout and disconnect from FTP server\n+      fos.close();\n+      throw new IOException(\"Unable to create file: \" + file + \", Aborting\");\n+    }\n+    return fos;\n+  }\n+\n+  /** This optional operation is not yet supported. */\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new IOException(\"Not supported\");\n+  }\n+  \n+  /**\n+   * Convenience method, so that we don't open a new connection when using this\n+   * method from within another method. Otherwise every API invocation incurs\n+   * the overhead of opening/closing a TCP connection.\n+   */\n+  private boolean exists(FTPClient client, Path file) {\n+    try {\n+      return getFileStatus(client, file) != null;\n+    } catch (FileNotFoundException fnfe) {\n+      return false;\n+    } catch (IOException ioe) {\n+      throw new FTPException(\"Failed to get file status\", ioe);\n+    }\n+  }\n+\n+  @Override\n+  public boolean delete(Path file, boolean recursive) throws IOException {\n+    FTPClient client = connect();\n+    try {\n+      boolean success = delete(client, file, recursive);\n+      return success;\n+    } finally {\n+      disconnect(client);\n+    }\n+  }\n+\n+  /** @deprecated Use delete(Path, boolean) instead */\n+  @Deprecated\n+  private boolean delete(FTPClient client, Path file) throws IOException {\n+    return delete(client, file, false);\n+  }\n+\n+  /**\n+   * Convenience method, so that we don't open a new connection when using this\n+   * method from within another method. Otherwise every API invocation incurs\n+   * the overhead of opening/closing a TCP connection.\n+   */\n+  private boolean delete(FTPClient client, Path file, boolean recursive)\n+      throws IOException {\n+    Path workDir = new Path(client.printWorkingDirectory());\n+    Path absolute = makeAbsolute(workDir, file);\n+    String pathName = absolute.toUri().getPath();\n+    FileStatus fileStat = getFileStatus(client, absolute);\n+    if (!fileStat.isDir()) {\n+      return client.deleteFile(pathName);\n+    }\n+    FileStatus[] dirEntries = listStatus(client, absolute);\n+    if (dirEntries != null && dirEntries.length > 0 && !(recursive)) {\n+      throw new IOException(\"Directory: \" + file + \" is not empty.\");\n+    }\n+    if (dirEntries != null) {\n+      for (int i = 0; i < dirEntries.length; i++) {\n+        delete(client, new Path(absolute, dirEntries[i].getPath()), recursive);\n+      }\n+    }\n+    return client.removeDirectory(pathName);\n+  }\n+\n+  private FsAction getFsAction(int accessGroup, FTPFile ftpFile) {\n+    FsAction action = FsAction.NONE;\n+    if (ftpFile.hasPermission(accessGroup, FTPFile.READ_PERMISSION)) {\n+      action.or(FsAction.READ);\n+    }\n+    if (ftpFile.hasPermission(accessGroup, FTPFile.WRITE_PERMISSION)) {\n+      action.or(FsAction.WRITE);\n+    }\n+    if (ftpFile.hasPermission(accessGroup, FTPFile.EXECUTE_PERMISSION)) {\n+      action.or(FsAction.EXECUTE);\n+    }\n+    return action;\n+  }\n+\n+  private FsPermission getPermissions(FTPFile ftpFile) {\n+    FsAction user, group, others;\n+    user = getFsAction(FTPFile.USER_ACCESS, ftpFile);\n+    group = getFsAction(FTPFile.GROUP_ACCESS, ftpFile);\n+    others = getFsAction(FTPFile.WORLD_ACCESS, ftpFile);\n+    return new FsPermission(user, group, others);\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public FileStatus[] listStatus(Path file) throws IOException {\n+    FTPClient client = connect();\n+    try {\n+      FileStatus[] stats = listStatus(client, file);\n+      return stats;\n+    } finally {\n+      disconnect(client);\n+    }\n+  }\n+\n+  /**\n+   * Convenience method, so that we don't open a new connection when using this\n+   * method from within another method. Otherwise every API invocation incurs\n+   * the overhead of opening/closing a TCP connection.\n+   */\n+  private FileStatus[] listStatus(FTPClient client, Path file)\n+      throws IOException {\n+    Path workDir = new Path(client.printWorkingDirectory());\n+    Path absolute = makeAbsolute(workDir, file);\n+    FileStatus fileStat = getFileStatus(client, absolute);\n+    if (!fileStat.isDir()) {\n+      return new FileStatus[] { fileStat };\n+    }\n+    FTPFile[] ftpFiles = client.listFiles(absolute.toUri().getPath());\n+    FileStatus[] fileStats = new FileStatus[ftpFiles.length];\n+    for (int i = 0; i < ftpFiles.length; i++) {\n+      fileStats[i] = getFileStatus(ftpFiles[i], absolute);\n+    }\n+    return fileStats;\n+  }\n+\n+  @Override\n+  public FileStatus getFileStatus(Path file) throws IOException {\n+    FTPClient client = connect();\n+    try {\n+      FileStatus status = getFileStatus(client, file);\n+      return status;\n+    } finally {\n+      disconnect(client);\n+    }\n+  }\n+\n+  /**\n+   * Convenience method, so that we don't open a new connection when using this\n+   * method from within another method. Otherwise every API invocation incurs\n+   * the overhead of opening/closing a TCP connection.\n+   */\n+  private FileStatus getFileStatus(FTPClient client, Path file)\n+      throws IOException {\n+    FileStatus fileStat = null;\n+    Path workDir = new Path(client.printWorkingDirectory());\n+    Path absolute = makeAbsolute(workDir, file);\n+    Path parentPath = absolute.getParent();\n+    if (parentPath == null) { // root dir\n+      long length = -1; // Length of root dir on server not known\n+      boolean isDir = true;\n+      int blockReplication = 1;\n+      long blockSize = DEFAULT_BLOCK_SIZE; // Block Size not known.\n+      long modTime = -1; // Modification time of root dir not known.\n+      Path root = new Path(\"/\");\n+      return new FileStatus(length, isDir, blockReplication, blockSize,\n+          modTime, root.makeQualified(this));\n+    }\n+    String pathName = parentPath.toUri().getPath();\n+    FTPFile[] ftpFiles = client.listFiles(pathName);\n+    if (ftpFiles != null) {\n+      for (FTPFile ftpFile : ftpFiles) {\n+        if (ftpFile.getName().equals(file.getName())) { // file found in dir\n+          fileStat = getFileStatus(ftpFile, parentPath);\n+          break;\n+        }\n+      }\n+      if (fileStat == null) {\n+        throw new FileNotFoundException(\"File \" + file + \" does not exist.\");\n+      }\n+    } else {\n+      throw new FileNotFoundException(\"File \" + file + \" does not exist.\");\n+    }\n+    return fileStat;\n+  }\n+\n+  /**\n+   * Convert the file information in FTPFile to a {@link FileStatus} object. *\n+   * \n+   * @param ftpFile\n+   * @param parentPath\n+   * @return FileStatus\n+   */\n+  private FileStatus getFileStatus(FTPFile ftpFile, Path parentPath) {\n+    long length = ftpFile.getSize();\n+    boolean isDir = ftpFile.isDirectory();\n+    int blockReplication = 1;\n+    // Using default block size since there is no way in FTP client to know of\n+    // block sizes on server. The assumption could be less than ideal.\n+    long blockSize = DEFAULT_BLOCK_SIZE;\n+    long modTime = ftpFile.getTimestamp().getTimeInMillis();\n+    long accessTime = 0;\n+    FsPermission permission = getPermissions(ftpFile);\n+    String user = ftpFile.getUser();\n+    String group = ftpFile.getGroup();\n+    Path filePath = new Path(parentPath, ftpFile.getName());\n+    return new FileStatus(length, isDir, blockReplication, blockSize, modTime,\n+        accessTime, permission, user, group, filePath.makeQualified(this));\n+  }\n+\n+  @Override\n+  public boolean mkdirs(Path file, FsPermission permission) throws IOException {\n+    FTPClient client = connect();\n+    try {\n+      boolean success = mkdirs(client, file, permission);\n+      return success;\n+    } finally {\n+      disconnect(client);\n+    }\n+  }\n+\n+  /**\n+   * Convenience method, so that we don't open a new connection when using this\n+   * method from within another method. Otherwise every API invocation incurs\n+   * the overhead of opening/closing a TCP connection.\n+   */\n+  private boolean mkdirs(FTPClient client, Path file, FsPermission permission)\n+      throws IOException {\n+    boolean created = true;\n+    Path workDir = new Path(client.printWorkingDirectory());\n+    Path absolute = makeAbsolute(workDir, file);\n+    String pathName = absolute.getName();\n+    if (!exists(client, absolute)) {\n+      Path parent = absolute.getParent();\n+      created = (parent == null || mkdirs(client, parent, FsPermission\n+          .getDefault()));\n+      if (created) {\n+        String parentDir = parent.toUri().getPath();\n+        client.changeWorkingDirectory(parentDir);\n+        created = created & client.makeDirectory(pathName);\n+      }\n+    } else if (isFile(client, absolute)) {\n+      throw new IOException(String.format(\n+          \"Can't make directory for path %s since it is a file.\", absolute));\n+    }\n+    return created;\n+  }\n+\n+  /**\n+   * Convenience method, so that we don't open a new connection when using this\n+   * method from within another method. Otherwise every API invocation incurs\n+   * the overhead of opening/closing a TCP connection.\n+   */\n+  private boolean isFile(FTPClient client, Path file) {\n+    try {\n+      return !getFileStatus(client, file).isDir();\n+    } catch (FileNotFoundException e) {\n+      return false; // file does not exist\n+    } catch (IOException ioe) {\n+      throw new FTPException(\"File check failed\", ioe);\n+    }\n+  }\n+\n+  /*\n+   * Assuming that parent of both source and destination is the same. Is the\n+   * assumption correct or it is suppose to work like 'move' ?\n+   */\n+  @Override\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    FTPClient client = connect();\n+    try {\n+      boolean success = rename(client, src, dst);\n+      return success;\n+    } finally {\n+      disconnect(client);\n+    }\n+  }\n+\n+  /**\n+   * Convenience method, so that we don't open a new connection when using this\n+   * method from within another method. Otherwise every API invocation incurs\n+   * the overhead of opening/closing a TCP connection.\n+   * \n+   * @param client\n+   * @param src\n+   * @param dst\n+   * @return\n+   * @throws IOException\n+   */\n+  private boolean rename(FTPClient client, Path src, Path dst)\n+      throws IOException {\n+    Path workDir = new Path(client.printWorkingDirectory());\n+    Path absoluteSrc = makeAbsolute(workDir, src);\n+    Path absoluteDst = makeAbsolute(workDir, dst);\n+    if (!exists(client, absoluteSrc)) {\n+      throw new IOException(\"Source path \" + src + \" does not exist\");\n+    }\n+    if (exists(client, absoluteDst)) {\n+      throw new IOException(\"Destination path \" + dst\n+          + \" already exist, cannot rename!\");\n+    }\n+    String parentSrc = absoluteSrc.getParent().toUri().toString();\n+    String parentDst = absoluteDst.getParent().toUri().toString();\n+    String from = src.getName();\n+    String to = dst.getName();\n+    if (!parentSrc.equals(parentDst)) {\n+      throw new IOException(\"Cannot rename parent(source): \" + parentSrc\n+          + \", parent(destination):  \" + parentDst);\n+    }\n+    client.changeWorkingDirectory(parentSrc);\n+    boolean renamed = client.rename(from, to);\n+    return renamed;\n+  }\n+\n+  @Override\n+  public Path getWorkingDirectory() {\n+    // Return home directory always since we do not maintain state.\n+    return getHomeDirectory();\n+  }\n+\n+  @Override\n+  public Path getHomeDirectory() {\n+    FTPClient client = null;\n+    try {\n+      client = connect();\n+      Path homeDir = new Path(client.printWorkingDirectory());\n+      return homeDir;\n+    } catch (IOException ioe) {\n+      throw new FTPException(\"Failed to get home directory\", ioe);\n+    } finally {\n+      try {\n+        disconnect(client);\n+      } catch (IOException ioe) {\n+        throw new FTPException(\"Failed to disconnect\", ioe);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void setWorkingDirectory(Path newDir) {\n+    // we do not maintain the working directory state\n+  }\n+}"
        },
        {
            "sha": "f1b78955ae207af330270ccb5f70ad4c0bd683ce",
            "filename": "src/java/org/apache/hadoop/fs/ftp/FTPInputStream.java",
            "status": "added",
            "additions": 126,
            "deletions": 0,
            "changes": 126,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fftp%2FFTPInputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fftp%2FFTPInputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fftp%2FFTPInputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,126 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.ftp;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+import org.apache.commons.net.ftp.FTPClient;\n+import org.apache.hadoop.fs.FSInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+\n+public class FTPInputStream extends FSInputStream {\n+\n+  InputStream wrappedStream;\n+  FTPClient client;\n+  FileSystem.Statistics stats;\n+  boolean closed;\n+  long pos;\n+\n+  public FTPInputStream(InputStream stream, FTPClient client,\n+      FileSystem.Statistics stats) {\n+    if (stream == null) {\n+      throw new IllegalArgumentException(\"Null InputStream\");\n+    }\n+    if (client == null || !client.isConnected()) {\n+      throw new IllegalArgumentException(\"FTP client null or not connected\");\n+    }\n+    this.wrappedStream = stream;\n+    this.client = client;\n+    this.stats = stats;\n+    this.pos = 0;\n+    this.closed = false;\n+  }\n+\n+  public long getPos() throws IOException {\n+    return pos;\n+  }\n+\n+  // We don't support seek.\n+  public void seek(long pos) throws IOException {\n+    throw new IOException(\"Seek not supported\");\n+  }\n+\n+  public boolean seekToNewSource(long targetPos) throws IOException {\n+    throw new IOException(\"Seek not supported\");\n+  }\n+\n+  public synchronized int read() throws IOException {\n+    if (closed) {\n+      throw new IOException(\"Stream closed\");\n+    }\n+\n+    int byteRead = wrappedStream.read();\n+    if (byteRead >= 0) {\n+      pos++;\n+    }\n+    if (stats != null & byteRead >= 0) {\n+      stats.incrementBytesRead(1);\n+    }\n+    return byteRead;\n+  }\n+\n+  public synchronized int read(byte buf[], int off, int len) throws IOException {\n+    if (closed) {\n+      throw new IOException(\"Stream closed\");\n+    }\n+\n+    int result = wrappedStream.read(buf, off, len);\n+    if (result > 0) {\n+      pos += result;\n+    }\n+    if (stats != null & result > 0) {\n+      stats.incrementBytesRead(result);\n+    }\n+\n+    return result;\n+  }\n+\n+  public synchronized void close() throws IOException {\n+    if (closed) {\n+      throw new IOException(\"Stream closed\");\n+    }\n+    super.close();\n+    closed = true;\n+    if (!client.isConnected()) {\n+      throw new FTPException(\"Client not connected\");\n+    }\n+\n+    boolean cmdCompleted = client.completePendingCommand();\n+    client.logout();\n+    client.disconnect();\n+    if (!cmdCompleted) {\n+      throw new FTPException(\"Could not complete transfer, Reply Code - \"\n+          + client.getReplyCode());\n+    }\n+  }\n+\n+  // Not supported.\n+\n+  public boolean markSupported() {\n+    return false;\n+  }\n+\n+  public void mark(int readLimit) {\n+    // Do nothing\n+  }\n+\n+  public void reset() throws IOException {\n+    throw new IOException(\"Mark not supported\");\n+  }\n+}"
        },
        {
            "sha": "f2a773663ea04400d2df31836de551eac2d7e112",
            "filename": "src/java/org/apache/hadoop/fs/kfs/IFSImpl.java",
            "status": "added",
            "additions": 60,
            "deletions": 0,
            "changes": 60,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FIFSImpl.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FIFSImpl.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FIFSImpl.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,60 @@\n+/**\n+ *\n+ * Licensed under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+ * implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ *\n+ * @author: Sriram Rao (Kosmix Corp.)\n+ * \n+ * We need to provide the ability to the code in fs/kfs without really\n+ * having a KFS deployment.  In particular, the glue code that wraps\n+ * around calls to KfsAccess object.  This is accomplished by defining a\n+ * filesystem implementation interface:  \n+ *   -- for testing purposes, a dummy implementation of this interface\n+ * will suffice; as long as the dummy implementation is close enough\n+ * to doing what KFS does, we are good.\n+ *   -- for deployment purposes with KFS, this interface is\n+ * implemented by the KfsImpl object.\n+ */\n+\n+package org.apache.hadoop.fs.kfs;\n+\n+import java.io.*;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.util.Progressable;\n+\n+interface IFSImpl {\n+    public boolean exists(String path) throws IOException;\n+    public boolean isDirectory(String path) throws IOException;\n+    public boolean isFile(String path) throws IOException;\n+    public String[] readdir(String path) throws IOException;\n+    public FileStatus[] readdirplus(Path path) throws IOException;\n+\n+    public int mkdirs(String path) throws IOException;\n+    public int rename(String source, String dest) throws IOException;\n+\n+    public int rmdir(String path) throws IOException; \n+    public int remove(String path) throws IOException;\n+    public long filesize(String path) throws IOException;\n+    public short getReplication(String path) throws IOException;\n+    public short setReplication(String path, short replication) throws IOException;\n+    public String[][] getDataLocation(String path, long start, long len) throws IOException;\n+\n+    public long getModificationTime(String path) throws IOException;\n+    public FSDataOutputStream create(String path, short replication, int bufferSize, Progressable progress) throws IOException;\n+    public FSDataInputStream open(String path, int bufferSize) throws IOException;\n+    public FSDataOutputStream append(String path, int bufferSize, Progressable progress) throws IOException;\n+    \n+};"
        },
        {
            "sha": "bc66ec2570a3b70d784102687a9fe4b15c97c900",
            "filename": "src/java/org/apache/hadoop/fs/kfs/KFSImpl.java",
            "status": "added",
            "additions": 151,
            "deletions": 0,
            "changes": 151,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FKFSImpl.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FKFSImpl.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FKFSImpl.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,151 @@\n+/**\n+ *\n+ * Licensed under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+ * implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ *\n+ * @author: Sriram Rao (Kosmix Corp.)\n+ * \n+ * Provide the implementation of KFS which turn into calls to KfsAccess.\n+ */\n+\n+package org.apache.hadoop.fs.kfs;\n+\n+import java.io.*;\n+\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.Path;\n+\n+import org.kosmix.kosmosfs.access.KfsAccess;\n+import org.kosmix.kosmosfs.access.KfsFileAttr;\n+import org.apache.hadoop.util.Progressable;\n+\n+class KFSImpl implements IFSImpl {\n+    private KfsAccess kfsAccess = null;\n+    private FileSystem.Statistics statistics;\n+\n+    @Deprecated\n+    public KFSImpl(String metaServerHost, int metaServerPort\n+                   ) throws IOException {\n+      this(metaServerHost, metaServerPort, null);\n+    }\n+\n+    public KFSImpl(String metaServerHost, int metaServerPort, \n+                   FileSystem.Statistics stats) throws IOException {\n+        kfsAccess = new KfsAccess(metaServerHost, metaServerPort);\n+        statistics = stats;\n+    }\n+\n+    public boolean exists(String path) throws IOException {\n+        return kfsAccess.kfs_exists(path);\n+    }\n+\n+    public boolean isDirectory(String path) throws IOException {\n+        return kfsAccess.kfs_isDirectory(path);\n+    }\n+\n+    public boolean isFile(String path) throws IOException {\n+        return kfsAccess.kfs_isFile(path);\n+    }\n+\n+    public String[] readdir(String path) throws IOException {\n+        return kfsAccess.kfs_readdir(path);\n+    }\n+\n+    public FileStatus[] readdirplus(Path path) throws IOException {\n+        String srep = path.toUri().getPath();\n+        KfsFileAttr[] fattr = kfsAccess.kfs_readdirplus(srep);\n+        if (fattr == null)\n+            return null;\n+        int numEntries = 0;\n+        for (int i = 0; i < fattr.length; i++) {\n+            if ((fattr[i].filename.compareTo(\".\") == 0) || (fattr[i].filename.compareTo(\"..\") == 0))\n+                continue;\n+            numEntries++;\n+        }\n+        FileStatus[] fstatus = new FileStatus[numEntries];\n+        int j = 0;\n+        for (int i = 0; i < fattr.length; i++) {\n+            if ((fattr[i].filename.compareTo(\".\") == 0) || (fattr[i].filename.compareTo(\"..\") == 0))\n+                continue;\n+            Path fn = new Path(path, fattr[i].filename);\n+\n+            if (fattr[i].isDirectory)\n+                fstatus[j] = new FileStatus(0, true, 1, 0, fattr[i].modificationTime, fn);\n+            else\n+                fstatus[j] = new FileStatus(fattr[i].filesize, fattr[i].isDirectory,\n+                                            fattr[i].replication,\n+                                            (long)\n+                                            (1 << 26),\n+                                            fattr[i].modificationTime,\n+                                            fn);\n+\n+            j++;\n+        }\n+        return fstatus;\n+    }\n+\n+\n+    public int mkdirs(String path) throws IOException {\n+        return kfsAccess.kfs_mkdirs(path);\n+    }\n+\n+    public int rename(String source, String dest) throws IOException {\n+        return kfsAccess.kfs_rename(source, dest);\n+    }\n+\n+    public int rmdir(String path) throws IOException {\n+        return kfsAccess.kfs_rmdir(path);\n+    }\n+\n+    public int remove(String path) throws IOException {\n+        return kfsAccess.kfs_remove(path);\n+    }\n+\n+    public long filesize(String path) throws IOException {\n+        return kfsAccess.kfs_filesize(path);\n+    }\n+\n+    public short getReplication(String path) throws IOException {\n+        return kfsAccess.kfs_getReplication(path);\n+    }\n+\n+    public short setReplication(String path, short replication) throws IOException {\n+        return kfsAccess.kfs_setReplication(path, replication);\n+    }\n+\n+    public String[][] getDataLocation(String path, long start, long len) throws IOException {\n+        return kfsAccess.kfs_getDataLocation(path, start, len);\n+    }\n+\n+    public long getModificationTime(String path) throws IOException {\n+        return kfsAccess.kfs_getModificationTime(path);\n+    }\n+\n+    public FSDataInputStream open(String path, int bufferSize) throws IOException {\n+        return new FSDataInputStream(new KFSInputStream(kfsAccess, path, \n+                                                        statistics));\n+    }\n+\n+    public FSDataOutputStream create(String path, short replication, int bufferSize, Progressable progress) throws IOException {\n+        return new FSDataOutputStream(new KFSOutputStream(kfsAccess, path, replication, false, progress), \n+                                      statistics);\n+    }\n+\n+    public FSDataOutputStream append(String path, int bufferSize, Progressable progress) throws IOException {\n+        // when opening for append, # of replicas is ignored\n+        return new FSDataOutputStream(new KFSOutputStream(kfsAccess, path, (short) 1, true, progress), \n+                                      statistics);\n+    }\n+}"
        },
        {
            "sha": "bb2c32c31bdce856a59a05d62baedc7c63652bb3",
            "filename": "src/java/org/apache/hadoop/fs/kfs/KFSInputStream.java",
            "status": "added",
            "additions": 130,
            "deletions": 0,
            "changes": 130,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FKFSInputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FKFSInputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FKFSInputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,130 @@\n+/**\n+ *\n+ * Licensed under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+ * implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ *\n+ * @author: Sriram Rao (Kosmix Corp.)\n+ * \n+ * Implements the Hadoop FSInputStream interfaces to allow applications to read\n+ * files in Kosmos File System (KFS).\n+ */\n+\n+package org.apache.hadoop.fs.kfs;\n+\n+import java.io.*;\n+import java.nio.ByteBuffer;\n+\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FSInputStream;\n+\n+import org.kosmix.kosmosfs.access.KfsAccess;\n+import org.kosmix.kosmosfs.access.KfsInputChannel;\n+\n+class KFSInputStream extends FSInputStream {\n+\n+    private KfsInputChannel kfsChannel;\n+    private FileSystem.Statistics statistics;\n+    private long fsize;\n+\n+    @Deprecated\n+    public KFSInputStream(KfsAccess kfsAccess, String path) {\n+      this(kfsAccess, path, null);\n+    }\n+\n+    public KFSInputStream(KfsAccess kfsAccess, String path,\n+                            FileSystem.Statistics stats) {\n+        this.statistics = stats;\n+        this.kfsChannel = kfsAccess.kfs_open(path);\n+        if (this.kfsChannel != null)\n+            this.fsize = kfsAccess.kfs_filesize(path);\n+        else\n+            this.fsize = 0;\n+    }\n+\n+    public long getPos() throws IOException {\n+        if (kfsChannel == null) {\n+            throw new IOException(\"File closed\");\n+        }\n+        return kfsChannel.tell();\n+    }\n+\n+    public synchronized int available() throws IOException {\n+        if (kfsChannel == null) {\n+            throw new IOException(\"File closed\");\n+        }\n+        return (int) (this.fsize - getPos());\n+    }\n+\n+    public synchronized void seek(long targetPos) throws IOException {\n+        if (kfsChannel == null) {\n+            throw new IOException(\"File closed\");\n+        }\n+        kfsChannel.seek(targetPos);\n+    }\n+\n+    public synchronized boolean seekToNewSource(long targetPos) throws IOException {\n+        return false;\n+    }\n+\n+    public synchronized int read() throws IOException {\n+        if (kfsChannel == null) {\n+            throw new IOException(\"File closed\");\n+        }\n+        byte b[] = new byte[1];\n+        int res = read(b, 0, 1);\n+        if (res == 1) {\n+          if (statistics != null) {\n+            statistics.incrementBytesRead(1);\n+          }\n+          return b[0] & 0xff;\n+        }\n+        return -1;\n+    }\n+\n+    public synchronized int read(byte b[], int off, int len) throws IOException {\n+        if (kfsChannel == null) {\n+            throw new IOException(\"File closed\");\n+        }\n+\tint res;\n+\n+\tres = kfsChannel.read(ByteBuffer.wrap(b, off, len));\n+\t// Use -1 to signify EOF\n+\tif (res == 0)\n+\t    return -1;\n+\tif (statistics != null) {\n+\t  statistics.incrementBytesRead(res);\n+\t}\n+\treturn res;\n+    }\n+\n+    public synchronized void close() throws IOException {\n+        if (kfsChannel == null) {\n+            return;\n+        }\n+\n+        kfsChannel.close();\n+        kfsChannel = null;\n+    }\n+\n+    public boolean markSupported() {\n+        return false;\n+    }\n+\n+    public void mark(int readLimit) {\n+        // Do nothing\n+    }\n+\n+    public void reset() throws IOException {\n+        throw new IOException(\"Mark not supported\");\n+    }\n+\n+}"
        },
        {
            "sha": "e55f4205d8fcad17f98016b3bc0e9abfb0c7f1e3",
            "filename": "src/java/org/apache/hadoop/fs/kfs/KFSOutputStream.java",
            "status": "added",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FKFSOutputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FKFSOutputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FKFSOutputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,97 @@\n+/**\n+ *\n+ * Licensed under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+ * implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ *\n+ * @author: Sriram Rao (Kosmix Corp.)\n+ * \n+ * Implements the Hadoop FSOutputStream interfaces to allow applications to write to\n+ * files in Kosmos File System (KFS).\n+ */\n+\n+package org.apache.hadoop.fs.kfs;\n+\n+import java.io.*;\n+import java.net.*;\n+import java.util.*;\n+import java.nio.ByteBuffer;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.util.Progressable;\n+\n+import org.kosmix.kosmosfs.access.KfsAccess;\n+import org.kosmix.kosmosfs.access.KfsOutputChannel;\n+\n+class KFSOutputStream extends OutputStream {\n+\n+    private String path;\n+    private KfsOutputChannel kfsChannel;\n+    private Progressable progressReporter;\n+\n+    public KFSOutputStream(KfsAccess kfsAccess, String path, short replication,\n+                           boolean append, Progressable prog) {\n+        this.path = path;\n+\n+        if ((append) && (kfsAccess.kfs_isFile(path)))\n+                this.kfsChannel = kfsAccess.kfs_append(path);\n+        else\n+                this.kfsChannel = kfsAccess.kfs_create(path, replication);\n+        this.progressReporter = prog;\n+    }\n+\n+    public long getPos() throws IOException {\n+        if (kfsChannel == null) {\n+            throw new IOException(\"File closed\");\n+        }\n+        return kfsChannel.tell();\n+    }\n+\n+    public void write(int v) throws IOException {\n+        if (kfsChannel == null) {\n+            throw new IOException(\"File closed\");\n+        }\n+        byte[] b = new byte[1];\n+\n+        b[0] = (byte) v;\n+        write(b, 0, 1);\n+    }\n+\n+    public void write(byte b[], int off, int len) throws IOException {\n+        if (kfsChannel == null) {\n+            throw new IOException(\"File closed\");\n+        }\n+\n+        // touch the progress before going into KFS since the call can block\n+        progressReporter.progress();\n+        kfsChannel.write(ByteBuffer.wrap(b, off, len));\n+    }\n+\n+    public void flush() throws IOException {\n+        if (kfsChannel == null) {\n+            throw new IOException(\"File closed\");\n+        }\n+        // touch the progress before going into KFS since the call can block\n+        progressReporter.progress();\n+        kfsChannel.sync();\n+    }\n+\n+    public synchronized void close() throws IOException {\n+        if (kfsChannel == null) {\n+            return;\n+        }\n+        flush();\n+        kfsChannel.close();\n+        kfsChannel = null;\n+    }\n+}"
        },
        {
            "sha": "57b27a2a0e98f0f53bf5eea377aa3e24c8197c35",
            "filename": "src/java/org/apache/hadoop/fs/kfs/KosmosFileSystem.java",
            "status": "added",
            "additions": 340,
            "deletions": 0,
            "changes": 340,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FKosmosFileSystem.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FKosmosFileSystem.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2FKosmosFileSystem.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,340 @@\n+/**\n+ *\n+ * Licensed under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n+ * implied. See the License for the specific language governing\n+ * permissions and limitations under the License.\n+ *\n+ * @author: Sriram Rao (Kosmix Corp.)\n+ * \n+ * Implements the Hadoop FS interfaces to allow applications to store\n+ *files in Kosmos File System (KFS).\n+ */\n+\n+package org.apache.hadoop.fs.kfs;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.FileUtil;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.util.Progressable;\n+\n+/**\n+ * A FileSystem backed by KFS.\n+ *\n+ */\n+\n+public class KosmosFileSystem extends FileSystem {\n+\n+    private FileSystem localFs;\n+    private IFSImpl kfsImpl = null;\n+    private URI uri;\n+    private Path workingDir = new Path(\"/\");\n+\n+    public KosmosFileSystem() {\n+\n+    }\n+\n+    KosmosFileSystem(IFSImpl fsimpl) {\n+        this.kfsImpl = fsimpl;\n+    }\n+\n+    @Override\n+    public URI getUri() {\n+\treturn uri;\n+    }\n+\n+    @Override\n+    public void initialize(URI uri, Configuration conf) throws IOException {\n+      super.initialize(uri, conf);\n+      try {\n+        if (kfsImpl == null) {\n+          if (uri.getHost() == null) {\n+            kfsImpl = new KFSImpl(conf.get(\"fs.kfs.metaServerHost\", \"\"),\n+                                  conf.getInt(\"fs.kfs.metaServerPort\", -1),\n+                                  statistics);\n+          } else {\n+            kfsImpl = new KFSImpl(uri.getHost(), uri.getPort(), statistics);\n+          }\n+        }\n+\n+        this.localFs = FileSystem.getLocal(conf);\n+        this.uri = URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n+        this.workingDir = new Path(\"/user\", System.getProperty(\"user.name\")\n+                                   ).makeQualified(this);\n+        setConf(conf);\n+\n+      } catch (Exception e) {\n+        e.printStackTrace();\n+        System.out.println(\"Unable to initialize KFS\");\n+        System.exit(-1);\n+      }\n+    }\n+\n+    @Override\n+    public Path getWorkingDirectory() {\n+\treturn workingDir;\n+    }\n+\n+    @Override\n+    public void setWorkingDirectory(Path dir) {\n+\tworkingDir = makeAbsolute(dir);\n+    }\n+\n+    private Path makeAbsolute(Path path) {\n+\tif (path.isAbsolute()) {\n+\t    return path;\n+\t}\n+\treturn new Path(workingDir, path);\n+    }\n+\n+    @Override\n+    public boolean mkdirs(Path path, FsPermission permission\n+        ) throws IOException {\n+\tPath absolute = makeAbsolute(path);\n+        String srep = absolute.toUri().getPath();\n+\n+\tint res;\n+\n+\t// System.out.println(\"Calling mkdirs on: \" + srep);\n+\n+\tres = kfsImpl.mkdirs(srep);\n+\t\n+\treturn res == 0;\n+    }\n+\n+    @Override\n+    @Deprecated\n+    public boolean isDirectory(Path path) throws IOException {\n+\tPath absolute = makeAbsolute(path);\n+        String srep = absolute.toUri().getPath();\n+\n+\t// System.out.println(\"Calling isdir on: \" + srep);\n+\n+        return kfsImpl.isDirectory(srep);\n+    }\n+\n+    @Override\n+    @Deprecated\n+    public boolean isFile(Path path) throws IOException {\n+\tPath absolute = makeAbsolute(path);\n+        String srep = absolute.toUri().getPath();\n+        return kfsImpl.isFile(srep);\n+    }\n+\n+    @Override\n+    public FileStatus[] listStatus(Path path) throws IOException {\n+        Path absolute = makeAbsolute(path);\n+        String srep = absolute.toUri().getPath();\n+\n+        if (kfsImpl.isFile(srep))\n+                return new FileStatus[] { getFileStatus(path) } ;\n+\n+        return kfsImpl.readdirplus(absolute);\n+    }\n+\n+    @Override\n+    public FileStatus getFileStatus(Path path) throws IOException {\n+\tPath absolute = makeAbsolute(path);\n+        String srep = absolute.toUri().getPath();\n+        if (!kfsImpl.exists(srep)) {\n+          throw new FileNotFoundException(\"File \" + path + \" does not exist.\");\n+        }\n+        if (kfsImpl.isDirectory(srep)) {\n+            // System.out.println(\"Status of path: \" + path + \" is dir\");\n+            return new FileStatus(0, true, 1, 0, kfsImpl.getModificationTime(srep), \n+                                  path.makeQualified(this));\n+        } else {\n+            // System.out.println(\"Status of path: \" + path + \" is file\");\n+            return new FileStatus(kfsImpl.filesize(srep), false, \n+                                  kfsImpl.getReplication(srep),\n+                                  getDefaultBlockSize(),\n+                                  kfsImpl.getModificationTime(srep),\n+                                  path.makeQualified(this));\n+        }\n+    }\n+    \n+    @Override\n+    public FSDataOutputStream append(Path f, int bufferSize,\n+        Progressable progress) throws IOException {\n+        Path parent = f.getParent();\n+        if (parent != null && !mkdirs(parent)) {\n+            throw new IOException(\"Mkdirs failed to create \" + parent);\n+        }\n+\n+        Path absolute = makeAbsolute(f);\n+        String srep = absolute.toUri().getPath();\n+\n+        return kfsImpl.append(srep, bufferSize, progress);\n+    }\n+\n+    @Override\n+    public FSDataOutputStream create(Path file, FsPermission permission,\n+                                     boolean overwrite, int bufferSize,\n+\t\t\t\t     short replication, long blockSize, Progressable progress)\n+\tthrows IOException {\n+\n+        if (exists(file)) {\n+            if (overwrite) {\n+                delete(file, true);\n+            } else {\n+                throw new IOException(\"File already exists: \" + file);\n+            }\n+        }\n+\n+\tPath parent = file.getParent();\n+\tif (parent != null && !mkdirs(parent)) {\n+\t    throw new IOException(\"Mkdirs failed to create \" + parent);\n+\t}\n+\n+        Path absolute = makeAbsolute(file);\n+        String srep = absolute.toUri().getPath();\n+\n+        return kfsImpl.create(srep, replication, bufferSize, progress);\n+    }\n+\n+    @Override\n+    public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+        if (!exists(path))\n+            throw new IOException(\"File does not exist: \" + path);\n+\n+        Path absolute = makeAbsolute(path);\n+        String srep = absolute.toUri().getPath();\n+\n+        return kfsImpl.open(srep, bufferSize);\n+    }\n+\n+    @Override\n+    public boolean rename(Path src, Path dst) throws IOException {\n+\tPath absoluteS = makeAbsolute(src);\n+        String srepS = absoluteS.toUri().getPath();\n+\tPath absoluteD = makeAbsolute(dst);\n+        String srepD = absoluteD.toUri().getPath();\n+\n+        // System.out.println(\"Calling rename on: \" + srepS + \" -> \" + srepD);\n+\n+        return kfsImpl.rename(srepS, srepD) == 0;\n+    }\n+\n+    // recursively delete the directory and its contents\n+    @Override\n+    public boolean delete(Path path, boolean recursive) throws IOException {\n+      Path absolute = makeAbsolute(path);\n+      String srep = absolute.toUri().getPath();\n+      if (kfsImpl.isFile(srep))\n+        return kfsImpl.remove(srep) == 0;\n+\n+      FileStatus[] dirEntries = listStatus(absolute);\n+      if ((!recursive) && (dirEntries != null) && \n+            (dirEntries.length != 0)) {\n+        throw new IOException(\"Directory \" + path.toString() + \n+        \" is not empty.\");\n+      }\n+      if (dirEntries != null) {\n+        for (int i = 0; i < dirEntries.length; i++) {\n+          delete(new Path(absolute, dirEntries[i].getPath()), recursive);\n+        }\n+      }\n+      return kfsImpl.rmdir(srep) == 0;\n+    }\n+    \n+    @Override\n+    public short getDefaultReplication() {\n+\treturn 3;\n+    }\n+\n+    @Override\n+    public boolean setReplication(Path path, short replication)\n+\tthrows IOException {\n+\n+\tPath absolute = makeAbsolute(path);\n+        String srep = absolute.toUri().getPath();\n+\n+        int res = kfsImpl.setReplication(srep, replication);\n+        return res >= 0;\n+    }\n+\n+    // 64MB is the KFS block size\n+\n+    @Override\n+    public long getDefaultBlockSize() {\n+\treturn 1 << 26;\n+    }\n+\n+    @Deprecated            \n+    public void lock(Path path, boolean shared) throws IOException {\n+\n+    }\n+\n+    @Deprecated            \n+    public void release(Path path) throws IOException {\n+\n+    }\n+\n+    /**\n+     * Return null if the file doesn't exist; otherwise, get the\n+     * locations of the various chunks of the file file from KFS.\n+     */\n+    @Override\n+    public BlockLocation[] getFileBlockLocations(FileStatus file, long start,\n+        long len) throws IOException {\n+\n+      if (file == null) {\n+        return null;\n+      }\n+      String srep = makeAbsolute(file.getPath()).toUri().getPath();\n+      String[][] hints = kfsImpl.getDataLocation(srep, start, len);\n+      if (hints == null) {\n+        return null;\n+      }\n+      BlockLocation[] result = new BlockLocation[hints.length];\n+      long blockSize = getDefaultBlockSize();\n+      long length = len;\n+      long blockStart = start;\n+      for(int i=0; i < result.length; ++i) {\n+        result[i] = new BlockLocation(null, hints[i], blockStart, \n+                                      length < blockSize ? length : blockSize);\n+        blockStart += blockSize;\n+        length -= blockSize;\n+      }\n+      return result;\n+    }\n+\n+    @Override\n+    public void copyFromLocalFile(boolean delSrc, Path src, Path dst) throws IOException {\n+\tFileUtil.copy(localFs, src, this, dst, delSrc, getConf());\n+    }\n+\n+    @Override\n+    public void copyToLocalFile(boolean delSrc, Path src, Path dst) throws IOException {\n+\tFileUtil.copy(this, src, localFs, dst, delSrc, getConf());\n+    }\n+\n+    @Override\n+    public Path startLocalOutput(Path fsOutputFile, Path tmpLocalFile)\n+\tthrows IOException {\n+\treturn tmpLocalFile;\n+    }\n+\n+    @Override\n+    public void completeLocalOutput(Path fsOutputFile, Path tmpLocalFile)\n+\tthrows IOException {\n+\tmoveFromLocalFile(tmpLocalFile, fsOutputFile);\n+    }\n+}"
        },
        {
            "sha": "365b60b4fa56e66d709a51ce4d90bca141ab0470",
            "filename": "src/java/org/apache/hadoop/fs/kfs/package.html",
            "status": "added",
            "additions": 98,
            "deletions": 0,
            "changes": 98,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fkfs%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,98 @@\n+<html>\n+\n+<!--\n+   Licensed to the Apache Software Foundation (ASF) under one or more\n+   contributor license agreements.  See the NOTICE file distributed with\n+   this work for additional information regarding copyright ownership.\n+   The ASF licenses this file to You under the Apache License, Version 2.0\n+   (the \"License\"); you may not use this file except in compliance with\n+   the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+-->\n+\n+<head></head>\n+<body>\n+<h1>A client for the Kosmos filesystem (KFS)</h1>\n+\n+<h3>Introduction</h3>\n+\n+This pages describes how to use Kosmos Filesystem \n+(<a href=\"http://kosmosfs.sourceforge.net\"> KFS </a>) as a backing\n+store with Hadoop.   This page assumes that you have downloaded the\n+KFS software and installed necessary binaries as outlined in the KFS\n+documentation.\n+\n+<h3>Steps</h3>\n+\n+        <ul>\n+          <li>In the Hadoop conf directory edit core-site.xml,\n+          add the following:\n+            <pre>\n+&lt;property&gt;\n+  &lt;name&gt;fs.kfs.impl&lt;/name&gt;\n+  &lt;value&gt;org.apache.hadoop.fs.kfs.KosmosFileSystem&lt;/value&gt;\n+  &lt;description&gt;The FileSystem for kfs: uris.&lt;/description&gt;\n+&lt;/property&gt;\n+            </pre>\n+\n+          <li>In the Hadoop conf directory edit core-site.xml,\n+          adding the following (with appropriate values for\n+          &lt;server&gt; and &lt;port&gt;):\n+            <pre>\n+&lt;property&gt;\n+  &lt;name&gt;fs.default.name&lt;/name&gt;\n+  &lt;value&gt;kfs://&lt;server:port&gt;&lt;/value&gt; \n+&lt;/property&gt;\n+\n+&lt;property&gt;\n+  &lt;name&gt;fs.kfs.metaServerHost&lt;/name&gt;\n+  &lt;value&gt;&lt;server&gt;&lt;/value&gt;\n+  &lt;description&gt;The location of the KFS meta server.&lt;/description&gt;\n+&lt;/property&gt;\n+\n+&lt;property&gt;\n+  &lt;name&gt;fs.kfs.metaServerPort&lt;/name&gt;\n+  &lt;value&gt;&lt;port&gt;&lt;/value&gt;\n+  &lt;description&gt;The location of the meta server's port.&lt;/description&gt;\n+&lt;/property&gt;\n+\n+</pre>\n+          </li>\n+\n+          <li>Copy KFS's <i> kfs-0.1.jar </i> to Hadoop's lib directory.  This step\n+          enables Hadoop's to load the KFS specific modules.  Note\n+          that, kfs-0.1.jar was built when you compiled KFS source\n+          code.  This jar file contains code that calls KFS's client\n+          library code via JNI; the native code is in KFS's <i>\n+          libkfsClient.so </i> library.\n+          </li>\n+\n+          <li> When the Hadoop map/reduce trackers start up, those\n+processes (on local as well as remote nodes) will now need to load\n+KFS's <i> libkfsClient.so </i> library.  To simplify this process, it is advisable to\n+store libkfsClient.so in an NFS accessible directory (similar to where\n+Hadoop binaries/scripts are stored); then, modify Hadoop's\n+conf/hadoop-env.sh adding the following line and providing suitable\n+value for &lt;path&gt;:\n+<pre>\n+export LD_LIBRARY_PATH=&lt;path&gt;\n+</pre>\n+\n+\n+          <li>Start only the map/reduce trackers\n+          <br />\n+          example: execute Hadoop's bin/start-mapred.sh</li>\n+        </ul>\n+<br/>\n+\n+If the map/reduce job trackers start up, all file-I/O is done to KFS.\n+\n+</body>\n+</html>"
        },
        {
            "sha": "71bfdc8a40f89d82bd1c5f98448df5ece5508160",
            "filename": "src/java/org/apache/hadoop/fs/package.html",
            "status": "added",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,23 @@\n+<html>\n+\n+<!--\n+   Licensed to the Apache Software Foundation (ASF) under one or more\n+   contributor license agreements.  See the NOTICE file distributed with\n+   this work for additional information regarding copyright ownership.\n+   The ASF licenses this file to You under the Apache License, Version 2.0\n+   (the \"License\"); you may not use this file except in compliance with\n+   the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+-->\n+\n+<body>\n+An abstract file system API.\n+</body>\n+</html>"
        },
        {
            "sha": "49880f9dcdd2d7de811d9ec8429f7a9986b0688f",
            "filename": "src/java/org/apache/hadoop/fs/permission/AccessControlException.java",
            "status": "added",
            "additions": 61,
            "deletions": 0,
            "changes": 61,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpermission%2FAccessControlException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpermission%2FAccessControlException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpermission%2FAccessControlException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,61 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.permission;\n+\n+import java.io.IOException;\n+\n+/**\n+ * An exception class for access control related issues.\n+ * @deprecated Use {@link org.apache.hadoop.security.AccessControlException} \n+ *             instead.\n+ */\n+@Deprecated\n+public class AccessControlException extends IOException {\n+  //Required by {@link java.io.Serializable}.\n+  private static final long serialVersionUID = 1L;\n+\n+  /**\n+   * Default constructor is needed for unwrapping from \n+   * {@link org.apache.hadoop.ipc.RemoteException}.\n+   */\n+  public AccessControlException() {\n+    super(\"Permission denied.\");\n+  }\n+\n+  /**\n+   * Constructs an {@link AccessControlException}\n+   * with the specified detail message.\n+   * @param s the detail message.\n+   */\n+  public AccessControlException(String s) {\n+    super(s);\n+  }\n+  \n+  /**\n+   * Constructs a new exception with the specified cause and a detail\n+   * message of <tt>(cause==null ? null : cause.toString())</tt> (which\n+   * typically contains the class and detail message of <tt>cause</tt>).\n+   * @param  cause the cause (which is saved for later retrieval by the\n+   *         {@link #getCause()} method).  (A <tt>null</tt> value is\n+   *         permitted, and indicates that the cause is nonexistent or\n+   *         unknown.)\n+   */\n+  public AccessControlException(Throwable cause) {\n+    super(cause);\n+  }\n+}"
        },
        {
            "sha": "5aafd21b33a5bb522cb56a0e1d7a2fa5f399289f",
            "filename": "src/java/org/apache/hadoop/fs/permission/FsAction.java",
            "status": "added",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpermission%2FFsAction.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpermission%2FFsAction.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpermission%2FFsAction.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,67 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.permission;\n+\n+/**\n+ * File system actions, e.g. read, write, etc.\n+ */\n+public enum FsAction {\n+  // POSIX style\n+  NONE(\"---\"),\n+  EXECUTE(\"--x\"),\n+  WRITE(\"-w-\"),\n+  WRITE_EXECUTE(\"-wx\"),\n+  READ(\"r--\"),\n+  READ_EXECUTE(\"r-x\"),\n+  READ_WRITE(\"rw-\"),\n+  ALL(\"rwx\");\n+\n+  /** Retain reference to value array. */\n+  private final static FsAction[] vals = values();\n+\n+  /** Symbolic representation */\n+  public final String SYMBOL;\n+\n+  private FsAction(String s) {\n+    SYMBOL = s;\n+  }\n+\n+  /**\n+   * Return true if this action implies that action.\n+   * @param that\n+   */\n+  public boolean implies(FsAction that) {\n+    if (that != null) {\n+      return (ordinal() & that.ordinal()) == that.ordinal();\n+    }\n+    return false;\n+  }\n+\n+  /** AND operation. */\n+  public FsAction and(FsAction that) {\n+    return vals[ordinal() & that.ordinal()];\n+  }\n+  /** OR operation. */\n+  public FsAction or(FsAction that) {\n+    return vals[ordinal() | that.ordinal()];\n+  }\n+  /** NOT operation. */\n+  public FsAction not() {\n+    return vals[7 - ordinal()];\n+  }\n+}"
        },
        {
            "sha": "e92d35bceacb42e775ad91f12b33e60b5658dfb7",
            "filename": "src/java/org/apache/hadoop/fs/permission/FsPermission.java",
            "status": "added",
            "additions": 232,
            "deletions": 0,
            "changes": 232,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpermission%2FFsPermission.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpermission%2FFsPermission.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpermission%2FFsPermission.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,232 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.permission;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.*;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+\n+/**\n+ * A class for file/directory permissions.\n+ */\n+public class FsPermission implements Writable {\n+  static final WritableFactory FACTORY = new WritableFactory() {\n+    public Writable newInstance() { return new FsPermission(); }\n+  };\n+  static {                                      // register a ctor\n+    WritableFactories.setFactory(FsPermission.class, FACTORY);\n+  }\n+\n+  /** Create an immutable {@link FsPermission} object. */\n+  public static FsPermission createImmutable(short permission) {\n+    return new FsPermission(permission) {\n+      public FsPermission applyUMask(FsPermission umask) {\n+        throw new UnsupportedOperationException();\n+      }\n+      public void readFields(DataInput in) throws IOException {\n+        throw new UnsupportedOperationException();\n+      }\n+    };\n+  }\n+\n+  //POSIX permission style\n+  private FsAction useraction = null;\n+  private FsAction groupaction = null;\n+  private FsAction otheraction = null;\n+  private boolean stickyBit = false;\n+\n+  private FsPermission() {}\n+\n+  /**\n+   * Construct by the given {@link FsAction}.\n+   * @param u user action\n+   * @param g group action\n+   * @param o other action\n+   */\n+  public FsPermission(FsAction u, FsAction g, FsAction o) {\n+    this(u, g, o, false);\n+  }\n+\n+  public FsPermission(FsAction u, FsAction g, FsAction o, boolean sb) {\n+    set(u, g, o, sb);\n+  }\n+\n+  /**\n+   * Construct by the given mode.\n+   * @param mode\n+   * @see #toShort()\n+   */\n+  public FsPermission(short mode) { fromShort(mode); }\n+\n+  /**\n+   * Copy constructor\n+   * \n+   * @param other other permission\n+   */\n+  public FsPermission(FsPermission other) {\n+    this.useraction = other.useraction;\n+    this.groupaction = other.groupaction;\n+    this.otheraction = other.otheraction;\n+  }\n+  \n+  /** Return user {@link FsAction}. */\n+  public FsAction getUserAction() {return useraction;}\n+\n+  /** Return group {@link FsAction}. */\n+  public FsAction getGroupAction() {return groupaction;}\n+\n+  /** Return other {@link FsAction}. */\n+  public FsAction getOtherAction() {return otheraction;}\n+\n+  private void set(FsAction u, FsAction g, FsAction o, boolean sb) {\n+    useraction = u;\n+    groupaction = g;\n+    otheraction = o;\n+    stickyBit = sb;\n+  }\n+\n+  public void fromShort(short n) {\n+    FsAction[] v = FsAction.values();\n+\n+    set(v[(n >>> 6) & 7], v[(n >>> 3) & 7], v[n & 7], (((n >>> 9) & 1) == 1) );\n+  }\n+\n+  /** {@inheritDoc} */\n+  public void write(DataOutput out) throws IOException {\n+    out.writeShort(toShort());\n+  }\n+\n+  /** {@inheritDoc} */\n+  public void readFields(DataInput in) throws IOException {\n+    fromShort(in.readShort());\n+  }\n+\n+  /**\n+   * Create and initialize a {@link FsPermission} from {@link DataInput}.\n+   */\n+  public static FsPermission read(DataInput in) throws IOException {\n+    FsPermission p = new FsPermission();\n+    p.readFields(in);\n+    return p;\n+  }\n+\n+  /**\n+   * Encode the object to a short.\n+   */\n+  public short toShort() {\n+    int s =  (stickyBit ? 1 << 9 : 0)     |\n+             (useraction.ordinal() << 6)  |\n+             (groupaction.ordinal() << 3) |\n+             otheraction.ordinal();\n+\n+    return (short)s;\n+  }\n+\n+  /** {@inheritDoc} */\n+  public boolean equals(Object obj) {\n+    if (obj instanceof FsPermission) {\n+      FsPermission that = (FsPermission)obj;\n+      return this.useraction == that.useraction\n+          && this.groupaction == that.groupaction\n+          && this.otheraction == that.otheraction\n+          && this.stickyBit == that.stickyBit;\n+    }\n+    return false;\n+  }\n+\n+  /** {@inheritDoc} */\n+  public int hashCode() {return toShort();}\n+\n+  /** {@inheritDoc} */\n+  public String toString() {\n+    String str = useraction.SYMBOL + groupaction.SYMBOL + otheraction.SYMBOL;\n+    if(stickyBit) {\n+      StringBuilder str2 = new StringBuilder(str);\n+      str2.replace(str2.length() - 1, str2.length(),\n+           otheraction.implies(FsAction.EXECUTE) ? \"t\" : \"T\");\n+      str = str2.toString();\n+    }\n+\n+    return str;\n+  }\n+\n+  /** Apply a umask to this permission and return a new one */\n+  public FsPermission applyUMask(FsPermission umask) {\n+    return new FsPermission(useraction.and(umask.useraction.not()),\n+        groupaction.and(umask.groupaction.not()),\n+        otheraction.and(umask.otheraction.not()));\n+  }\n+\n+  /** umask property label */\n+  public static final String UMASK_LABEL = \"dfs.umask\";\n+  public static final int DEFAULT_UMASK = 0022;\n+\n+  /** Get the user file creation mask (umask) */\n+  public static FsPermission getUMask(Configuration conf) {\n+    int umask = DEFAULT_UMASK;\n+    if (conf != null) {\n+      umask = conf.getInt(UMASK_LABEL, DEFAULT_UMASK);\n+    }\n+    return new FsPermission((short)umask);\n+  }\n+\n+  public boolean getStickyBit() {\n+    return stickyBit;\n+  }\n+\n+  /** Set the user file creation mask (umask) */\n+  public static void setUMask(Configuration conf, FsPermission umask) {\n+    conf.setInt(UMASK_LABEL, umask.toShort());\n+  }\n+\n+  /** Get the default permission. */\n+  public static FsPermission getDefault() {\n+    return new FsPermission((short)00777);\n+  }\n+\n+  /**\n+   * Create a FsPermission from a Unix symbolic permission string\n+   * @param unixSymbolicPermission e.g. \"-rw-rw-rw-\"\n+   */\n+  public static FsPermission valueOf(String unixSymbolicPermission) {\n+    if (unixSymbolicPermission == null) {\n+      return null;\n+    }\n+    else if (unixSymbolicPermission.length() != 10) {\n+      throw new IllegalArgumentException(\"length != 10(unixSymbolicPermission=\"\n+          + unixSymbolicPermission + \")\");\n+    }\n+\n+    int n = 0;\n+    for(int i = 1; i < unixSymbolicPermission.length(); i++) {\n+      n = n << 1;\n+      char c = unixSymbolicPermission.charAt(i);\n+      n += (c == '-' || c == 'T' || c == 'S') ? 0: 1;\n+    }\n+\n+    // Add sticky bit value if set\n+    if(unixSymbolicPermission.charAt(9) == 't' ||\n+        unixSymbolicPermission.charAt(9) == 'T')\n+      n += 01000;\n+\n+    return new FsPermission((short)n);\n+  }\n+}"
        },
        {
            "sha": "4f36abbe625980cc1273d2439d4b576a11e5bd03",
            "filename": "src/java/org/apache/hadoop/fs/permission/PermissionStatus.java",
            "status": "added",
            "additions": 118,
            "deletions": 0,
            "changes": 118,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpermission%2FPermissionStatus.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpermission%2FPermissionStatus.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fpermission%2FPermissionStatus.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,118 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.permission;\n+\n+import org.apache.hadoop.io.*;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+\n+/**\n+ * Store permission related information.\n+ */\n+public class PermissionStatus implements Writable {\n+  static final WritableFactory FACTORY = new WritableFactory() {\n+    public Writable newInstance() { return new PermissionStatus(); }\n+  };\n+  static {                                      // register a ctor\n+    WritableFactories.setFactory(PermissionStatus.class, FACTORY);\n+  }\n+\n+  /** Create an immutable {@link PermissionStatus} object. */\n+  public static PermissionStatus createImmutable(\n+      String user, String group, FsPermission permission) {\n+    return new PermissionStatus(user, group, permission) {\n+      public PermissionStatus applyUMask(FsPermission umask) {\n+        throw new UnsupportedOperationException();\n+      }\n+      public void readFields(DataInput in) throws IOException {\n+        throw new UnsupportedOperationException();\n+      }\n+    };\n+  }\n+\n+  private String username;\n+  private String groupname;\n+  private FsPermission permission;\n+\n+  private PermissionStatus() {}\n+\n+  /** Constructor */\n+  public PermissionStatus(String user, String group, FsPermission permission) {\n+    username = user;\n+    groupname = group;\n+    this.permission = permission;\n+  }\n+\n+  /** Return user name */\n+  public String getUserName() {return username;}\n+\n+  /** Return group name */\n+  public String getGroupName() {return groupname;}\n+\n+  /** Return permission */\n+  public FsPermission getPermission() {return permission;}\n+\n+  /**\n+   * Apply umask.\n+   * @see FsPermission#applyUMask(FsPermission)\n+   */\n+  public PermissionStatus applyUMask(FsPermission umask) {\n+    permission = permission.applyUMask(umask);\n+    return this;\n+  }\n+\n+  /** {@inheritDoc} */\n+  public void readFields(DataInput in) throws IOException {\n+    username = Text.readString(in);\n+    groupname = Text.readString(in);\n+    permission = FsPermission.read(in);\n+  }\n+\n+  /** {@inheritDoc} */\n+  public void write(DataOutput out) throws IOException {\n+    write(out, username, groupname, permission);\n+  }\n+\n+  /**\n+   * Create and initialize a {@link PermissionStatus} from {@link DataInput}.\n+   */\n+  public static PermissionStatus read(DataInput in) throws IOException {\n+    PermissionStatus p = new PermissionStatus();\n+    p.readFields(in);\n+    return p;\n+  }\n+\n+  /**\n+   * Serialize a {@link PermissionStatus} from its base components.\n+   */\n+  public static void write(DataOutput out,\n+                           String username, \n+                           String groupname,\n+                           FsPermission permission) throws IOException {\n+    Text.writeString(out, username);\n+    Text.writeString(out, groupname);\n+    permission.write(out);\n+  }\n+\n+  /** {@inheritDoc} */\n+  public String toString() {\n+    return username + \":\" + groupname + \":\" + permission;\n+  }\n+}"
        },
        {
            "sha": "e24ad264038e2597af2a9f4308c851c73371a73d",
            "filename": "src/java/org/apache/hadoop/fs/s3/Block.java",
            "status": "added",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FBlock.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FBlock.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FBlock.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,47 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3;\n+\n+/**\n+ * Holds metadata about a block of data being stored in a {@link FileSystemStore}.\n+ */\n+public class Block {\n+  private long id;\n+\n+  private long length;\n+\n+  public Block(long id, long length) {\n+    this.id = id;\n+    this.length = length;\n+  }\n+\n+  public long getId() {\n+    return id;\n+  }\n+\n+  public long getLength() {\n+    return length;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \"Block[\" + id + \", \" + length + \"]\";\n+  }\n+\n+}"
        },
        {
            "sha": "a46472a815032911d914dff1bf0e6f3b885926a8",
            "filename": "src/java/org/apache/hadoop/fs/s3/FileSystemStore.java",
            "status": "added",
            "additions": 63,
            "deletions": 0,
            "changes": 63,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FFileSystemStore.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FFileSystemStore.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FFileSystemStore.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,63 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.Set;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+\n+/**\n+ * A facility for storing and retrieving {@link INode}s and {@link Block}s.\n+ */\n+public interface FileSystemStore {\n+  \n+  void initialize(URI uri, Configuration conf) throws IOException;\n+  String getVersion() throws IOException;\n+\n+  void storeINode(Path path, INode inode) throws IOException;\n+  void storeBlock(Block block, File file) throws IOException;\n+  \n+  boolean inodeExists(Path path) throws IOException;\n+  boolean blockExists(long blockId) throws IOException;\n+\n+  INode retrieveINode(Path path) throws IOException;\n+  File retrieveBlock(Block block, long byteRangeStart) throws IOException;\n+\n+  void deleteINode(Path path) throws IOException;\n+  void deleteBlock(Block block) throws IOException;\n+\n+  Set<Path> listSubPaths(Path path) throws IOException;\n+  Set<Path> listDeepSubPaths(Path path) throws IOException;\n+\n+  /**\n+   * Delete everything. Used for testing.\n+   * @throws IOException\n+   */\n+  void purge() throws IOException;\n+  \n+  /**\n+   * Diagnostic method to dump all INodes to the console.\n+   * @throws IOException\n+   */\n+  void dump() throws IOException;\n+}"
        },
        {
            "sha": "ec7f67c266c76c474fb985931863596ba06fe8bd",
            "filename": "src/java/org/apache/hadoop/fs/s3/INode.java",
            "status": "added",
            "additions": 117,
            "deletions": 0,
            "changes": 117,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FINode.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FINode.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FINode.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,117 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataInputStream;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Holds file metadata including type (regular file, or directory),\n+ * and the list of blocks that are pointers to the data.\n+ */\n+public class INode {\n+\t\n+  enum FileType {\n+    DIRECTORY, FILE\n+  }\n+  \n+  public static final FileType[] FILE_TYPES = {\n+    FileType.DIRECTORY,\n+    FileType.FILE\n+  };\n+\n+  public static final INode DIRECTORY_INODE = new INode(FileType.DIRECTORY, null);\n+  \n+  private FileType fileType;\n+  private Block[] blocks;\n+\n+  public INode(FileType fileType, Block[] blocks) {\n+    this.fileType = fileType;\n+    if (isDirectory() && blocks != null) {\n+      throw new IllegalArgumentException(\"A directory cannot contain blocks.\");\n+    }\n+    this.blocks = blocks;\n+  }\n+\n+  public Block[] getBlocks() {\n+    return blocks;\n+  }\n+  \n+  public FileType getFileType() {\n+    return fileType;\n+  }\n+\n+  public boolean isDirectory() {\n+    return fileType == FileType.DIRECTORY;\n+  }  \n+\n+  public boolean isFile() {\n+    return fileType == FileType.FILE;\n+  }\n+  \n+  public long getSerializedLength() {\n+    return 1L + (blocks == null ? 0 : 4 + blocks.length * 16);\n+  }\n+  \n+\n+  public InputStream serialize() throws IOException {\n+    ByteArrayOutputStream bytes = new ByteArrayOutputStream();\n+    DataOutputStream out = new DataOutputStream(bytes);\n+    out.writeByte(fileType.ordinal());\n+    if (isFile()) {\n+      out.writeInt(blocks.length);\n+      for (int i = 0; i < blocks.length; i++) {\n+        out.writeLong(blocks[i].getId());\n+        out.writeLong(blocks[i].getLength());\n+      }\n+    }\n+    out.close();\n+    return new ByteArrayInputStream(bytes.toByteArray());\n+  }\n+  \n+  public static INode deserialize(InputStream in) throws IOException {\n+    if (in == null) {\n+      return null;\n+    }\n+    DataInputStream dataIn = new DataInputStream(in);\n+    FileType fileType = INode.FILE_TYPES[dataIn.readByte()];\n+    switch (fileType) {\n+    case DIRECTORY:\n+      in.close();\n+      return INode.DIRECTORY_INODE;\n+    case FILE:\n+      int numBlocks = dataIn.readInt();\n+      Block[] blocks = new Block[numBlocks];\n+      for (int i = 0; i < numBlocks; i++) {\n+        long id = dataIn.readLong();\n+        long length = dataIn.readLong();\n+        blocks[i] = new Block(id, length);\n+      }\n+      in.close();\n+      return new INode(fileType, blocks);\n+    default:\n+      throw new IllegalArgumentException(\"Cannot deserialize inode.\");\n+    }    \n+  }  \n+  \n+}"
        },
        {
            "sha": "b5131d624499a6ae086776782a8a4e7004729f7a",
            "filename": "src/java/org/apache/hadoop/fs/s3/Jets3tFileSystemStore.java",
            "status": "added",
            "additions": 390,
            "deletions": 0,
            "changes": 390,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FJets3tFileSystemStore.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FJets3tFileSystemStore.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FJets3tFileSystemStore.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,390 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3;\n+\n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.Closeable;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.s3.INode.FileType;\n+import org.jets3t.service.S3Service;\n+import org.jets3t.service.S3ServiceException;\n+import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n+import org.jets3t.service.model.S3Bucket;\n+import org.jets3t.service.model.S3Object;\n+import org.jets3t.service.security.AWSCredentials;\n+\n+class Jets3tFileSystemStore implements FileSystemStore {\n+  \n+  private static final String FILE_SYSTEM_NAME = \"fs\";\n+  private static final String FILE_SYSTEM_VALUE = \"Hadoop\";\n+\n+  private static final String FILE_SYSTEM_TYPE_NAME = \"fs-type\";\n+  private static final String FILE_SYSTEM_TYPE_VALUE = \"block\";\n+\n+  private static final String FILE_SYSTEM_VERSION_NAME = \"fs-version\";\n+  private static final String FILE_SYSTEM_VERSION_VALUE = \"1\";\n+  \n+  private static final Map<String, String> METADATA =\n+    new HashMap<String, String>();\n+  \n+  static {\n+    METADATA.put(FILE_SYSTEM_NAME, FILE_SYSTEM_VALUE);\n+    METADATA.put(FILE_SYSTEM_TYPE_NAME, FILE_SYSTEM_TYPE_VALUE);\n+    METADATA.put(FILE_SYSTEM_VERSION_NAME, FILE_SYSTEM_VERSION_VALUE);\n+  }\n+\n+  private static final String PATH_DELIMITER = Path.SEPARATOR;\n+  private static final String BLOCK_PREFIX = \"block_\";\n+\n+  private Configuration conf;\n+  \n+  private S3Service s3Service;\n+\n+  private S3Bucket bucket;\n+  \n+  private int bufferSize;\n+  \n+  private static final Log LOG = \n+    LogFactory.getLog(Jets3tFileSystemStore.class.getName());\n+  \n+  public void initialize(URI uri, Configuration conf) throws IOException {\n+    \n+    this.conf = conf;\n+    \n+    S3Credentials s3Credentials = new S3Credentials();\n+    s3Credentials.initialize(uri, conf);\n+    try {\n+      AWSCredentials awsCredentials =\n+        new AWSCredentials(s3Credentials.getAccessKey(),\n+            s3Credentials.getSecretAccessKey());\n+      this.s3Service = new RestS3Service(awsCredentials);\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+    bucket = new S3Bucket(uri.getHost());\n+\n+    this.bufferSize = conf.getInt(\"io.file.buffer.size\", 4096);\n+  }\n+\n+  public String getVersion() throws IOException {\n+    return FILE_SYSTEM_VERSION_VALUE;\n+  }\n+\n+  private void delete(String key) throws IOException {\n+    try {\n+      s3Service.deleteObject(bucket, key);\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+\n+  public void deleteINode(Path path) throws IOException {\n+    delete(pathToKey(path));\n+  }\n+\n+  public void deleteBlock(Block block) throws IOException {\n+    delete(blockToKey(block));\n+  }\n+\n+  public boolean inodeExists(Path path) throws IOException {\n+    InputStream in = get(pathToKey(path), true);\n+    if (in == null) {\n+      return false;\n+    }\n+    in.close();\n+    return true;\n+  }\n+  \n+  public boolean blockExists(long blockId) throws IOException {\n+    InputStream in = get(blockToKey(blockId), false);\n+    if (in == null) {\n+      return false;\n+    }\n+    in.close();\n+    return true;\n+  }\n+\n+  private InputStream get(String key, boolean checkMetadata)\n+      throws IOException {\n+    \n+    try {\n+      S3Object object = s3Service.getObject(bucket, key);\n+      if (checkMetadata) {\n+        checkMetadata(object);\n+      }\n+      return object.getDataInputStream();\n+    } catch (S3ServiceException e) {\n+      if (\"NoSuchKey\".equals(e.getS3ErrorCode())) {\n+        return null;\n+      }\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+\n+  private InputStream get(String key, long byteRangeStart) throws IOException {\n+    try {\n+      S3Object object = s3Service.getObject(bucket, key, null, null, null,\n+                                            null, byteRangeStart, null);\n+      return object.getDataInputStream();\n+    } catch (S3ServiceException e) {\n+      if (\"NoSuchKey\".equals(e.getS3ErrorCode())) {\n+        return null;\n+      }\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+\n+  private void checkMetadata(S3Object object) throws S3FileSystemException,\n+      S3ServiceException {\n+    \n+    String name = (String) object.getMetadata(FILE_SYSTEM_NAME);\n+    if (!FILE_SYSTEM_VALUE.equals(name)) {\n+      throw new S3FileSystemException(\"Not a Hadoop S3 file.\");\n+    }\n+    String type = (String) object.getMetadata(FILE_SYSTEM_TYPE_NAME);\n+    if (!FILE_SYSTEM_TYPE_VALUE.equals(type)) {\n+      throw new S3FileSystemException(\"Not a block file.\");\n+    }\n+    String dataVersion = (String) object.getMetadata(FILE_SYSTEM_VERSION_NAME);\n+    if (!FILE_SYSTEM_VERSION_VALUE.equals(dataVersion)) {\n+      throw new VersionMismatchException(FILE_SYSTEM_VERSION_VALUE,\n+          dataVersion);\n+    }\n+  }\n+\n+  public INode retrieveINode(Path path) throws IOException {\n+    return INode.deserialize(get(pathToKey(path), true));\n+  }\n+\n+  public File retrieveBlock(Block block, long byteRangeStart)\n+    throws IOException {\n+    File fileBlock = null;\n+    InputStream in = null;\n+    OutputStream out = null;\n+    try {\n+      fileBlock = newBackupFile();\n+      in = get(blockToKey(block), byteRangeStart);\n+      out = new BufferedOutputStream(new FileOutputStream(fileBlock));\n+      byte[] buf = new byte[bufferSize];\n+      int numRead;\n+      while ((numRead = in.read(buf)) >= 0) {\n+        out.write(buf, 0, numRead);\n+      }\n+      return fileBlock;\n+    } catch (IOException e) {\n+      // close output stream to file then delete file\n+      closeQuietly(out);\n+      out = null; // to prevent a second close\n+      if (fileBlock != null) {\n+        boolean b = fileBlock.delete();\n+        if (!b) {\n+          LOG.warn(\"Ignoring failed delete\");\n+        }\n+      }\n+      throw e;\n+    } finally {\n+      closeQuietly(out);\n+      closeQuietly(in);\n+    }\n+  }\n+  \n+  private File newBackupFile() throws IOException {\n+    File dir = new File(conf.get(\"fs.s3.buffer.dir\"));\n+    if (!dir.exists() && !dir.mkdirs()) {\n+      throw new IOException(\"Cannot create S3 buffer directory: \" + dir);\n+    }\n+    File result = File.createTempFile(\"input-\", \".tmp\", dir);\n+    result.deleteOnExit();\n+    return result;\n+  }\n+\n+  public Set<Path> listSubPaths(Path path) throws IOException {\n+    try {\n+      String prefix = pathToKey(path);\n+      if (!prefix.endsWith(PATH_DELIMITER)) {\n+        prefix += PATH_DELIMITER;\n+      }\n+      S3Object[] objects = s3Service.listObjects(bucket, prefix, PATH_DELIMITER);\n+      Set<Path> prefixes = new TreeSet<Path>();\n+      for (int i = 0; i < objects.length; i++) {\n+        prefixes.add(keyToPath(objects[i].getKey()));\n+      }\n+      prefixes.remove(path);\n+      return prefixes;\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+  \n+  public Set<Path> listDeepSubPaths(Path path) throws IOException {\n+    try {\n+      String prefix = pathToKey(path);\n+      if (!prefix.endsWith(PATH_DELIMITER)) {\n+        prefix += PATH_DELIMITER;\n+      }\n+      S3Object[] objects = s3Service.listObjects(bucket, prefix, null);\n+      Set<Path> prefixes = new TreeSet<Path>();\n+      for (int i = 0; i < objects.length; i++) {\n+        prefixes.add(keyToPath(objects[i].getKey()));\n+      }\n+      prefixes.remove(path);\n+      return prefixes;\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }    \n+  }\n+\n+  private void put(String key, InputStream in, long length, boolean storeMetadata)\n+      throws IOException {\n+    \n+    try {\n+      S3Object object = new S3Object(key);\n+      object.setDataInputStream(in);\n+      object.setContentType(\"binary/octet-stream\");\n+      object.setContentLength(length);\n+      if (storeMetadata) {\n+        object.addAllMetadata(METADATA);\n+      }\n+      s3Service.putObject(bucket, object);\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+\n+  public void storeINode(Path path, INode inode) throws IOException {\n+    put(pathToKey(path), inode.serialize(), inode.getSerializedLength(), true);\n+  }\n+\n+  public void storeBlock(Block block, File file) throws IOException {\n+    BufferedInputStream in = null;\n+    try {\n+      in = new BufferedInputStream(new FileInputStream(file));\n+      put(blockToKey(block), in, block.getLength(), false);\n+    } finally {\n+      closeQuietly(in);\n+    }    \n+  }\n+\n+  private void closeQuietly(Closeable closeable) {\n+    if (closeable != null) {\n+      try {\n+        closeable.close();\n+      } catch (IOException e) {\n+        // ignore\n+      }\n+    }\n+  }\n+\n+  private String pathToKey(Path path) {\n+    if (!path.isAbsolute()) {\n+      throw new IllegalArgumentException(\"Path must be absolute: \" + path);\n+    }\n+    return path.toUri().getPath();\n+  }\n+\n+  private Path keyToPath(String key) {\n+    return new Path(key);\n+  }\n+  \n+  private String blockToKey(long blockId) {\n+    return BLOCK_PREFIX + blockId;\n+  }\n+\n+  private String blockToKey(Block block) {\n+    return blockToKey(block.getId());\n+  }\n+\n+  public void purge() throws IOException {\n+    try {\n+      S3Object[] objects = s3Service.listObjects(bucket);\n+      for (int i = 0; i < objects.length; i++) {\n+        s3Service.deleteObject(bucket, objects[i].getKey());\n+      }\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+\n+  public void dump() throws IOException {\n+    StringBuilder sb = new StringBuilder(\"S3 Filesystem, \");\n+    sb.append(bucket.getName()).append(\"\\n\");\n+    try {\n+      S3Object[] objects = s3Service.listObjects(bucket, PATH_DELIMITER, null);\n+      for (int i = 0; i < objects.length; i++) {\n+        Path path = keyToPath(objects[i].getKey());\n+        sb.append(path).append(\"\\n\");\n+        INode m = retrieveINode(path);\n+        sb.append(\"\\t\").append(m.getFileType()).append(\"\\n\");\n+        if (m.getFileType() == FileType.DIRECTORY) {\n+          continue;\n+        }\n+        for (int j = 0; j < m.getBlocks().length; j++) {\n+          sb.append(\"\\t\").append(m.getBlocks()[j]).append(\"\\n\");\n+        }\n+      }\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+    System.out.println(sb);\n+  }\n+\n+}"
        },
        {
            "sha": "cce31f0869f89536165f8e10f74b6a36f3233735",
            "filename": "src/java/org/apache/hadoop/fs/s3/MigrationTool.java",
            "status": "added",
            "additions": 280,
            "deletions": 0,
            "changes": 280,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FMigrationTool.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FMigrationTool.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FMigrationTool.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,280 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.s3;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.UnsupportedEncodingException;\n+import java.net.URI;\n+import java.net.URLDecoder;\n+import java.net.URLEncoder;\n+import java.util.Set;\n+import java.util.TreeSet;\n+\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.util.Tool;\n+import org.apache.hadoop.util.ToolRunner;\n+import org.jets3t.service.S3Service;\n+import org.jets3t.service.S3ServiceException;\n+import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n+import org.jets3t.service.model.S3Bucket;\n+import org.jets3t.service.model.S3Object;\n+import org.jets3t.service.security.AWSCredentials;\n+\n+/**\n+ * <p>\n+ * This class is a tool for migrating data from an older to a newer version\n+ * of an S3 filesystem.\n+ * </p>\n+ * <p>\n+ * All files in the filesystem are migrated by re-writing the block metadata\n+ * - no datafiles are touched.\n+ * </p>\n+ */\n+public class MigrationTool extends Configured implements Tool {\n+  \n+  private S3Service s3Service;\n+  private S3Bucket bucket;\n+  \n+  public static void main(String[] args) throws Exception {\n+    int res = ToolRunner.run(new MigrationTool(), args);\n+    System.exit(res);\n+  }\n+  \n+  public int run(String[] args) throws Exception {\n+    \n+    if (args.length == 0) {\n+      System.err.println(\"Usage: MigrationTool <S3 file system URI>\");\n+      System.err.println(\"\\t<S3 file system URI>\\tfilesystem to migrate\");\n+      ToolRunner.printGenericCommandUsage(System.err);\n+      return -1;\n+    }\n+    \n+    URI uri = URI.create(args[0]);\n+    \n+    initialize(uri);\n+    \n+    FileSystemStore newStore = new Jets3tFileSystemStore();\n+    newStore.initialize(uri, getConf());\n+    \n+    if (get(\"%2F\") != null) { \n+      System.err.println(\"Current version number is [unversioned].\");\n+      System.err.println(\"Target version number is \" +\n+          newStore.getVersion() + \".\");\n+      Store oldStore = new UnversionedStore();\n+      migrate(oldStore, newStore);\n+      return 0;\n+    } else {\n+      S3Object root = get(\"/\");\n+      if (root != null) {\n+        String version = (String) root.getMetadata(\"fs-version\");\n+        if (version == null) {\n+          System.err.println(\"Can't detect version - exiting.\");\n+        } else {\n+          String newVersion = newStore.getVersion();\n+          System.err.println(\"Current version number is \" + version + \".\");\n+          System.err.println(\"Target version number is \" + newVersion + \".\");\n+          if (version.equals(newStore.getVersion())) {\n+            System.err.println(\"No migration required.\");\n+            return 0;\n+          }\n+          // use version number to create Store\n+          //Store oldStore = ... \n+          //migrate(oldStore, newStore);\n+          System.err.println(\"Not currently implemented.\");\n+          return 0;\n+        }\n+      }\n+      System.err.println(\"Can't detect version - exiting.\");\n+      return 0;\n+    }\n+    \n+  }\n+  \n+  public void initialize(URI uri) throws IOException {\n+    \n+    \n+    \n+    try {\n+      String accessKey = null;\n+      String secretAccessKey = null;\n+      String userInfo = uri.getUserInfo();\n+      if (userInfo != null) {\n+        int index = userInfo.indexOf(':');\n+        if (index != -1) {\n+          accessKey = userInfo.substring(0, index);\n+          secretAccessKey = userInfo.substring(index + 1);\n+        } else {\n+          accessKey = userInfo;\n+        }\n+      }\n+      if (accessKey == null) {\n+        accessKey = getConf().get(\"fs.s3.awsAccessKeyId\");\n+      }\n+      if (secretAccessKey == null) {\n+        secretAccessKey = getConf().get(\"fs.s3.awsSecretAccessKey\");\n+      }\n+      if (accessKey == null && secretAccessKey == null) {\n+        throw new IllegalArgumentException(\"AWS \" +\n+                                           \"Access Key ID and Secret Access Key \" +\n+                                           \"must be specified as the username \" +\n+                                           \"or password (respectively) of a s3 URL, \" +\n+                                           \"or by setting the \" +\n+                                           \"fs.s3.awsAccessKeyId or \" +                         \n+                                           \"fs.s3.awsSecretAccessKey properties (respectively).\");\n+      } else if (accessKey == null) {\n+        throw new IllegalArgumentException(\"AWS \" +\n+                                           \"Access Key ID must be specified \" +\n+                                           \"as the username of a s3 URL, or by setting the \" +\n+                                           \"fs.s3.awsAccessKeyId property.\");\n+      } else if (secretAccessKey == null) {\n+        throw new IllegalArgumentException(\"AWS \" +\n+                                           \"Secret Access Key must be specified \" +\n+                                           \"as the password of a s3 URL, or by setting the \" +\n+                                           \"fs.s3.awsSecretAccessKey property.\");         \n+      }\n+      AWSCredentials awsCredentials =\n+        new AWSCredentials(accessKey, secretAccessKey);\n+      this.s3Service = new RestS3Service(awsCredentials);\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+    bucket = new S3Bucket(uri.getHost());\n+  }\n+  \n+  private void migrate(Store oldStore, FileSystemStore newStore)\n+      throws IOException {\n+    for (Path path : oldStore.listAllPaths()) {\n+      INode inode = oldStore.retrieveINode(path);\n+      oldStore.deleteINode(path);\n+      newStore.storeINode(path, inode);\n+    }\n+  }\n+  \n+  private S3Object get(String key) {\n+    try {\n+      return s3Service.getObject(bucket, key);\n+    } catch (S3ServiceException e) {\n+      if (\"NoSuchKey\".equals(e.getS3ErrorCode())) {\n+        return null;\n+      }\n+    }\n+    return null;\n+  }\n+  \n+  interface Store {\n+\n+    Set<Path> listAllPaths() throws IOException;\n+    INode retrieveINode(Path path) throws IOException;\n+    void deleteINode(Path path) throws IOException;\n+    \n+  }\n+  \n+  class UnversionedStore implements Store {\n+\n+    public Set<Path> listAllPaths() throws IOException {\n+      try {\n+        String prefix = urlEncode(Path.SEPARATOR);\n+        S3Object[] objects = s3Service.listObjects(bucket, prefix, null);\n+        Set<Path> prefixes = new TreeSet<Path>();\n+        for (int i = 0; i < objects.length; i++) {\n+          prefixes.add(keyToPath(objects[i].getKey()));\n+        }\n+        return prefixes;\n+      } catch (S3ServiceException e) {\n+        if (e.getCause() instanceof IOException) {\n+          throw (IOException) e.getCause();\n+        }\n+        throw new S3Exception(e);\n+      }   \n+    }\n+\n+    public void deleteINode(Path path) throws IOException {\n+      delete(pathToKey(path));\n+    }\n+    \n+    private void delete(String key) throws IOException {\n+      try {\n+        s3Service.deleteObject(bucket, key);\n+      } catch (S3ServiceException e) {\n+        if (e.getCause() instanceof IOException) {\n+          throw (IOException) e.getCause();\n+        }\n+        throw new S3Exception(e);\n+      }\n+    }\n+    \n+    public INode retrieveINode(Path path) throws IOException {\n+      return INode.deserialize(get(pathToKey(path)));\n+    }\n+\n+    private InputStream get(String key) throws IOException {\n+      try {\n+        S3Object object = s3Service.getObject(bucket, key);\n+        return object.getDataInputStream();\n+      } catch (S3ServiceException e) {\n+        if (\"NoSuchKey\".equals(e.getS3ErrorCode())) {\n+          return null;\n+        }\n+        if (e.getCause() instanceof IOException) {\n+          throw (IOException) e.getCause();\n+        }\n+        throw new S3Exception(e);\n+      }\n+    }\n+    \n+    private String pathToKey(Path path) {\n+      if (!path.isAbsolute()) {\n+        throw new IllegalArgumentException(\"Path must be absolute: \" + path);\n+      }\n+      return urlEncode(path.toUri().getPath());\n+    }\n+    \n+    private Path keyToPath(String key) {\n+      return new Path(urlDecode(key));\n+    }\n+\n+    private String urlEncode(String s) {\n+      try {\n+        return URLEncoder.encode(s, \"UTF-8\");\n+      } catch (UnsupportedEncodingException e) {\n+        // Should never happen since every implementation of the Java Platform\n+        // is required to support UTF-8.\n+        // See http://java.sun.com/j2se/1.5.0/docs/api/java/nio/charset/Charset.html\n+        throw new IllegalStateException(e);\n+      }\n+    }\n+    \n+    private String urlDecode(String s) {\n+      try {\n+        return URLDecoder.decode(s, \"UTF-8\");\n+      } catch (UnsupportedEncodingException e) {\n+        // Should never happen since every implementation of the Java Platform\n+        // is required to support UTF-8.\n+        // See http://java.sun.com/j2se/1.5.0/docs/api/java/nio/charset/Charset.html\n+        throw new IllegalStateException(e);\n+      }\n+    }\n+    \n+  }\n+  \n+}"
        },
        {
            "sha": "039499e2a65602a9d4853e51475ae727280029b8",
            "filename": "src/java/org/apache/hadoop/fs/s3/S3Credentials.java",
            "status": "added",
            "additions": 99,
            "deletions": 0,
            "changes": 99,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3Credentials.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3Credentials.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3Credentials.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,99 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3;\n+\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+\n+/**\n+ * <p>\n+ * Extracts AWS credentials from the filesystem URI or configuration.\n+ * </p>\n+ */\n+public class S3Credentials {\n+  \n+  private String accessKey;\n+  private String secretAccessKey; \n+\n+  /**\n+   * @throws IllegalArgumentException if credentials for S3 cannot be\n+   * determined.\n+   */\n+  public void initialize(URI uri, Configuration conf) {\n+    if (uri.getHost() == null) {\n+      throw new IllegalArgumentException(\"Invalid hostname in URI \" + uri);\n+    }\n+    \n+    String userInfo = uri.getUserInfo();\n+    if (userInfo != null) {\n+      int index = userInfo.indexOf(':');\n+      if (index != -1) {\n+        accessKey = userInfo.substring(0, index);\n+        secretAccessKey = userInfo.substring(index + 1);\n+      } else {\n+        accessKey = userInfo;\n+      }\n+    }\n+    \n+    String scheme = uri.getScheme();\n+    String accessKeyProperty = String.format(\"fs.%s.awsAccessKeyId\", scheme);\n+    String secretAccessKeyProperty =\n+      String.format(\"fs.%s.awsSecretAccessKey\", scheme);\n+    if (accessKey == null) {\n+      accessKey = conf.get(accessKeyProperty);\n+    }\n+    if (secretAccessKey == null) {\n+      secretAccessKey = conf.get(secretAccessKeyProperty);\n+    }\n+    if (accessKey == null && secretAccessKey == null) {\n+      throw new IllegalArgumentException(\"AWS \" +\n+                                         \"Access Key ID and Secret Access \" +\n+                                         \"Key must be specified as the \" +\n+                                         \"username or password \" +\n+                                         \"(respectively) of a \" + scheme +\n+                                         \" URL, or by setting the \" +\n+                                         accessKeyProperty + \" or \" +\n+                                         secretAccessKeyProperty +\n+                                         \" properties (respectively).\");\n+    } else if (accessKey == null) {\n+      throw new IllegalArgumentException(\"AWS \" +\n+                                         \"Access Key ID must be specified \" +\n+                                         \"as the username of a \" + scheme +\n+                                         \" URL, or by setting the \" +\n+                                         accessKeyProperty + \" property.\");\n+    } else if (secretAccessKey == null) {\n+      throw new IllegalArgumentException(\"AWS \" +\n+                                         \"Secret Access Key must be \" +\n+                                         \"specified as the password of a \" +\n+                                         scheme + \" URL, or by setting the \" +\n+                                         secretAccessKeyProperty +\n+                                         \" property.\");       \n+    }\n+\n+  }\n+  \n+  public String getAccessKey() {\n+    return accessKey;\n+  }\n+  \n+  public String getSecretAccessKey() {\n+    return secretAccessKey;\n+  }\n+}"
        },
        {
            "sha": "7047676a6c7f016fa550355053d41b2fd8d75e5f",
            "filename": "src/java/org/apache/hadoop/fs/s3/S3Exception.java",
            "status": "added",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3Exception.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3Exception.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3Exception.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,34 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Thrown if there is a problem communicating with Amazon S3.\n+ */\n+public class S3Exception extends IOException {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  public S3Exception(Throwable t) {\n+    super(t);\n+  }\n+\n+}"
        },
        {
            "sha": "b0013aa0a96e04746d2277b36f2e8e6e3be5ba38",
            "filename": "src/java/org/apache/hadoop/fs/s3/S3FileSystem.java",
            "status": "added",
            "additions": 361,
            "deletions": 0,
            "changes": 361,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3FileSystem.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3FileSystem.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3FileSystem.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,361 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.fs.s3native.NativeS3FileSystem;\n+import org.apache.hadoop.io.retry.RetryPolicies;\n+import org.apache.hadoop.io.retry.RetryPolicy;\n+import org.apache.hadoop.io.retry.RetryProxy;\n+import org.apache.hadoop.util.Progressable;\n+\n+/**\n+ * <p>\n+ * A block-based {@link FileSystem} backed by\n+ * <a href=\"http://aws.amazon.com/s3\">Amazon S3</a>.\n+ * </p>\n+ * @see NativeS3FileSystem\n+ */\n+public class S3FileSystem extends FileSystem {\n+\n+  private URI uri;\n+\n+  private FileSystemStore store;\n+\n+  private Path workingDir;\n+\n+  public S3FileSystem() {\n+    // set store in initialize()\n+  }\n+  \n+  public S3FileSystem(FileSystemStore store) {\n+    this.store = store;\n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  @Override\n+  public void initialize(URI uri, Configuration conf) throws IOException {\n+    super.initialize(uri, conf);\n+    if (store == null) {\n+      store = createDefaultStore(conf);\n+    }\n+    store.initialize(uri, conf);\n+    setConf(conf);\n+    this.uri = URI.create(uri.getScheme() + \"://\" + uri.getAuthority());    \n+    this.workingDir =\n+      new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this);\n+  }  \n+\n+  private static FileSystemStore createDefaultStore(Configuration conf) {\n+    FileSystemStore store = new Jets3tFileSystemStore();\n+    \n+    RetryPolicy basePolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(\n+                                                                               conf.getInt(\"fs.s3.maxRetries\", 4),\n+                                                                               conf.getLong(\"fs.s3.sleepTimeSeconds\", 10), TimeUnit.SECONDS);\n+    Map<Class<? extends Exception>,RetryPolicy> exceptionToPolicyMap =\n+      new HashMap<Class<? extends Exception>, RetryPolicy>();\n+    exceptionToPolicyMap.put(IOException.class, basePolicy);\n+    exceptionToPolicyMap.put(S3Exception.class, basePolicy);\n+    \n+    RetryPolicy methodPolicy = RetryPolicies.retryByException(\n+                                                              RetryPolicies.TRY_ONCE_THEN_FAIL, exceptionToPolicyMap);\n+    Map<String,RetryPolicy> methodNameToPolicyMap = new HashMap<String,RetryPolicy>();\n+    methodNameToPolicyMap.put(\"storeBlock\", methodPolicy);\n+    methodNameToPolicyMap.put(\"retrieveBlock\", methodPolicy);\n+    \n+    return (FileSystemStore) RetryProxy.create(FileSystemStore.class,\n+                                               store, methodNameToPolicyMap);\n+  }\n+\n+  @Override\n+  public Path getWorkingDirectory() {\n+    return workingDir;\n+  }\n+\n+  @Override\n+  public void setWorkingDirectory(Path dir) {\n+    workingDir = makeAbsolute(dir);\n+  }\n+\n+  private Path makeAbsolute(Path path) {\n+    if (path.isAbsolute()) {\n+      return path;\n+    }\n+    return new Path(workingDir, path);\n+  }\n+\n+  /**\n+   * @param permission Currently ignored.\n+   */\n+  @Override\n+  public boolean mkdirs(Path path, FsPermission permission) throws IOException {\n+    Path absolutePath = makeAbsolute(path);\n+    List<Path> paths = new ArrayList<Path>();\n+    do {\n+      paths.add(0, absolutePath);\n+      absolutePath = absolutePath.getParent();\n+    } while (absolutePath != null);\n+    \n+    boolean result = true;\n+    for (Path p : paths) {\n+      result &= mkdir(p);\n+    }\n+    return result;\n+  }\n+  \n+  private boolean mkdir(Path path) throws IOException {\n+    Path absolutePath = makeAbsolute(path);\n+    INode inode = store.retrieveINode(absolutePath);\n+    if (inode == null) {\n+      store.storeINode(absolutePath, INode.DIRECTORY_INODE);\n+    } else if (inode.isFile()) {\n+      throw new IOException(String.format(\n+          \"Can't make directory for path %s since it is a file.\",\n+          absolutePath));\n+    }\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean isFile(Path path) throws IOException {\n+    INode inode = store.retrieveINode(makeAbsolute(path));\n+    if (inode == null) {\n+      return false;\n+    }\n+    return inode.isFile();\n+  }\n+\n+  private INode checkFile(Path path) throws IOException {\n+    INode inode = store.retrieveINode(makeAbsolute(path));\n+    if (inode == null) {\n+      throw new IOException(\"No such file.\");\n+    }\n+    if (inode.isDirectory()) {\n+      throw new IOException(\"Path \" + path + \" is a directory.\");\n+    }\n+    return inode;\n+  }\n+\n+  @Override\n+  public FileStatus[] listStatus(Path f) throws IOException {\n+    Path absolutePath = makeAbsolute(f);\n+    INode inode = store.retrieveINode(absolutePath);\n+    if (inode == null) {\n+      return null;\n+    }\n+    if (inode.isFile()) {\n+      return new FileStatus[] {\n+        new S3FileStatus(f.makeQualified(this), inode)\n+      };\n+    }\n+    ArrayList<FileStatus> ret = new ArrayList<FileStatus>();\n+    for (Path p : store.listSubPaths(absolutePath)) {\n+      ret.add(getFileStatus(p.makeQualified(this)));\n+    }\n+    return ret.toArray(new FileStatus[0]);\n+  }\n+\n+  /** This optional operation is not yet supported. */\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new IOException(\"Not supported\");\n+  }\n+\n+  /**\n+   * @param permission Currently ignored.\n+   */\n+  @Override\n+  public FSDataOutputStream create(Path file, FsPermission permission,\n+      boolean overwrite, int bufferSize,\n+      short replication, long blockSize, Progressable progress)\n+    throws IOException {\n+\n+    INode inode = store.retrieveINode(makeAbsolute(file));\n+    if (inode != null) {\n+      if (overwrite) {\n+        delete(file, true);\n+      } else {\n+        throw new IOException(\"File already exists: \" + file);\n+      }\n+    } else {\n+      Path parent = file.getParent();\n+      if (parent != null) {\n+        if (!mkdirs(parent)) {\n+          throw new IOException(\"Mkdirs failed to create \" + parent.toString());\n+        }\n+      }      \n+    }\n+    return new FSDataOutputStream\n+        (new S3OutputStream(getConf(), store, makeAbsolute(file),\n+                            blockSize, progress, bufferSize),\n+         statistics);\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path path, int bufferSize) throws IOException {\n+    INode inode = checkFile(path);\n+    return new FSDataInputStream(new S3InputStream(getConf(), store, inode,\n+                                                   statistics));\n+  }\n+\n+  @Override\n+  public boolean rename(Path src, Path dst) throws IOException {\n+    Path absoluteSrc = makeAbsolute(src);\n+    INode srcINode = store.retrieveINode(absoluteSrc);\n+    if (srcINode == null) {\n+      // src path doesn't exist\n+      return false; \n+    }\n+    Path absoluteDst = makeAbsolute(dst);\n+    INode dstINode = store.retrieveINode(absoluteDst);\n+    if (dstINode != null && dstINode.isDirectory()) {\n+      absoluteDst = new Path(absoluteDst, absoluteSrc.getName());\n+      dstINode = store.retrieveINode(absoluteDst);\n+    }\n+    if (dstINode != null) {\n+      // dst path already exists - can't overwrite\n+      return false;\n+    }\n+    Path dstParent = absoluteDst.getParent();\n+    if (dstParent != null) {\n+      INode dstParentINode = store.retrieveINode(dstParent);\n+      if (dstParentINode == null || dstParentINode.isFile()) {\n+        // dst parent doesn't exist or is a file\n+        return false;\n+      }\n+    }\n+    return renameRecursive(absoluteSrc, absoluteDst);\n+  }\n+  \n+  private boolean renameRecursive(Path src, Path dst) throws IOException {\n+    INode srcINode = store.retrieveINode(src);\n+    store.storeINode(dst, srcINode);\n+    store.deleteINode(src);\n+    if (srcINode.isDirectory()) {\n+      for (Path oldSrc : store.listDeepSubPaths(src)) {\n+        INode inode = store.retrieveINode(oldSrc);\n+        if (inode == null) {\n+          return false;\n+        }\n+        String oldSrcPath = oldSrc.toUri().getPath();\n+        String srcPath = src.toUri().getPath();\n+        String dstPath = dst.toUri().getPath();\n+        Path newDst = new Path(oldSrcPath.replaceFirst(srcPath, dstPath));\n+        store.storeINode(newDst, inode);\n+        store.deleteINode(oldSrc);\n+      }\n+    }\n+    return true;\n+  }\n+\n+  public boolean delete(Path path, boolean recursive) throws IOException {\n+   Path absolutePath = makeAbsolute(path);\n+   INode inode = store.retrieveINode(absolutePath);\n+   if (inode == null) {\n+     return false;\n+   }\n+   if (inode.isFile()) {\n+     store.deleteINode(absolutePath);\n+     for (Block block: inode.getBlocks()) {\n+       store.deleteBlock(block);\n+     }\n+   } else {\n+     FileStatus[] contents = listStatus(absolutePath);\n+     if (contents == null) {\n+       return false;\n+     }\n+     if ((contents.length !=0) && (!recursive)) {\n+       throw new IOException(\"Directory \" + path.toString() \n+           + \" is not empty.\");\n+     }\n+     for (FileStatus p:contents) {\n+       if (!delete(p.getPath(), recursive)) {\n+         return false;\n+       }\n+     }\n+     store.deleteINode(absolutePath);\n+   }\n+   return true;\n+  }\n+  \n+  /**\n+   * FileStatus for S3 file systems. \n+   */\n+  @Override\n+  public FileStatus getFileStatus(Path f)  throws IOException {\n+    INode inode = store.retrieveINode(makeAbsolute(f));\n+    if (inode == null) {\n+      throw new FileNotFoundException(f + \": No such file or directory.\");\n+    }\n+    return new S3FileStatus(f.makeQualified(this), inode);\n+  }\n+\n+  // diagnostic methods\n+\n+  void dump() throws IOException {\n+    store.dump();\n+  }\n+\n+  void purge() throws IOException {\n+    store.purge();\n+  }\n+\n+  private static class S3FileStatus extends FileStatus {\n+\n+    S3FileStatus(Path f, INode inode) throws IOException {\n+      super(findLength(inode), inode.isDirectory(), 1,\n+            findBlocksize(inode), 0, f);\n+    }\n+\n+    private static long findLength(INode inode) {\n+      if (!inode.isDirectory()) {\n+        long length = 0L;\n+        for (Block block : inode.getBlocks()) {\n+          length += block.getLength();\n+        }\n+        return length;\n+      }\n+      return 0;\n+    }\n+\n+    private static long findBlocksize(INode inode) {\n+      final Block[] ret = inode.getBlocks();\n+      return ret == null ? 0L : ret[0].getLength();\n+    }\n+  }\n+}"
        },
        {
            "sha": "f4a5141adbc213918a8b1f9af9fdcae3a6a3f63b",
            "filename": "src/java/org/apache/hadoop/fs/s3/S3FileSystemException.java",
            "status": "added",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3FileSystemException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3FileSystemException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3FileSystemException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,31 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.s3;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Thrown when there is a fatal exception while using {@link S3FileSystem}.\n+ */\n+public class S3FileSystemException extends IOException {\n+  private static final long serialVersionUID = 1L;\n+\n+  public S3FileSystemException(String message) {\n+    super(message);\n+  }\n+}"
        },
        {
            "sha": "db5eded7ad351fc040149c6e0e86b747729e544f",
            "filename": "src/java/org/apache/hadoop/fs/s3/S3InputStream.java",
            "status": "added",
            "additions": 211,
            "deletions": 0,
            "changes": 211,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3InputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3InputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3InputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,211 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3;\n+\n+import java.io.DataInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSInputStream;\n+import org.apache.hadoop.fs.FileSystem;\n+\n+class S3InputStream extends FSInputStream {\n+\n+  private FileSystemStore store;\n+\n+  private Block[] blocks;\n+\n+  private boolean closed;\n+\n+  private long fileLength;\n+\n+  private long pos = 0;\n+\n+  private File blockFile;\n+  \n+  private DataInputStream blockStream;\n+\n+  private long blockEnd = -1;\n+  \n+  private FileSystem.Statistics stats;\n+  \n+  private static final Log LOG = \n+    LogFactory.getLog(S3InputStream.class.getName());\n+\n+\n+  @Deprecated\n+  public S3InputStream(Configuration conf, FileSystemStore store,\n+                       INode inode) {\n+    this(conf, store, inode, null);\n+  }\n+\n+  public S3InputStream(Configuration conf, FileSystemStore store,\n+                       INode inode, FileSystem.Statistics stats) {\n+    \n+    this.store = store;\n+    this.stats = stats;\n+    this.blocks = inode.getBlocks();\n+    for (Block block : blocks) {\n+      this.fileLength += block.getLength();\n+    }\n+  }\n+\n+  @Override\n+  public synchronized long getPos() throws IOException {\n+    return pos;\n+  }\n+\n+  @Override\n+  public synchronized int available() throws IOException {\n+    return (int) (fileLength - pos);\n+  }\n+\n+  @Override\n+  public synchronized void seek(long targetPos) throws IOException {\n+    if (targetPos > fileLength) {\n+      throw new IOException(\"Cannot seek after EOF\");\n+    }\n+    pos = targetPos;\n+    blockEnd = -1;\n+  }\n+\n+  @Override\n+  public synchronized boolean seekToNewSource(long targetPos) throws IOException {\n+    return false;\n+  }\n+\n+  @Override\n+  public synchronized int read() throws IOException {\n+    if (closed) {\n+      throw new IOException(\"Stream closed\");\n+    }\n+    int result = -1;\n+    if (pos < fileLength) {\n+      if (pos > blockEnd) {\n+        blockSeekTo(pos);\n+      }\n+      result = blockStream.read();\n+      if (result >= 0) {\n+        pos++;\n+      }\n+    }\n+    if (stats != null & result >= 0) {\n+      stats.incrementBytesRead(1);\n+    }\n+    return result;\n+  }\n+\n+  @Override\n+  public synchronized int read(byte buf[], int off, int len) throws IOException {\n+    if (closed) {\n+      throw new IOException(\"Stream closed\");\n+    }\n+    if (pos < fileLength) {\n+      if (pos > blockEnd) {\n+        blockSeekTo(pos);\n+      }\n+      int realLen = Math.min(len, (int) (blockEnd - pos + 1));\n+      int result = blockStream.read(buf, off, realLen);\n+      if (result >= 0) {\n+        pos += result;\n+      }\n+      if (stats != null && result > 0) {\n+        stats.incrementBytesRead(result);\n+      }\n+      return result;\n+    }\n+    return -1;\n+  }\n+\n+  private synchronized void blockSeekTo(long target) throws IOException {\n+    //\n+    // Compute desired block\n+    //\n+    int targetBlock = -1;\n+    long targetBlockStart = 0;\n+    long targetBlockEnd = 0;\n+    for (int i = 0; i < blocks.length; i++) {\n+      long blockLength = blocks[i].getLength();\n+      targetBlockEnd = targetBlockStart + blockLength - 1;\n+\n+      if (target >= targetBlockStart && target <= targetBlockEnd) {\n+        targetBlock = i;\n+        break;\n+      } else {\n+        targetBlockStart = targetBlockEnd + 1;\n+      }\n+    }\n+    if (targetBlock < 0) {\n+      throw new IOException(\n+                            \"Impossible situation: could not find target position \" + target);\n+    }\n+    long offsetIntoBlock = target - targetBlockStart;\n+\n+    // read block blocks[targetBlock] from position offsetIntoBlock\n+\n+    this.blockFile = store.retrieveBlock(blocks[targetBlock], offsetIntoBlock);\n+\n+    this.pos = target;\n+    this.blockEnd = targetBlockEnd;\n+    this.blockStream = new DataInputStream(new FileInputStream(blockFile));\n+\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (closed) {\n+      return;\n+    }\n+    if (blockStream != null) {\n+      blockStream.close();\n+      blockStream = null;\n+    }\n+    if (blockFile != null) {\n+      boolean b = blockFile.delete();\n+      if (!b) {\n+        LOG.warn(\"Ignoring failed delete\");\n+      }\n+    }\n+    super.close();\n+    closed = true;\n+  }\n+\n+  /**\n+   * We don't support marks.\n+   */\n+  @Override\n+  public boolean markSupported() {\n+    return false;\n+  }\n+\n+  @Override\n+  public void mark(int readLimit) {\n+    // Do nothing\n+  }\n+\n+  @Override\n+  public void reset() throws IOException {\n+    throw new IOException(\"Mark not supported\");\n+  }\n+\n+}"
        },
        {
            "sha": "f3fee2d5342f2cccb0567ef0cb8b86858dc25001",
            "filename": "src/java/org/apache/hadoop/fs/s3/S3OutputStream.java",
            "status": "added",
            "additions": 231,
            "deletions": 0,
            "changes": 231,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3OutputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3OutputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FS3OutputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,231 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3;\n+\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Random;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.s3.INode.FileType;\n+import org.apache.hadoop.util.Progressable;\n+\n+class S3OutputStream extends OutputStream {\n+\n+  private Configuration conf;\n+  \n+  private int bufferSize;\n+\n+  private FileSystemStore store;\n+\n+  private Path path;\n+\n+  private long blockSize;\n+\n+  private File backupFile;\n+\n+  private OutputStream backupStream;\n+\n+  private Random r = new Random();\n+\n+  private boolean closed;\n+\n+  private int pos = 0;\n+\n+  private long filePos = 0;\n+\n+  private int bytesWrittenToBlock = 0;\n+\n+  private byte[] outBuf;\n+\n+  private List<Block> blocks = new ArrayList<Block>();\n+\n+  private Block nextBlock;\n+  \n+  private static final Log LOG = \n+    LogFactory.getLog(S3OutputStream.class.getName());\n+\n+\n+  public S3OutputStream(Configuration conf, FileSystemStore store,\n+                        Path path, long blockSize, Progressable progress,\n+                        int buffersize) throws IOException {\n+    \n+    this.conf = conf;\n+    this.store = store;\n+    this.path = path;\n+    this.blockSize = blockSize;\n+    this.backupFile = newBackupFile();\n+    this.backupStream = new FileOutputStream(backupFile);\n+    this.bufferSize = buffersize;\n+    this.outBuf = new byte[bufferSize];\n+\n+  }\n+\n+  private File newBackupFile() throws IOException {\n+    File dir = new File(conf.get(\"fs.s3.buffer.dir\"));\n+    if (!dir.exists() && !dir.mkdirs()) {\n+      throw new IOException(\"Cannot create S3 buffer directory: \" + dir);\n+    }\n+    File result = File.createTempFile(\"output-\", \".tmp\", dir);\n+    result.deleteOnExit();\n+    return result;\n+  }\n+\n+  public long getPos() throws IOException {\n+    return filePos;\n+  }\n+\n+  @Override\n+  public synchronized void write(int b) throws IOException {\n+    if (closed) {\n+      throw new IOException(\"Stream closed\");\n+    }\n+\n+    if ((bytesWrittenToBlock + pos == blockSize) || (pos >= bufferSize)) {\n+      flush();\n+    }\n+    outBuf[pos++] = (byte) b;\n+    filePos++;\n+  }\n+\n+  @Override\n+  public synchronized void write(byte b[], int off, int len) throws IOException {\n+    if (closed) {\n+      throw new IOException(\"Stream closed\");\n+    }\n+    while (len > 0) {\n+      int remaining = bufferSize - pos;\n+      int toWrite = Math.min(remaining, len);\n+      System.arraycopy(b, off, outBuf, pos, toWrite);\n+      pos += toWrite;\n+      off += toWrite;\n+      len -= toWrite;\n+      filePos += toWrite;\n+\n+      if ((bytesWrittenToBlock + pos >= blockSize) || (pos == bufferSize)) {\n+        flush();\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public synchronized void flush() throws IOException {\n+    if (closed) {\n+      throw new IOException(\"Stream closed\");\n+    }\n+\n+    if (bytesWrittenToBlock + pos >= blockSize) {\n+      flushData((int) blockSize - bytesWrittenToBlock);\n+    }\n+    if (bytesWrittenToBlock == blockSize) {\n+      endBlock();\n+    }\n+    flushData(pos);\n+  }\n+\n+  private synchronized void flushData(int maxPos) throws IOException {\n+    int workingPos = Math.min(pos, maxPos);\n+\n+    if (workingPos > 0) {\n+      //\n+      // To the local block backup, write just the bytes\n+      //\n+      backupStream.write(outBuf, 0, workingPos);\n+\n+      //\n+      // Track position\n+      //\n+      bytesWrittenToBlock += workingPos;\n+      System.arraycopy(outBuf, workingPos, outBuf, 0, pos - workingPos);\n+      pos -= workingPos;\n+    }\n+  }\n+\n+  private synchronized void endBlock() throws IOException {\n+    //\n+    // Done with local copy\n+    //\n+    backupStream.close();\n+\n+    //\n+    // Send it to S3\n+    //\n+    // TODO: Use passed in Progressable to report progress.\n+    nextBlockOutputStream();\n+    store.storeBlock(nextBlock, backupFile);\n+    internalClose();\n+\n+    //\n+    // Delete local backup, start new one\n+    //\n+    boolean b = backupFile.delete();\n+    if (!b) {\n+      LOG.warn(\"Ignoring failed delete\");\n+    }\n+    backupFile = newBackupFile();\n+    backupStream = new FileOutputStream(backupFile);\n+    bytesWrittenToBlock = 0;\n+  }\n+\n+  private synchronized void nextBlockOutputStream() throws IOException {\n+    long blockId = r.nextLong();\n+    while (store.blockExists(blockId)) {\n+      blockId = r.nextLong();\n+    }\n+    nextBlock = new Block(blockId, bytesWrittenToBlock);\n+    blocks.add(nextBlock);\n+    bytesWrittenToBlock = 0;\n+  }\n+\n+  private synchronized void internalClose() throws IOException {\n+    INode inode = new INode(FileType.FILE, blocks.toArray(new Block[blocks\n+                                                                    .size()]));\n+    store.storeINode(path, inode);\n+  }\n+\n+  @Override\n+  public synchronized void close() throws IOException {\n+    if (closed) {\n+      return;\n+    }\n+\n+    flush();\n+    if (filePos == 0 || bytesWrittenToBlock != 0) {\n+      endBlock();\n+    }\n+\n+    backupStream.close();\n+    boolean b = backupFile.delete();\n+    if (!b) {\n+      LOG.warn(\"Ignoring failed delete\");\n+    }\n+\n+    super.close();\n+\n+    closed = true;\n+  }\n+\n+}"
        },
        {
            "sha": "22c6d67f777a65f7996896d9c66c140446f68356",
            "filename": "src/java/org/apache/hadoop/fs/s3/VersionMismatchException.java",
            "status": "added",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FVersionMismatchException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FVersionMismatchException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2FVersionMismatchException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,32 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.s3;\n+\n+/**\n+ * Thrown when Hadoop cannot read the version of the data stored\n+ * in {@link S3FileSystem}.\n+ */\n+public class VersionMismatchException extends S3FileSystemException {\n+  private static final long serialVersionUID = 1L;\n+\n+  public VersionMismatchException(String clientVersion, String dataVersion) {\n+    super(\"Version mismatch: client expects version \" + clientVersion +\n+        \", but data has version \" +\n+        (dataVersion == null ? \"[unversioned]\" : dataVersion));\n+  }\n+}"
        },
        {
            "sha": "dd601e104e52a5b4bd36b255a90a0e84f5fa3eb8",
            "filename": "src/java/org/apache/hadoop/fs/s3/package.html",
            "status": "added",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,55 @@\n+<html>\n+\n+<!--\n+   Licensed to the Apache Software Foundation (ASF) under one or more\n+   contributor license agreements.  See the NOTICE file distributed with\n+   this work for additional information regarding copyright ownership.\n+   The ASF licenses this file to You under the Apache License, Version 2.0\n+   (the \"License\"); you may not use this file except in compliance with\n+   the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+-->\n+\n+<body>\n+\n+<p>A distributed, block-based implementation of {@link\n+org.apache.hadoop.fs.FileSystem} that uses <a href=\"http://aws.amazon.com/s3\">Amazon S3</a>\n+as a backing store.</p>\n+\n+<p>\n+Files are stored in S3 as blocks (represented by \n+{@link org.apache.hadoop.fs.s3.Block}), which have an ID and a length.\n+Block metadata is stored in S3 as a small record (represented by \n+{@link org.apache.hadoop.fs.s3.INode}) using the URL-encoded\n+path string as a key. Inodes record the file type (regular file or directory) and the list of blocks.\n+This design makes it easy to seek to any given position in a file by reading the inode data to compute\n+which block to access, then using S3's support for \n+<a href=\"http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.35.2\">HTTP Range</a> headers\n+to start streaming from the correct position.\n+Renames are also efficient since only the inode is moved (by a DELETE followed by a PUT since \n+S3 does not support renames).\n+</p>\n+<p>\n+For a single file <i>/dir1/file1</i> which takes two blocks of storage, the file structure in S3\n+would be something like this:\n+</p>\n+<pre>\n+/\n+/dir1\n+/dir1/file1\n+block-6415776850131549260\n+block-3026438247347758425\n+</pre>\n+<p>\n+Inodes start with a leading <code>/</code>, while blocks are prefixed with <code>block-</code>.\n+</p>\n+\n+</body>\n+</html>"
        },
        {
            "sha": "23797e81c0dc084c92a0063ca1a5afcfdb1bae2c",
            "filename": "src/java/org/apache/hadoop/fs/s3native/FileMetadata.java",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FFileMetadata.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FFileMetadata.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FFileMetadata.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,54 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3native;\n+\n+/**\n+ * <p>\n+ * Holds basic metadata for a file stored in a {@link NativeFileSystemStore}.\n+ * </p>\n+ */\n+class FileMetadata {\n+  private final String key;\n+  private final long length;\n+  private final long lastModified;\n+  \n+  public FileMetadata(String key, long length, long lastModified) {\n+    this.key = key;\n+    this.length = length;\n+    this.lastModified = lastModified;\n+  }\n+  \n+  public String getKey() {\n+    return key;\n+  }\n+  \n+  public long getLength() {\n+    return length;\n+  }\n+\n+  public long getLastModified() {\n+    return lastModified;\n+  }\n+  \n+  @Override\n+  public String toString() {\n+    return \"FileMetadata[\" + key + \", \" + length + \", \" + lastModified + \"]\";\n+  }\n+  \n+}"
        },
        {
            "sha": "b24a8e06b7c468f216f5c8cf905a2b8797d46386",
            "filename": "src/java/org/apache/hadoop/fs/s3native/Jets3tNativeFileSystemStore.java",
            "status": "added",
            "additions": 255,
            "deletions": 0,
            "changes": 255,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FJets3tNativeFileSystemStore.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FJets3tNativeFileSystemStore.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FJets3tNativeFileSystemStore.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,255 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3native;\n+\n+import static org.apache.hadoop.fs.s3native.NativeS3FileSystem.PATH_DELIMITER;\n+\n+import java.io.BufferedInputStream;\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.s3.S3Credentials;\n+import org.apache.hadoop.fs.s3.S3Exception;\n+import org.jets3t.service.S3ObjectsChunk;\n+import org.jets3t.service.S3Service;\n+import org.jets3t.service.S3ServiceException;\n+import org.jets3t.service.impl.rest.httpclient.RestS3Service;\n+import org.jets3t.service.model.S3Bucket;\n+import org.jets3t.service.model.S3Object;\n+import org.jets3t.service.security.AWSCredentials;\n+\n+class Jets3tNativeFileSystemStore implements NativeFileSystemStore {\n+  \n+  private S3Service s3Service;\n+  private S3Bucket bucket;\n+  \n+  public void initialize(URI uri, Configuration conf) throws IOException {\n+    S3Credentials s3Credentials = new S3Credentials();\n+    s3Credentials.initialize(uri, conf);\n+    try {\n+      AWSCredentials awsCredentials =\n+        new AWSCredentials(s3Credentials.getAccessKey(),\n+            s3Credentials.getSecretAccessKey());\n+      this.s3Service = new RestS3Service(awsCredentials);\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+    bucket = new S3Bucket(uri.getHost());\n+  }\n+  \n+  public void storeFile(String key, File file, byte[] md5Hash)\n+    throws IOException {\n+    \n+    BufferedInputStream in = null;\n+    try {\n+      in = new BufferedInputStream(new FileInputStream(file));\n+      S3Object object = new S3Object(key);\n+      object.setDataInputStream(in);\n+      object.setContentType(\"binary/octet-stream\");\n+      object.setContentLength(file.length());\n+      if (md5Hash != null) {\n+        object.setMd5Hash(md5Hash);\n+      }\n+      s3Service.putObject(bucket, object);\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    } finally {\n+      if (in != null) {\n+        try {\n+          in.close();\n+        } catch (IOException e) {\n+          // ignore\n+        }\n+      }\n+    }\n+  }\n+\n+  public void storeEmptyFile(String key) throws IOException {\n+    try {\n+      S3Object object = new S3Object(key);\n+      object.setDataInputStream(new ByteArrayInputStream(new byte[0]));\n+      object.setContentType(\"binary/octet-stream\");\n+      object.setContentLength(0);\n+      s3Service.putObject(bucket, object);\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+  \n+  public FileMetadata retrieveMetadata(String key) throws IOException {\n+    try {\n+      S3Object object = s3Service.getObjectDetails(bucket, key);\n+      return new FileMetadata(key, object.getContentLength(),\n+          object.getLastModifiedDate().getTime());\n+    } catch (S3ServiceException e) {\n+      // Following is brittle. Is there a better way?\n+      if (e.getMessage().contains(\"ResponseCode=404\")) {\n+        return null;\n+      }\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+  \n+  public InputStream retrieve(String key) throws IOException {\n+    try {\n+      S3Object object = s3Service.getObject(bucket, key);\n+      return object.getDataInputStream();\n+    } catch (S3ServiceException e) {\n+      if (\"NoSuchKey\".equals(e.getS3ErrorCode())) {\n+        return null;\n+      }\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+  \n+  public InputStream retrieve(String key, long byteRangeStart)\n+    throws IOException {\n+    try {\n+      S3Object object = s3Service.getObject(bucket, key, null, null, null,\n+                                            null, byteRangeStart, null);\n+      return object.getDataInputStream();\n+    } catch (S3ServiceException e) {\n+      if (\"NoSuchKey\".equals(e.getS3ErrorCode())) {\n+        return null;\n+      }\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+\n+  public PartialListing list(String prefix, int maxListingLength)\n+    throws IOException {\n+    return list(prefix, maxListingLength, null);\n+  }\n+  \n+  public PartialListing list(String prefix, int maxListingLength,\n+      String priorLastKey) throws IOException {\n+\n+    return list(prefix, PATH_DELIMITER, maxListingLength, priorLastKey);\n+  }\n+\n+  public PartialListing listAll(String prefix, int maxListingLength,\n+      String priorLastKey) throws IOException {\n+\n+    return list(prefix, null, maxListingLength, priorLastKey);\n+  }\n+\n+  private PartialListing list(String prefix, String delimiter,\n+      int maxListingLength, String priorLastKey) throws IOException {\n+    try {\n+      if (prefix.length() > 0 && !prefix.endsWith(PATH_DELIMITER)) {\n+        prefix += PATH_DELIMITER;\n+      }\n+      S3ObjectsChunk chunk = s3Service.listObjectsChunked(bucket.getName(),\n+          prefix, delimiter, maxListingLength, priorLastKey);\n+      \n+      FileMetadata[] fileMetadata =\n+        new FileMetadata[chunk.getObjects().length];\n+      for (int i = 0; i < fileMetadata.length; i++) {\n+        S3Object object = chunk.getObjects()[i];\n+        fileMetadata[i] = new FileMetadata(object.getKey(),\n+            object.getContentLength(), object.getLastModifiedDate().getTime());\n+      }\n+      return new PartialListing(chunk.getPriorLastKey(), fileMetadata,\n+          chunk.getCommonPrefixes());\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+\n+  public void delete(String key) throws IOException {\n+    try {\n+      s3Service.deleteObject(bucket, key);\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+  \n+  public void rename(String srcKey, String dstKey) throws IOException {\n+    try {\n+      s3Service.moveObject(bucket.getName(), srcKey, bucket.getName(),\n+          new S3Object(dstKey), false);\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+\n+  public void purge(String prefix) throws IOException {\n+    try {\n+      S3Object[] objects = s3Service.listObjects(bucket, prefix, null);\n+      for (int i = 0; i < objects.length; i++) {\n+        s3Service.deleteObject(bucket, objects[i].getKey());\n+      }\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+  }\n+\n+  public void dump() throws IOException {\n+    StringBuilder sb = new StringBuilder(\"S3 Native Filesystem, \");\n+    sb.append(bucket.getName()).append(\"\\n\");\n+    try {\n+      S3Object[] objects = s3Service.listObjects(bucket);\n+      for (int i = 0; i < objects.length; i++) {\n+        sb.append(objects[i].getKey()).append(\"\\n\");\n+      }\n+    } catch (S3ServiceException e) {\n+      if (e.getCause() instanceof IOException) {\n+        throw (IOException) e.getCause();\n+      }\n+      throw new S3Exception(e);\n+    }\n+    System.out.println(sb);\n+  }\n+  \n+}"
        },
        {
            "sha": "eb0a682486980e78be11810324ced4ee359c51d1",
            "filename": "src/java/org/apache/hadoop/fs/s3native/NativeFileSystemStore.java",
            "status": "added",
            "additions": 65,
            "deletions": 0,
            "changes": 65,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FNativeFileSystemStore.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FNativeFileSystemStore.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FNativeFileSystemStore.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,65 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3native;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.net.URI;\n+\n+import org.apache.hadoop.conf.Configuration;\n+\n+/**\n+ * <p>\n+ * An abstraction for a key-based {@link File} store.\n+ * </p>\n+ */\n+interface NativeFileSystemStore {\n+  \n+  void initialize(URI uri, Configuration conf) throws IOException;\n+  \n+  void storeFile(String key, File file, byte[] md5Hash) throws IOException;\n+  void storeEmptyFile(String key) throws IOException;\n+  \n+  FileMetadata retrieveMetadata(String key) throws IOException;\n+  InputStream retrieve(String key) throws IOException;\n+  InputStream retrieve(String key, long byteRangeStart) throws IOException;\n+  \n+  PartialListing list(String prefix, int maxListingLength) throws IOException;\n+  PartialListing list(String prefix, int maxListingLength, String priorLastKey)\n+    throws IOException;\n+  PartialListing listAll(String prefix, int maxListingLength,\n+      String priorLastKey) throws IOException;\n+  \n+  void delete(String key) throws IOException;\n+\n+  void rename(String srcKey, String dstKey) throws IOException;\n+  \n+  /**\n+   * Delete all keys with the given prefix. Used for testing.\n+   * @throws IOException\n+   */\n+  void purge(String prefix) throws IOException;\n+  \n+  /**\n+   * Diagnostic method to dump state to the console.\n+   * @throws IOException\n+   */\n+  void dump() throws IOException;\n+}"
        },
        {
            "sha": "7ec60655dd9c0674cf379ee631be37c8d4aa017f",
            "filename": "src/java/org/apache/hadoop/fs/s3native/NativeS3FileSystem.java",
            "status": "added",
            "additions": 578,
            "deletions": 0,
            "changes": 578,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FNativeS3FileSystem.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FNativeS3FileSystem.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FNativeS3FileSystem.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,578 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3native;\n+\n+import java.io.BufferedOutputStream;\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.net.URI;\n+import java.security.DigestOutputStream;\n+import java.security.MessageDigest;\n+import java.security.NoSuchAlgorithmException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.TreeSet;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BufferedFSInputStream;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FSInputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.fs.s3.S3Exception;\n+import org.apache.hadoop.io.retry.RetryPolicies;\n+import org.apache.hadoop.io.retry.RetryPolicy;\n+import org.apache.hadoop.io.retry.RetryProxy;\n+import org.apache.hadoop.util.Progressable;\n+\n+/**\n+ * <p>\n+ * A {@link FileSystem} for reading and writing files stored on\n+ * <a href=\"http://aws.amazon.com/s3\">Amazon S3</a>.\n+ * Unlike {@link org.apache.hadoop.fs.s3.S3FileSystem} this implementation\n+ * stores files on S3 in their\n+ * native form so they can be read by other S3 tools.\n+ * </p>\n+ * @see org.apache.hadoop.fs.s3.S3FileSystem\n+ */\n+public class NativeS3FileSystem extends FileSystem {\n+  \n+  public static final Log LOG = \n+    LogFactory.getLog(NativeS3FileSystem.class);\n+  \n+  private static final String FOLDER_SUFFIX = \"_$folder$\";\n+  private static final long MAX_S3_FILE_SIZE = 5 * 1024 * 1024 * 1024L;\n+  static final String PATH_DELIMITER = Path.SEPARATOR;\n+  private static final int S3_MAX_LISTING_LENGTH = 1000;\n+  \n+  private class NativeS3FsInputStream extends FSInputStream {\n+    \n+    private InputStream in;\n+    private final String key;\n+    private long pos = 0;\n+    \n+    public NativeS3FsInputStream(InputStream in, String key) {\n+      this.in = in;\n+      this.key = key;\n+    }\n+    \n+    public synchronized int read() throws IOException {\n+      int result = in.read();\n+      if (result != -1) {\n+        pos++;\n+      }\n+      if (statistics != null && result != -1) {\n+        statistics.incrementBytesRead(1);\n+      }\n+      return result;\n+    }\n+    public synchronized int read(byte[] b, int off, int len)\n+      throws IOException {\n+      \n+      int result = in.read(b, off, len);\n+      if (result > 0) {\n+        pos += result;\n+      }\n+      if (statistics != null && result > 0) {\n+        statistics.incrementBytesRead(result);\n+      }\n+      return result;\n+    }\n+\n+    public void close() throws IOException {\n+      in.close();\n+    }\n+\n+    public synchronized void seek(long pos) throws IOException {\n+      in.close();\n+      in = store.retrieve(key, pos);\n+      this.pos = pos;\n+    }\n+    public synchronized long getPos() throws IOException {\n+      return pos;\n+    }\n+    public boolean seekToNewSource(long targetPos) throws IOException {\n+      return false;\n+    }\n+  }\n+  \n+  private class NativeS3FsOutputStream extends OutputStream {\n+    \n+    private Configuration conf;\n+    private String key;\n+    private File backupFile;\n+    private OutputStream backupStream;\n+    private MessageDigest digest;\n+    private boolean closed;\n+    \n+    public NativeS3FsOutputStream(Configuration conf,\n+        NativeFileSystemStore store, String key, Progressable progress,\n+        int bufferSize) throws IOException {\n+      this.conf = conf;\n+      this.key = key;\n+      this.backupFile = newBackupFile();\n+      try {\n+        this.digest = MessageDigest.getInstance(\"MD5\");\n+        this.backupStream = new BufferedOutputStream(new DigestOutputStream(\n+            new FileOutputStream(backupFile), this.digest));\n+      } catch (NoSuchAlgorithmException e) {\n+        LOG.warn(\"Cannot load MD5 digest algorithm,\" +\n+            \"skipping message integrity check.\", e);\n+        this.backupStream = new BufferedOutputStream(\n+            new FileOutputStream(backupFile));\n+      }\n+    }\n+\n+    private File newBackupFile() throws IOException {\n+      File dir = new File(conf.get(\"fs.s3.buffer.dir\"));\n+      if (!dir.mkdirs() && !dir.exists()) {\n+        throw new IOException(\"Cannot create S3 buffer directory: \" + dir);\n+      }\n+      File result = File.createTempFile(\"output-\", \".tmp\", dir);\n+      result.deleteOnExit();\n+      return result;\n+    }\n+    \n+    @Override\n+    public void flush() throws IOException {\n+      backupStream.flush();\n+    }\n+    \n+    @Override\n+    public synchronized void close() throws IOException {\n+      if (closed) {\n+        return;\n+      }\n+\n+      backupStream.close();\n+      \n+      try {\n+        byte[] md5Hash = digest == null ? null : digest.digest();\n+        store.storeFile(key, backupFile, md5Hash);\n+      } finally {\n+        if (!backupFile.delete()) {\n+          LOG.warn(\"Could not delete temporary s3n file: \" + backupFile);\n+        }\n+        super.close();\n+        closed = true;\n+      } \n+\n+    }\n+\n+    @Override\n+    public void write(int b) throws IOException {\n+      backupStream.write(b);\n+    }\n+\n+    @Override\n+    public void write(byte[] b, int off, int len) throws IOException {\n+      backupStream.write(b, off, len);\n+    }\n+    \n+    \n+  }\n+  \n+  private URI uri;\n+  private NativeFileSystemStore store;\n+  private Path workingDir;\n+  \n+  public NativeS3FileSystem() {\n+    // set store in initialize()\n+  }\n+  \n+  public NativeS3FileSystem(NativeFileSystemStore store) {\n+    this.store = store;\n+  }\n+  \n+  @Override\n+  public void initialize(URI uri, Configuration conf) throws IOException {\n+    super.initialize(uri, conf);\n+    if (store == null) {\n+      store = createDefaultStore(conf);\n+    }\n+    store.initialize(uri, conf);\n+    setConf(conf);\n+    this.uri = URI.create(uri.getScheme() + \"://\" + uri.getAuthority());\n+    this.workingDir =\n+      new Path(\"/user\", System.getProperty(\"user.name\")).makeQualified(this);\n+  }\n+  \n+  private static NativeFileSystemStore createDefaultStore(Configuration conf) {\n+    NativeFileSystemStore store = new Jets3tNativeFileSystemStore();\n+    \n+    RetryPolicy basePolicy = RetryPolicies.retryUpToMaximumCountWithFixedSleep(\n+        conf.getInt(\"fs.s3.maxRetries\", 4),\n+        conf.getLong(\"fs.s3.sleepTimeSeconds\", 10), TimeUnit.SECONDS);\n+    Map<Class<? extends Exception>, RetryPolicy> exceptionToPolicyMap =\n+      new HashMap<Class<? extends Exception>, RetryPolicy>();\n+    exceptionToPolicyMap.put(IOException.class, basePolicy);\n+    exceptionToPolicyMap.put(S3Exception.class, basePolicy);\n+    \n+    RetryPolicy methodPolicy = RetryPolicies.retryByException(\n+        RetryPolicies.TRY_ONCE_THEN_FAIL, exceptionToPolicyMap);\n+    Map<String, RetryPolicy> methodNameToPolicyMap =\n+      new HashMap<String, RetryPolicy>();\n+    methodNameToPolicyMap.put(\"storeFile\", methodPolicy);\n+    \n+    return (NativeFileSystemStore)\n+      RetryProxy.create(NativeFileSystemStore.class, store,\n+          methodNameToPolicyMap);\n+  }\n+  \n+  private static String pathToKey(Path path) {\n+    if (!path.isAbsolute()) {\n+      throw new IllegalArgumentException(\"Path must be absolute: \" + path);\n+    }\n+    return path.toUri().getPath().substring(1); // remove initial slash\n+  }\n+  \n+  private static Path keyToPath(String key) {\n+    return new Path(\"/\" + key);\n+  }\n+  \n+  private Path makeAbsolute(Path path) {\n+    if (path.isAbsolute()) {\n+      return path;\n+    }\n+    return new Path(workingDir, path);\n+  }\n+\n+  /** This optional operation is not yet supported. */\n+  public FSDataOutputStream append(Path f, int bufferSize,\n+      Progressable progress) throws IOException {\n+    throw new IOException(\"Not supported\");\n+  }\n+  \n+  @Override\n+  public FSDataOutputStream create(Path f, FsPermission permission,\n+      boolean overwrite, int bufferSize, short replication, long blockSize,\n+      Progressable progress) throws IOException {\n+\n+    if (exists(f) && !overwrite) {\n+      throw new IOException(\"File already exists:\"+f);\n+    }\n+    Path absolutePath = makeAbsolute(f);\n+    String key = pathToKey(absolutePath);\n+    return new FSDataOutputStream(new NativeS3FsOutputStream(getConf(), store,\n+        key, progress, bufferSize), statistics);\n+  }\n+  \n+  @Override\n+  public boolean delete(Path f, boolean recursive) throws IOException {\n+    FileStatus status;\n+    try {\n+      status = getFileStatus(f);\n+    } catch (FileNotFoundException e) {\n+      return false;\n+    }\n+    Path absolutePath = makeAbsolute(f);\n+    String key = pathToKey(absolutePath);\n+    if (status.isDir()) {\n+      FileStatus[] contents = listStatus(f);\n+      if (!recursive && contents.length > 0) {\n+        throw new IOException(\"Directory \" + f.toString() + \" is not empty.\");\n+      }\n+      for (FileStatus p : contents) {\n+        if (!delete(p.getPath(), recursive)) {\n+          return false;\n+        }\n+      }\n+      store.delete(key + FOLDER_SUFFIX);\n+    } else {\n+      store.delete(key);\n+    }\n+    return true;\n+  }\n+\n+  @Override\n+  public FileStatus getFileStatus(Path f) throws IOException {\n+    \n+    Path absolutePath = makeAbsolute(f);\n+    String key = pathToKey(absolutePath);\n+    \n+    if (key.length() == 0) { // root always exists\n+      return newDirectory(absolutePath);\n+    }\n+    \n+    FileMetadata meta = store.retrieveMetadata(key);\n+    if (meta != null) {\n+      return newFile(meta, absolutePath);\n+    }\n+    if (store.retrieveMetadata(key + FOLDER_SUFFIX) != null) {\n+      return newDirectory(absolutePath);\n+    }\n+    \n+    PartialListing listing = store.list(key, 1);\n+    if (listing.getFiles().length > 0 ||\n+        listing.getCommonPrefixes().length > 0) {\n+      return newDirectory(absolutePath);\n+    }\n+    \n+    throw new FileNotFoundException(absolutePath +\n+        \": No such file or directory.\");\n+    \n+  }\n+\n+  @Override\n+  public URI getUri() {\n+    return uri;\n+  }\n+\n+  /**\n+   * <p>\n+   * If <code>f</code> is a file, this method will make a single call to S3.\n+   * If <code>f</code> is a directory, this method will make a maximum of\n+   * (<i>n</i> / 1000) + 2 calls to S3, where <i>n</i> is the total number of\n+   * files and directories contained directly in <code>f</code>.\n+   * </p>\n+   */\n+  @Override\n+  public FileStatus[] listStatus(Path f) throws IOException {\n+\n+    Path absolutePath = makeAbsolute(f);\n+    String key = pathToKey(absolutePath);\n+    \n+    if (key.length() > 0) {\n+      FileMetadata meta = store.retrieveMetadata(key);\n+      if (meta != null) {\n+        return new FileStatus[] { newFile(meta, absolutePath) };\n+      }\n+    }\n+    \n+    URI pathUri = absolutePath.toUri();\n+    Set<FileStatus> status = new TreeSet<FileStatus>();\n+    String priorLastKey = null;\n+    do {\n+      PartialListing listing = store.list(key, S3_MAX_LISTING_LENGTH, \n+          priorLastKey);\n+      for (FileMetadata fileMetadata : listing.getFiles()) {\n+        Path subpath = keyToPath(fileMetadata.getKey());\n+        String relativePath = pathUri.relativize(subpath.toUri()).getPath();\n+        if (relativePath.endsWith(FOLDER_SUFFIX)) {\n+          status.add(newDirectory(new Path(absolutePath,\n+              relativePath.substring(0,\n+                  relativePath.indexOf(FOLDER_SUFFIX)))));\n+        } else {\n+          status.add(newFile(fileMetadata, subpath));\n+        }\n+      }\n+      for (String commonPrefix : listing.getCommonPrefixes()) {\n+        Path subpath = keyToPath(commonPrefix);\n+        String relativePath = pathUri.relativize(subpath.toUri()).getPath();\n+        status.add(newDirectory(new Path(absolutePath, relativePath)));\n+      }\n+      priorLastKey = listing.getPriorLastKey();\n+    } while (priorLastKey != null);\n+    \n+    if (status.isEmpty() &&\n+        store.retrieveMetadata(key + FOLDER_SUFFIX) == null) {\n+      return null;\n+    }\n+    \n+    return status.toArray(new FileStatus[0]);\n+  }\n+  \n+  private FileStatus newFile(FileMetadata meta, Path path) {\n+    return new FileStatus(meta.getLength(), false, 1, MAX_S3_FILE_SIZE,\n+        meta.getLastModified(), path.makeQualified(this));\n+  }\n+  \n+  private FileStatus newDirectory(Path path) {\n+    return new FileStatus(0, true, 1, MAX_S3_FILE_SIZE, 0,\n+        path.makeQualified(this));\n+  }\n+\n+  @Override\n+  public boolean mkdirs(Path f, FsPermission permission) throws IOException {\n+    Path absolutePath = makeAbsolute(f);\n+    List<Path> paths = new ArrayList<Path>();\n+    do {\n+      paths.add(0, absolutePath);\n+      absolutePath = absolutePath.getParent();\n+    } while (absolutePath != null);\n+    \n+    boolean result = true;\n+    for (Path path : paths) {\n+      result &= mkdir(path);\n+    }\n+    return result;\n+  }\n+  \n+  private boolean mkdir(Path f) throws IOException {\n+    try {\n+      FileStatus fileStatus = getFileStatus(f);\n+      if (!fileStatus.isDir()) {\n+        throw new IOException(String.format(\n+            \"Can't make directory for path %s since it is a file.\", f));\n+\n+      }\n+    } catch (FileNotFoundException e) {\n+      String key = pathToKey(f) + FOLDER_SUFFIX;\n+      store.storeEmptyFile(key);    \n+    }\n+    return true;\n+  }\n+\n+  @Override\n+  public FSDataInputStream open(Path f, int bufferSize) throws IOException {\n+    if (!exists(f)) {\n+      throw new FileNotFoundException(f.toString());\n+    }\n+    Path absolutePath = makeAbsolute(f);\n+    String key = pathToKey(absolutePath);\n+    return new FSDataInputStream(new BufferedFSInputStream(\n+        new NativeS3FsInputStream(store.retrieve(key), key), bufferSize));\n+  }\n+  \n+  // rename() and delete() use this method to ensure that the parent directory\n+  // of the source does not vanish.\n+  private void createParent(Path path) throws IOException {\n+      Path parent = path.getParent();\n+      if (parent != null) {\n+          String key = pathToKey(makeAbsolute(parent));\n+          if (key.length() > 0) {\n+              store.storeEmptyFile(key + FOLDER_SUFFIX);\n+          }\n+      }\n+  }\n+  \n+  private boolean existsAndIsFile(Path f) throws IOException {\n+    \n+    Path absolutePath = makeAbsolute(f);\n+    String key = pathToKey(absolutePath);\n+    \n+    if (key.length() == 0) {\n+        return false;\n+    }\n+    \n+    FileMetadata meta = store.retrieveMetadata(key);\n+    if (meta != null) {\n+        // S3 object with given key exists, so this is a file\n+        return true;\n+    }\n+    \n+    if (store.retrieveMetadata(key + FOLDER_SUFFIX) != null) {\n+        // Signifies empty directory\n+        return false;\n+    }\n+    \n+    PartialListing listing = store.list(key, 1, null);\n+    if (listing.getFiles().length > 0 ||\n+        listing.getCommonPrefixes().length > 0) {\n+        // Non-empty directory\n+        return false;\n+    }\n+    \n+    throw new FileNotFoundException(absolutePath +\n+        \": No such file or directory\");\n+}\n+\n+\n+  @Override\n+  public boolean rename(Path src, Path dst) throws IOException {\n+\n+    String srcKey = pathToKey(makeAbsolute(src));\n+\n+    if (srcKey.length() == 0) {\n+      // Cannot rename root of file system\n+      return false;\n+    }\n+\n+    // Figure out the final destination\n+    String dstKey;\n+    try {\n+      boolean dstIsFile = existsAndIsFile(dst);\n+      if (dstIsFile) {\n+        // Attempting to overwrite a file using rename()\n+        return false;\n+      } else {\n+        // Move to within the existent directory\n+        dstKey = pathToKey(makeAbsolute(new Path(dst, src.getName())));\n+      }\n+    } catch (FileNotFoundException e) {\n+      // dst doesn't exist, so we can proceed\n+      dstKey = pathToKey(makeAbsolute(dst));\n+      try {\n+        if (!getFileStatus(dst.getParent()).isDir()) {\n+          return false; // parent dst is a file\n+        }\n+      } catch (FileNotFoundException ex) {\n+        return false; // parent dst does not exist\n+      }\n+    }\n+\n+    try {\n+      boolean srcIsFile = existsAndIsFile(src);\n+      if (srcIsFile) {\n+        store.rename(srcKey, dstKey);\n+      } else {\n+        // Move the folder object\n+        store.delete(srcKey + FOLDER_SUFFIX);\n+        store.storeEmptyFile(dstKey + FOLDER_SUFFIX);\n+\n+        // Move everything inside the folder\n+        String priorLastKey = null;\n+        do {\n+          PartialListing listing = store.listAll(srcKey, S3_MAX_LISTING_LENGTH,\n+              priorLastKey);\n+          for (FileMetadata file : listing.getFiles()) {\n+            store.rename(file.getKey(), dstKey\n+                + file.getKey().substring(srcKey.length()));\n+          }\n+          priorLastKey = listing.getPriorLastKey();\n+        } while (priorLastKey != null);\n+      }\n+\n+      createParent(src);\n+      return true;\n+\n+    } catch (FileNotFoundException e) {\n+      // Source file does not exist;\n+      return false;\n+    }\n+  }\n+\n+\n+  /**\n+   * Set the working directory to the given directory.\n+   */\n+  @Override\n+  public void setWorkingDirectory(Path newDir) {\n+    workingDir = newDir;\n+  }\n+  \n+  @Override\n+  public Path getWorkingDirectory() {\n+    return workingDir;\n+  }\n+\n+}"
        },
        {
            "sha": "899758660d21829ca5a020b3ee322c1a512169a3",
            "filename": "src/java/org/apache/hadoop/fs/s3native/PartialListing.java",
            "status": "added",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FPartialListing.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FPartialListing.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2FPartialListing.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,59 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.fs.s3native;\n+\n+/**\n+ * <p>\n+ * Holds information on a directory listing for a\n+ * {@link NativeFileSystemStore}.\n+ * This includes the {@link FileMetadata files} and directories\n+ * (their names) contained in a directory.\n+ * </p>\n+ * <p>\n+ * This listing may be returned in chunks, so a <code>priorLastKey</code>\n+ * is provided so that the next chunk may be requested.\n+ * </p>\n+ * @see NativeFileSystemStore#list(String, int, String)\n+ */\n+class PartialListing {\n+  \n+  private final String priorLastKey;\n+  private final FileMetadata[] files;\n+  private final String[] commonPrefixes;\n+  \n+  public PartialListing(String priorLastKey, FileMetadata[] files,\n+      String[] commonPrefixes) {\n+    this.priorLastKey = priorLastKey;\n+    this.files = files;\n+    this.commonPrefixes = commonPrefixes;\n+  }\n+\n+  public FileMetadata[] getFiles() {\n+    return files;\n+  }\n+\n+  public String[] getCommonPrefixes() {\n+    return commonPrefixes;\n+  }\n+\n+  public String getPriorLastKey() {\n+    return priorLastKey;\n+  }\n+  \n+}"
        },
        {
            "sha": "24b9b1df460c4f43f1d2cdd1571d8d8a4e94e31d",
            "filename": "src/java/org/apache/hadoop/fs/s3native/package.html",
            "status": "added",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fs3native%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,32 @@\n+<html>\n+\n+<!--\n+   Licensed to the Apache Software Foundation (ASF) under one or more\n+   contributor license agreements.  See the NOTICE file distributed with\n+   this work for additional information regarding copyright ownership.\n+   The ASF licenses this file to You under the Apache License, Version 2.0\n+   (the \"License\"); you may not use this file except in compliance with\n+   the License.  You may obtain a copy of the License at\n+\n+       http://www.apache.org/licenses/LICENSE-2.0\n+\n+   Unless required by applicable law or agreed to in writing, software\n+   distributed under the License is distributed on an \"AS IS\" BASIS,\n+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+   See the License for the specific language governing permissions and\n+   limitations under the License.\n+-->\n+\n+<body>\n+\n+<p>\n+A distributed implementation of {@link\n+org.apache.hadoop.fs.FileSystem} for reading and writing files on\n+<a href=\"http://aws.amazon.com/s3\">Amazon S3</a>.\n+Unlike {@link org.apache.hadoop.fs.s3.S3FileSystem}, which is block-based,\n+this implementation stores\n+files on S3 in their native form for interoperability with other S3 tools.\n+</p>\n+\n+</body>\n+</html>"
        },
        {
            "sha": "06883a2086f0f36493a6ab1c4891d862f54c9ee3",
            "filename": "src/java/org/apache/hadoop/fs/shell/Command.java",
            "status": "added",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fshell%2FCommand.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fshell%2FCommand.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fshell%2FCommand.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,86 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.shell;\n+\n+import java.io.*;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.ipc.RemoteException;\n+\n+/**\n+ * An abstract class for the execution of a file system command\n+ */\n+abstract public class Command extends Configured {\n+  protected String[] args;\n+  \n+  /** Constructor */\n+  protected Command(Configuration conf) {\n+    super(conf);\n+  }\n+  \n+  /** Return the command's name excluding the leading character - */\n+  abstract public String getCommandName();\n+  \n+  /** \n+   * Execute the command on the input path\n+   * \n+   * @param path the input path\n+   * @throws IOException if any error occurs\n+   */\n+  abstract protected void run(Path path) throws IOException;\n+  \n+  /** \n+   * For each source path, execute the command\n+   * \n+   * @return 0 if it runs successfully; -1 if it fails\n+   */\n+  public int runAll() {\n+    int exitCode = 0;\n+    for (String src : args) {\n+      try {\n+        Path srcPath = new Path(src);\n+        FileSystem fs = srcPath.getFileSystem(getConf());\n+        FileStatus[] statuses = fs.globStatus(srcPath);\n+        if (statuses == null) {\n+          System.err.println(\"Can not find listing for \" + src);\n+          exitCode = -1;\n+        } else {\n+          for(FileStatus s : statuses) {\n+            run(s.getPath());\n+          }\n+        }\n+      } catch (RemoteException re) {\n+        exitCode = -1;\n+        String content = re.getLocalizedMessage();\n+        int eol = content.indexOf('\\n');\n+        if (eol>=0) {\n+          content = content.substring(0, eol);\n+        }\n+        System.err.println(getCommandName() + \": \" + content);\n+      } catch (IOException e) {\n+        exitCode = -1;\n+        System.err.println(getCommandName() + \": \" + e.getLocalizedMessage());\n+      }\n+    }\n+    return exitCode;\n+  }\n+}"
        },
        {
            "sha": "c1d84d3670c2b5601b7c578ebf151ebc70ad4243",
            "filename": "src/java/org/apache/hadoop/fs/shell/CommandFormat.java",
            "status": "added",
            "additions": 75,
            "deletions": 0,
            "changes": 75,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fshell%2FCommandFormat.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fshell%2FCommandFormat.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fshell%2FCommandFormat.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,75 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.shell;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Parse the args of a command and check the format of args.\n+ */\n+public class CommandFormat {\n+  final String name;\n+  final int minPar, maxPar;\n+  final Map<String, Boolean> options = new HashMap<String, Boolean>();\n+\n+  /** constructor */\n+  public CommandFormat(String n, int min, int max, String ... possibleOpt) {\n+    name = n;\n+    minPar = min;\n+    maxPar = max;\n+    for(String opt : possibleOpt)\n+      options.put(opt, Boolean.FALSE);\n+  }\n+\n+  /** Parse parameters starting from the given position\n+   * \n+   * @param args an array of input arguments\n+   * @param pos the position at which starts to parse\n+   * @return a list of parameters\n+   */\n+  public List<String> parse(String[] args, int pos) {\n+    List<String> parameters = new ArrayList<String>();\n+    for(; pos < args.length; pos++) {\n+      if (args[pos].charAt(0) == '-' && args[pos].length() > 1) {\n+        String opt = args[pos].substring(1);\n+        if (options.containsKey(opt))\n+          options.put(opt, Boolean.TRUE);\n+        else\n+          throw new IllegalArgumentException(\"Illegal option \" + args[pos]);\n+      }\n+      else\n+        parameters.add(args[pos]);\n+    }\n+    int psize = parameters.size();\n+    if (psize < minPar || psize > maxPar)\n+      throw new IllegalArgumentException(\"Illegal number of arguments\");\n+    return parameters;\n+  }\n+  \n+  /** Return if the option is set or not\n+   * \n+   * @param option String representation of an option\n+   * @return true is the option is set; false otherwise\n+   */\n+  public boolean getOpt(String option) {\n+    return options.get(option);\n+  }\n+}"
        },
        {
            "sha": "2a1317ee6c0d58d25e4231068eb37fd15fa8dca8",
            "filename": "src/java/org/apache/hadoop/fs/shell/CommandUtils.java",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fshell%2FCommandUtils.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fshell%2FCommandUtils.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fshell%2FCommandUtils.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,28 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.shell;\n+\n+final class CommandUtils {\n+  static String formatDescription(String usage, String... desciptions) {\n+    StringBuilder b = new StringBuilder(usage + \": \" + desciptions[0]);\n+    for(int i = 1; i < desciptions.length; i++) {\n+      b.append(\"\\n\\t\\t\" + desciptions[i]);\n+    }\n+    return b.toString();\n+  }\n+}"
        },
        {
            "sha": "abacb2a690df0583534447944dd474402f1caefa",
            "filename": "src/java/org/apache/hadoop/fs/shell/Count.java",
            "status": "added",
            "additions": 77,
            "deletions": 0,
            "changes": 77,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fshell%2FCount.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fshell%2FCount.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Ffs%2Fshell%2FCount.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,77 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.fs.shell;\n+\n+import java.io.*;\n+import java.util.List;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+\n+/**\n+ * Count the number of directories, files, bytes, quota, and remaining quota.\n+ */\n+public class Count extends Command {\n+  public static final String NAME = \"count\";\n+  public static final String USAGE = \"-\" + NAME + \"[-q] <path>\";\n+  public static final String DESCRIPTION = CommandUtils.formatDescription(USAGE, \n+      \"Count the number of directories, files and bytes under the paths\",\n+      \"that match the specified file pattern.  The output columns are:\",\n+      \"DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME or\",\n+      \"QUOTA REMAINING_QUATA SPACE_QUOTA REMAINING_SPACE_QUOTA \",\n+      \"      DIR_COUNT FILE_COUNT CONTENT_SIZE FILE_NAME\");\n+  \n+  private boolean qOption;\n+\n+  /** Constructor\n+   * \n+   * @param cmd the count command\n+   * @param pos the starting index of the arguments \n+   */\n+  public Count(String[] cmd, int pos, Configuration conf) {\n+    super(conf);\n+    CommandFormat c = new CommandFormat(NAME, 1, Integer.MAX_VALUE, \"q\");\n+    List<String> parameters = c.parse(cmd, pos);\n+    this.args = parameters.toArray(new String[parameters.size()]);\n+    if (this.args.length == 0) { // default path is the current working directory\n+      this.args = new String[] {\".\"};\n+    }\n+    this.qOption = c.getOpt(\"q\") ? true: false;\n+  }\n+  \n+  /** Check if a command is the count command\n+   * \n+   * @param cmd A string representation of a command starting with \"-\"\n+   * @return true if this is a count command; false otherwise\n+   */\n+  public static boolean matches(String cmd) {\n+    return (\"-\" + NAME).equals(cmd); \n+  }\n+\n+  @Override\n+  public String getCommandName() {\n+    return NAME;\n+  }\n+\n+  @Override\n+  protected void run(Path path) throws IOException {\n+    FileSystem fs = path.getFileSystem(getConf());\n+    System.out.println(fs.getContentSummary(path).toString(qOption) + path);\n+  }\n+}"
        },
        {
            "sha": "40557c08d7af03e936e1679516ad2445239aff82",
            "filename": "src/java/org/apache/hadoop/http/FilterContainer.java",
            "status": "added",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhttp%2FFilterContainer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhttp%2FFilterContainer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhttp%2FFilterContainer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,40 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.http;\n+\n+import java.util.Map;\n+\n+/**\n+ * A container class for javax.servlet.Filter. \n+ */\n+public interface FilterContainer {\n+  /**\n+   * Add a filter to the container.\n+   * @param name Filter name\n+   * @param classname Filter class name\n+   * @param parameters a map from parameter names to initial values\n+   */\n+  void addFilter(String name, String classname, Map<String, String> parameters);\n+  /**\n+   * Add a global filter to the container.\n+   * @param name filter name\n+   * @param classname filter class name\n+   * @param parameters a map from parameter names to initial values\n+   */\n+  void addGlobalFilter(String name, String classname, Map<String, String> parameters);\n+}"
        },
        {
            "sha": "3f4765e29bea6159d4824ad0ed95c06089a3d9b3",
            "filename": "src/java/org/apache/hadoop/http/FilterInitializer.java",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhttp%2FFilterInitializer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhttp%2FFilterInitializer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhttp%2FFilterInitializer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,29 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.http;\n+\n+/**\n+ * Initialize a javax.servlet.Filter. \n+ */\n+public abstract class FilterInitializer {\n+  /**\n+   * Initialize a Filter to a FilterContainer.\n+   * @param container The filter container\n+   */\n+  abstract void initFilter(FilterContainer container);\n+}\n\\ No newline at end of file"
        },
        {
            "sha": "a739ba69acea92904cd4b423da056130153d0d42",
            "filename": "src/java/org/apache/hadoop/http/HttpServer.java",
            "status": "added",
            "additions": 519,
            "deletions": 0,
            "changes": 519,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhttp%2FHttpServer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhttp%2FHttpServer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fhttp%2FHttpServer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,519 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.http;\n+\n+import java.io.IOException;\n+import java.io.PrintWriter;\n+import java.net.BindException;\n+import java.net.InetSocketAddress;\n+import java.net.URL;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.nio.channels.ServerSocketChannel;\n+\n+import javax.servlet.ServletException;\n+import javax.servlet.http.HttpServlet;\n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.log.LogLevel;\n+import org.apache.hadoop.metrics.MetricsServlet;\n+import org.apache.hadoop.util.ReflectionUtils;\n+\n+import org.mortbay.jetty.Connector;\n+import org.mortbay.jetty.Handler;\n+import org.mortbay.jetty.Server;\n+import org.mortbay.jetty.handler.ContextHandlerCollection;\n+import org.mortbay.jetty.nio.SelectChannelConnector;\n+import org.mortbay.jetty.security.SslSocketConnector;\n+import org.mortbay.jetty.servlet.Context;\n+import org.mortbay.jetty.servlet.DefaultServlet;\n+import org.mortbay.jetty.servlet.FilterHolder;\n+import org.mortbay.jetty.servlet.FilterMapping;\n+import org.mortbay.jetty.servlet.ServletHandler;\n+import org.mortbay.jetty.servlet.ServletHolder;\n+import org.mortbay.jetty.webapp.WebAppContext;\n+import org.mortbay.thread.QueuedThreadPool;\n+import org.mortbay.util.MultiException;\n+\n+/**\n+ * Create a Jetty embedded server to answer http requests. The primary goal\n+ * is to serve up status information for the server.\n+ * There are three contexts:\n+ *   \"/logs/\" -> points to the log directory\n+ *   \"/static/\" -> points to common static files (src/webapps/static)\n+ *   \"/\" -> the jsp server code from (src/webapps/<name>)\n+ */\n+public class HttpServer implements FilterContainer {\n+  public static final Log LOG = LogFactory.getLog(HttpServer.class);\n+\n+  static final String FILTER_INITIALIZER_PROPERTY\n+      = \"hadoop.http.filter.initializers\";\n+\n+  protected final Server webServer;\n+  protected final Connector listener;\n+  protected final WebAppContext webAppContext;\n+  protected final boolean findPort;\n+  protected final Map<Context, Boolean> defaultContexts =\n+      new HashMap<Context, Boolean>();\n+  protected final List<String> filterNames = new ArrayList<String>();\n+  private static final int MAX_RETRIES = 10;\n+\n+  /** Same as this(name, bindAddress, port, findPort, null); */\n+  public HttpServer(String name, String bindAddress, int port, boolean findPort\n+      ) throws IOException {\n+    this(name, bindAddress, port, findPort, new Configuration());\n+  }\n+\n+  /**\n+   * Create a status server on the given port.\n+   * The jsp scripts are taken from src/webapps/<name>.\n+   * @param name The name of the server\n+   * @param port The port to use on the server\n+   * @param findPort whether the server should start at the given port and \n+   *        increment by 1 until it finds a free port.\n+   * @param conf Configuration \n+   */\n+  public HttpServer(String name, String bindAddress, int port,\n+      boolean findPort, Configuration conf) throws IOException {\n+    webServer = new Server();\n+    this.findPort = findPort;\n+\n+    listener = createBaseListener(conf);\n+    listener.setHost(bindAddress);\n+    listener.setPort(port);\n+    webServer.addConnector(listener);\n+\n+    webServer.setThreadPool(new QueuedThreadPool());\n+\n+    final String appDir = getWebAppsPath();\n+    ContextHandlerCollection contexts = new ContextHandlerCollection();\n+    webServer.setHandler(contexts);\n+\n+    webAppContext = new WebAppContext();\n+    webAppContext.setContextPath(\"/\");\n+    webAppContext.setWar(appDir + \"/\" + name);\n+    webServer.addHandler(webAppContext);\n+\n+    addDefaultApps(contexts, appDir);\n+\n+    final FilterInitializer[] initializers = getFilterInitializers(conf); \n+    if (initializers != null) {\n+      for(FilterInitializer c : initializers) {\n+        c.initFilter(this);\n+      }\n+    }\n+    addDefaultServlets();\n+  }\n+\n+  /**\n+   * Create a required listener for the Jetty instance listening on the port\n+   * provided. This wrapper and all subclasses must create at least one\n+   * listener.\n+   */\n+  protected Connector createBaseListener(Configuration conf)\n+      throws IOException {\n+    SelectChannelConnector ret = new SelectChannelConnector();\n+    ret.setLowResourceMaxIdleTime(10000);\n+    ret.setAcceptQueueSize(128);\n+    ret.setResolveNames(false);\n+    ret.setUseDirectBuffers(false);\n+    return ret;\n+  }\n+\n+  /** Get an array of FilterConfiguration specified in the conf */\n+  private static FilterInitializer[] getFilterInitializers(Configuration conf) {\n+    if (conf == null) {\n+      return null;\n+    }\n+\n+    Class<?>[] classes = conf.getClasses(FILTER_INITIALIZER_PROPERTY);\n+    if (classes == null) {\n+      return null;\n+    }\n+\n+    FilterInitializer[] initializers = new FilterInitializer[classes.length];\n+    for(int i = 0; i < classes.length; i++) {\n+      initializers[i] = (FilterInitializer)ReflectionUtils.newInstance(\n+          classes[i], conf);\n+    }\n+    return initializers;\n+  }\n+\n+  /**\n+   * Add default apps.\n+   * @param appDir The application directory\n+   * @throws IOException\n+   */\n+  protected void addDefaultApps(ContextHandlerCollection parent,\n+      final String appDir) throws IOException {\n+    // set up the context for \"/logs/\" if \"hadoop.log.dir\" property is defined. \n+    String logDir = System.getProperty(\"hadoop.log.dir\");\n+    if (logDir != null) {\n+      Context logContext = new Context(parent, \"/logs\");\n+      logContext.setResourceBase(logDir);\n+      logContext.addServlet(DefaultServlet.class, \"/\");\n+      defaultContexts.put(logContext, true);\n+    }\n+    // set up the context for \"/static/*\"\n+    Context staticContext = new Context(parent, \"/static\");\n+    staticContext.setResourceBase(appDir + \"/static\");\n+    staticContext.addServlet(DefaultServlet.class, \"/*\");\n+    defaultContexts.put(staticContext, true);\n+  }\n+  \n+  /**\n+   * Add default servlets.\n+   */\n+  protected void addDefaultServlets() {\n+    // set up default servlets\n+    addServlet(\"stacks\", \"/stacks\", StackServlet.class);\n+    addServlet(\"logLevel\", \"/logLevel\", LogLevel.Servlet.class);\n+    addServlet(\"metrics\", \"/metrics\", MetricsServlet.class);\n+  }\n+\n+  public void addContext(Context ctxt, boolean isFiltered)\n+      throws IOException {\n+    webServer.addHandler(ctxt);\n+    defaultContexts.put(ctxt, isFiltered);\n+  }\n+\n+  /**\n+   * Add a context \n+   * @param pathSpec The path spec for the context\n+   * @param dir The directory containing the context\n+   * @param isFiltered if true, the servlet is added to the filter path mapping \n+   * @throws IOException\n+   */\n+  protected void addContext(String pathSpec, String dir, boolean isFiltered) throws IOException {\n+    if (0 == webServer.getHandlers().length) {\n+      throw new RuntimeException(\"Couldn't find handler\");\n+    }\n+    WebAppContext webAppCtx = new WebAppContext();\n+    webAppCtx.setContextPath(pathSpec);\n+    webAppCtx.setWar(dir);\n+    addContext(webAppCtx, true);\n+  }\n+\n+  /**\n+   * Set a value in the webapp context. These values are available to the jsp\n+   * pages as \"application.getAttribute(name)\".\n+   * @param name The name of the attribute\n+   * @param value The value of the attribute\n+   */\n+  public void setAttribute(String name, Object value) {\n+    webAppContext.setAttribute(name, value);\n+  }\n+\n+  /**\n+   * Add a servlet in the server.\n+   * @param name The name of the servlet (can be passed as null)\n+   * @param pathSpec The path spec for the servlet\n+   * @param clazz The servlet class\n+   */\n+  public void addServlet(String name, String pathSpec,\n+      Class<? extends HttpServlet> clazz) {\n+    addInternalServlet(name, pathSpec, clazz);\n+    addFilterPathMapping(pathSpec, webAppContext);\n+  }\n+\n+  /**\n+   * Add an internal servlet in the server.\n+   * @param name The name of the servlet (can be passed as null)\n+   * @param pathSpec The path spec for the servlet\n+   * @param clazz The servlet class\n+   * @deprecated this is a temporary method\n+   */\n+  @Deprecated\n+  public void addInternalServlet(String name, String pathSpec,\n+      Class<? extends HttpServlet> clazz) {\n+    ServletHolder holder = new ServletHolder(clazz);\n+    if (name != null) {\n+      holder.setName(name);\n+    }\n+    webAppContext.addServlet(holder, pathSpec);\n+  }\n+\n+  /** {@inheritDoc} */\n+  public void addFilter(String name, String classname,\n+      Map<String, String> parameters) {\n+\n+    final String[] USER_FACING_URLS = { \"*.html\", \"*.jsp\" };\n+    defineFilter(webAppContext, name, classname, parameters, USER_FACING_URLS);\n+    final String[] ALL_URLS = { \"/*\" };\n+    for (Map.Entry<Context, Boolean> e : defaultContexts.entrySet()) {\n+      if (e.getValue()) {\n+        Context ctx = e.getKey();\n+        defineFilter(ctx, name, classname, parameters, ALL_URLS);\n+        LOG.info(\"Added filter \" + name + \" (class=\" + classname\n+            + \") to context \" + ctx.getDisplayName());\n+      }\n+    }\n+    filterNames.add(name);\n+  }\n+\n+  /** {@inheritDoc} */\n+  public void addGlobalFilter(String name, String classname,\n+      Map<String, String> parameters) {\n+    final String[] ALL_URLS = { \"/*\" };\n+    defineFilter(webAppContext, name, classname, parameters, ALL_URLS);\n+    for (Context ctx : defaultContexts.keySet()) {\n+      defineFilter(ctx, name, classname, parameters, ALL_URLS);\n+    }\n+    LOG.info(\"Added global filter\" + name + \" (class=\" + classname + \")\");\n+  }\n+\n+  /**\n+   * Define a filter for a context and set up default url mappings.\n+   */\n+  protected void defineFilter(Context ctx, String name,\n+      String classname, Map<String,String> parameters, String[] urls) {\n+\n+    FilterHolder holder = new FilterHolder();\n+    holder.setName(name);\n+    holder.setClassName(classname);\n+    holder.setInitParameters(parameters);\n+    FilterMapping fmap = new FilterMapping();\n+    fmap.setPathSpecs(urls);\n+    fmap.setDispatches(Handler.ALL);\n+    fmap.setFilterName(name);\n+    ServletHandler handler = ctx.getServletHandler();\n+    handler.addFilter(holder, fmap);\n+  }\n+\n+  /**\n+   * Add the path spec to the filter path mapping.\n+   * @param pathSpec The path spec\n+   * @param webAppCtx The WebApplicationContext to add to\n+   */\n+  protected void addFilterPathMapping(String pathSpec,\n+      Context webAppCtx) {\n+    ServletHandler handler = webAppCtx.getServletHandler();\n+    for(String name : filterNames) {\n+      FilterMapping fmap = new FilterMapping();\n+      fmap.setPathSpec(pathSpec);\n+      fmap.setFilterName(name);\n+      fmap.setDispatches(Handler.ALL);\n+      handler.addFilterMapping(fmap);\n+    }\n+  }\n+  \n+  /**\n+   * Get the value in the webapp context.\n+   * @param name The name of the attribute\n+   * @return The value of the attribute\n+   */\n+  public Object getAttribute(String name) {\n+    return webAppContext.getAttribute(name);\n+  }\n+\n+  /**\n+   * Get the pathname to the webapps files.\n+   * @return the pathname as a URL\n+   * @throws IOException if 'webapps' directory cannot be found on CLASSPATH.\n+   */\n+  protected String getWebAppsPath() throws IOException {\n+    URL url = getClass().getClassLoader().getResource(\"webapps\");\n+    if (url == null) \n+      throw new IOException(\"webapps not found in CLASSPATH\"); \n+    return url.toString();\n+  }\n+\n+  /**\n+   * Get the port that the server is on\n+   * @return the port\n+   */\n+  public int getPort() {\n+    return webServer.getConnectors()[0].getLocalPort();\n+  }\n+\n+  /**\n+   * Set the min, max number of worker threads (simultaneous connections).\n+   */\n+  public void setThreads(int min, int max) {\n+    QueuedThreadPool pool = (QueuedThreadPool) webServer.getThreadPool() ;\n+    pool.setMinThreads(min);\n+    pool.setMaxThreads(max);\n+  }\n+\n+  /**\n+   * Configure an ssl listener on the server.\n+   * @param addr address to listen on\n+   * @param keystore location of the keystore\n+   * @param storPass password for the keystore\n+   * @param keyPass password for the key\n+   * @deprecated Use {@link #addSslListener(InetSocketAddress, Configuration, boolean)}\n+   */\n+  @Deprecated\n+  public void addSslListener(InetSocketAddress addr, String keystore,\n+      String storPass, String keyPass) throws IOException {\n+    if (webServer.isStarted()) {\n+      throw new IOException(\"Failed to add ssl listener\");\n+    }\n+    SslSocketConnector sslListener = new SslSocketConnector();\n+    sslListener.setHost(addr.getHostName());\n+    sslListener.setPort(addr.getPort());\n+    sslListener.setKeystore(keystore);\n+    sslListener.setPassword(storPass);\n+    sslListener.setKeyPassword(keyPass);\n+    webServer.addConnector(sslListener);\n+  }\n+\n+  /**\n+   * Configure an ssl listener on the server.\n+   * @param addr address to listen on\n+   * @param sslConf conf to retrieve ssl options\n+   * @param needClientAuth whether client authentication is required\n+   */\n+  public void addSslListener(InetSocketAddress addr, Configuration sslConf,\n+      boolean needClientAuth) throws IOException {\n+    if (webServer.isStarted()) {\n+      throw new IOException(\"Failed to add ssl listener\");\n+    }\n+    if (needClientAuth) {\n+      // setting up SSL truststore for authenticating clients\n+      System.setProperty(\"javax.net.ssl.trustStore\", sslConf.get(\n+          \"ssl.server.truststore.location\", \"\"));\n+      System.setProperty(\"javax.net.ssl.trustStorePassword\", sslConf.get(\n+          \"ssl.server.truststore.password\", \"\"));\n+      System.setProperty(\"javax.net.ssl.trustStoreType\", sslConf.get(\n+          \"ssl.server.truststore.type\", \"jks\"));\n+    }\n+    SslSocketConnector sslListener = new SslSocketConnector();\n+    sslListener.setHost(addr.getHostName());\n+    sslListener.setPort(addr.getPort());\n+    sslListener.setKeystore(sslConf.get(\"ssl.server.keystore.location\"));\n+    sslListener.setPassword(sslConf.get(\"ssl.server.keystore.password\", \"\"));\n+    sslListener.setKeyPassword(sslConf.get(\"ssl.server.keystore.keypassword\", \"\"));\n+    sslListener.setKeystoreType(sslConf.get(\"ssl.server.keystore.type\", \"jks\"));\n+    sslListener.setNeedClientAuth(needClientAuth);\n+    webServer.addConnector(sslListener);\n+  }\n+\n+  /**\n+   * Start the server. Does not wait for the server to start.\n+   */\n+  public void start() throws IOException {\n+    try {\n+      int port = 0;\n+      int oriPort = listener.getPort(); // The original requested port\n+      while (true) {\n+        try {\n+          port = webServer.getConnectors()[0].getLocalPort();\n+          LOG.info(\"Port returned by webServer.getConnectors()[0].\" +\n+          \t\t\"getLocalPort() before open() is \"+ port + \n+          \t\t\". Opening the listener on \" + oriPort);\n+          listener.open();\n+          port = listener.getLocalPort();\n+          LOG.info(\"listener.getLocalPort() returned \" + listener.getLocalPort() + \n+                \" webServer.getConnectors()[0].getLocalPort() returned \" +\n+                webServer.getConnectors()[0].getLocalPort());\n+          //Workaround to handle the problem reported in HADOOP-4744\n+          if (port < 0) {\n+            Thread.sleep(100);\n+            int numRetries = 1;\n+            while (port < 0) {\n+              LOG.warn(\"listener.getLocalPort returned \" + port);\n+              if (numRetries++ > MAX_RETRIES) {\n+                throw new Exception(\" listener.getLocalPort is returning \" +\n+                \t\t\"less than 0 even after \" +numRetries+\" resets\");\n+              }\n+              for (int i = 0; i < 2; i++) {\n+                LOG.info(\"Retrying listener.getLocalPort()\");\n+                port = listener.getLocalPort();\n+                if (port > 0) {\n+                  break;\n+                }\n+                Thread.sleep(200);\n+              }\n+              if (port > 0) {\n+                break;\n+              }\n+              LOG.info(\"Bouncing the listener\");\n+              listener.close();\n+              Thread.sleep(1000);\n+              listener.setPort(oriPort == 0 ? 0 : (oriPort += 1));\n+              listener.open();\n+              Thread.sleep(100);\n+              port = listener.getLocalPort();\n+            }\n+          } //Workaround end\n+          LOG.info(\"Jetty bound to port \" + port);\n+          webServer.start();\n+          break;\n+        } catch (IOException ex) {\n+          // if this is a bind exception,\n+          // then try the next port number.\n+          if (ex instanceof BindException) {\n+            if (!findPort) {\n+              throw (BindException) ex;\n+            }\n+          } else {\n+            LOG.info(\"HttpServer.start() threw a non Bind IOException\"); \n+            throw ex;\n+          }\n+        } catch (MultiException ex) {\n+          LOG.info(\"HttpServer.start() threw a MultiException\"); \n+          throw ex;\n+        }\n+        listener.setPort((oriPort += 1));\n+      }\n+    } catch (IOException e) {\n+      throw e;\n+    } catch (Exception e) {\n+      throw new IOException(\"Problem starting http server\", e);\n+    }\n+  }\n+\n+  /**\n+   * stop the server\n+   */\n+  public void stop() throws Exception {\n+    listener.close();\n+    webServer.stop();\n+  }\n+\n+  public void join() throws InterruptedException {\n+    webServer.join();\n+  }\n+\n+  /**\n+   * A very simple servlet to serve up a text representation of the current\n+   * stack traces. It both returns the stacks to the caller and logs them.\n+   * Currently the stack traces are done sequentially rather than exactly the\n+   * same data.\n+   */\n+  public static class StackServlet extends HttpServlet {\n+    private static final long serialVersionUID = -6284183679759467039L;\n+\n+    @Override\n+    public void doGet(HttpServletRequest request, HttpServletResponse response)\n+      throws ServletException, IOException {\n+      \n+      PrintWriter out = new PrintWriter(response.getOutputStream());\n+      ReflectionUtils.printThreadInfo(out, \"\");\n+      out.close();\n+      ReflectionUtils.logThreadInfo(LOG, \"jsp requested\", 1);      \n+    }\n+  }\n+}"
        },
        {
            "sha": "5829d4f111154c1917ff49d003109893e1b23ae1",
            "filename": "src/java/org/apache/hadoop/io/AbstractMapWritable.java",
            "status": "added",
            "additions": 207,
            "deletions": 0,
            "changes": 207,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FAbstractMapWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FAbstractMapWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FAbstractMapWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,207 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.io;\n+\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.hadoop.conf.Configurable;\n+import org.apache.hadoop.conf.Configuration;\n+\n+/**\n+ * Abstract base class for MapWritable and SortedMapWritable\n+ * \n+ * Unlike org.apache.nutch.crawl.MapWritable, this class allows creation of\n+ * MapWritable&lt;Writable, MapWritable&gt; so the CLASS_TO_ID and ID_TO_CLASS\n+ * maps travel with the class instead of being static.\n+ * \n+ * Class ids range from 1 to 127 so there can be at most 127 distinct classes\n+ * in any specific map instance.\n+ */\n+public abstract class AbstractMapWritable implements Writable, Configurable {\n+  private AtomicReference<Configuration> conf;\n+  \n+  /* Class to id mappings */\n+  private Map<Class, Byte> classToIdMap = new ConcurrentHashMap<Class, Byte>();\n+  \n+  /* Id to Class mappings */\n+  private Map<Byte, Class> idToClassMap = new ConcurrentHashMap<Byte, Class>();\n+  \n+  /* The number of new classes (those not established by the constructor) */\n+  private volatile byte newClasses = 0;\n+  \n+  /** @return the number of known classes */\n+  byte getNewClasses() {\n+    return newClasses;\n+  }\n+\n+  /**\n+   * Used to add \"predefined\" classes and by Writable to copy \"new\" classes.\n+   */\n+  private synchronized void addToMap(Class clazz, byte id) {\n+    if (classToIdMap.containsKey(clazz)) {\n+      byte b = classToIdMap.get(clazz);\n+      if (b != id) {\n+        throw new IllegalArgumentException (\"Class \" + clazz.getName() +\n+          \" already registered but maps to \" + b + \" and not \" + id);\n+      }\n+    }\n+    if (idToClassMap.containsKey(id)) {\n+      Class c = idToClassMap.get(id);\n+      if (!c.equals(clazz)) {\n+        throw new IllegalArgumentException(\"Id \" + id + \" exists but maps to \" +\n+            c.getName() + \" and not \" + clazz.getName());\n+      }\n+    }\n+    classToIdMap.put(clazz, id);\n+    idToClassMap.put(id, clazz);\n+  }\n+  \n+  /** Add a Class to the maps if it is not already present. */ \n+  protected synchronized void addToMap(Class clazz) {\n+    if (classToIdMap.containsKey(clazz)) {\n+      return;\n+    }\n+    if (newClasses + 1 > Byte.MAX_VALUE) {\n+      throw new IndexOutOfBoundsException(\"adding an additional class would\" +\n+      \" exceed the maximum number allowed\");\n+    }\n+    byte id = ++newClasses;\n+    addToMap(clazz, id);\n+  }\n+\n+  /** @return the Class class for the specified id */\n+  protected Class getClass(byte id) {\n+    return idToClassMap.get(id);\n+  }\n+\n+  /** @return the id for the specified Class */\n+  protected byte getId(Class clazz) {\n+    return classToIdMap.containsKey(clazz) ? classToIdMap.get(clazz) : -1;\n+  }\n+\n+  /** Used by child copy constructors. */\n+  protected synchronized void copy(Writable other) {\n+    if (other != null) {\n+      try {\n+        DataOutputBuffer out = new DataOutputBuffer();\n+        other.write(out);\n+        DataInputBuffer in = new DataInputBuffer();\n+        in.reset(out.getData(), out.getLength());\n+        readFields(in);\n+        \n+      } catch (IOException e) {\n+        throw new IllegalArgumentException(\"map cannot be copied: \" +\n+            e.getMessage());\n+      }\n+      \n+    } else {\n+      throw new IllegalArgumentException(\"source map cannot be null\");\n+    }\n+  }\n+  \n+  /** constructor. */\n+  protected AbstractMapWritable() {\n+    this.conf = new AtomicReference<Configuration>();\n+\n+    addToMap(ArrayWritable.class,\n+        Byte.valueOf(Integer.valueOf(-127).byteValue())); \n+    addToMap(BooleanWritable.class,\n+        Byte.valueOf(Integer.valueOf(-126).byteValue()));\n+    addToMap(BytesWritable.class,\n+        Byte.valueOf(Integer.valueOf(-125).byteValue()));\n+    addToMap(FloatWritable.class,\n+        Byte.valueOf(Integer.valueOf(-124).byteValue()));\n+    addToMap(IntWritable.class,\n+        Byte.valueOf(Integer.valueOf(-123).byteValue()));\n+    addToMap(LongWritable.class,\n+        Byte.valueOf(Integer.valueOf(-122).byteValue()));\n+    addToMap(MapWritable.class,\n+        Byte.valueOf(Integer.valueOf(-121).byteValue()));\n+    addToMap(MD5Hash.class,\n+        Byte.valueOf(Integer.valueOf(-120).byteValue()));\n+    addToMap(NullWritable.class,\n+        Byte.valueOf(Integer.valueOf(-119).byteValue()));\n+    addToMap(ObjectWritable.class,\n+        Byte.valueOf(Integer.valueOf(-118).byteValue()));\n+    addToMap(SortedMapWritable.class,\n+        Byte.valueOf(Integer.valueOf(-117).byteValue()));\n+    addToMap(Text.class,\n+        Byte.valueOf(Integer.valueOf(-116).byteValue()));\n+    addToMap(TwoDArrayWritable.class,\n+        Byte.valueOf(Integer.valueOf(-115).byteValue()));\n+    \n+    // UTF8 is deprecated so we don't support it\n+\n+    addToMap(VIntWritable.class,\n+        Byte.valueOf(Integer.valueOf(-114).byteValue()));\n+    addToMap(VLongWritable.class,\n+        Byte.valueOf(Integer.valueOf(-113).byteValue()));\n+\n+  }\n+\n+  /** @return the conf */\n+  public Configuration getConf() {\n+    return conf.get();\n+  }\n+\n+  /** @param conf the conf to set */\n+  public void setConf(Configuration conf) {\n+    this.conf.set(conf);\n+  }\n+  \n+  /** {@inheritDoc} */\n+  public void write(DataOutput out) throws IOException {\n+    \n+    // First write out the size of the class table and any classes that are\n+    // \"unknown\" classes\n+    \n+    out.writeByte(newClasses);\n+\n+    for (byte i = 1; i <= newClasses; i++) {\n+      out.writeByte(i);\n+      out.writeUTF(getClass(i).getName());\n+    }\n+  }\n+  \n+  /** {@inheritDoc} */\n+  public void readFields(DataInput in) throws IOException {\n+    \n+    // Get the number of \"unknown\" classes\n+    \n+    newClasses = in.readByte();\n+    \n+    // Then read in the class names and add them to our tables\n+    \n+    for (int i = 0; i < newClasses; i++) {\n+      byte id = in.readByte();\n+      String className = in.readUTF();\n+      try {\n+        addToMap(Class.forName(className), id);\n+        \n+      } catch (ClassNotFoundException e) {\n+        throw new IOException(\"can't find class: \" + className + \" because \"+\n+            e.getMessage());\n+      }\n+    }\n+  }    \n+}"
        },
        {
            "sha": "dafb6ae600ed034da4677f7b6735a26359ecb335",
            "filename": "src/java/org/apache/hadoop/io/ArrayFile.java",
            "status": "added",
            "additions": 94,
            "deletions": 0,
            "changes": 94,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FArrayFile.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FArrayFile.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FArrayFile.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36",
            "patch": "@@ -0,0 +1,94 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.io;\n+\n+import java.io.*;\n+import org.apache.hadoop.fs.*;\n+import org.apache.hadoop.conf.*;\n+import org.apache.hadoop.util.*;\n+import org.apache.hadoop.io.SequenceFile.CompressionType;\n+\n+\n+/** A dense file-based mapping from integers to values. */\n+public class ArrayFile extends MapFile {\n+\n+  protected ArrayFile() {}                            // no public ctor\n+\n+  /** Write a new array file. */\n+  public static class Writer extends MapFile.Writer {\n+    private LongWritable count = new LongWritable(0);\n+\n+    /** Create the named file for values of the named class. */\n+    public Writer(Configuration conf, FileSystem fs,\n+                  String file, Class<? extends Writable> valClass)\n+      throws IOException {\n+      super(conf, fs, file, LongWritable.class, valClass);\n+    }\n+\n+    /** Create the named file for values of the named class. */\n+    public Writer(Configuration conf, FileSystem fs,\n+                  String file, Class<? extends Writable> valClass,\n+                  CompressionType compress, Progressable progress)\n+      throws IOException {\n+      super(conf, fs, file, LongWritable.class, valClass, compress, progress);\n+    }\n+\n+    /** Append a value to the file. */\n+    public synchronized void append(Writable value) throws IOException {\n+      super.append(count, value);                 // add to map\n+      count.set(count.get()+1);                   // increment count\n+    }\n+  }\n+\n+  /** Provide access to an existing array file. */\n+  public static class Reader extends MapFile.Reader {\n+    private LongWritable key = new LongWritable();\n+\n+    /** Construct an array reader for the named file.*/\n+    public Reader(FileSystem fs, String file, Configuration conf) throws IOException {\n+      super(fs, file, conf);\n+    }\n+\n+    /** Positions the reader before its <code>n</code>th value. */\n+    public synchronized void seek(long n) throws IOException {\n+      key.set(n);\n+      seek(key);\n+    }\n+\n+    /** Read and return the next value in the file. */\n+    public synchronized Writable next(Writable value) throws IOException {\n+      return next(key, value) ? value : null;\n+    }\n+\n+    /** Returns the key associated with the most recent call to {@link\n+     * #seek(long)}, {@link #next(Writable)}, or {@link\n+     * #get(long,Writable)}. */\n+    public synchronized long key() throws IOException {\n+      return key.get();\n+    }\n+\n+    /** Return the <code>n</code>th value in the file. */\n+    public synchronized Writable get(long n, Writable value)\n+      throws IOException {\n+      key.set(n);\n+      return get(key, value);\n+    }\n+  }\n+\n+}"
        },
        {
            "sha": "9c6643548a0e62fce35efd1a2bf8312a2e84db3c",
            "filename": "src/java/org/apache/hadoop/io/ArrayWritable.java",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FArrayWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FArrayWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FArrayWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "0fb0882e4f706ce9e6982eadc05d352bc198ad8e",
            "filename": "src/java/org/apache/hadoop/io/BinaryComparable.java",
            "status": "added",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FBinaryComparable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FBinaryComparable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FBinaryComparable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "aa616a4565dca167819e5f3a9f269370833c52a2",
            "filename": "src/java/org/apache/hadoop/io/BloomMapFile.java",
            "status": "added",
            "additions": 259,
            "deletions": 0,
            "changes": 259,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FBloomMapFile.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FBloomMapFile.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FBloomMapFile.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "1ef1a29457196d504ba89265a88cfb819ae0a004",
            "filename": "src/java/org/apache/hadoop/io/BooleanWritable.java",
            "status": "added",
            "additions": 111,
            "deletions": 0,
            "changes": 111,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FBooleanWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FBooleanWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FBooleanWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "f9bd2e8eb60da877a2aace63c3a2e5197eb4664c",
            "filename": "src/java/org/apache/hadoop/io/ByteWritable.java",
            "status": "added",
            "additions": 87,
            "deletions": 0,
            "changes": 87,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FByteWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FByteWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FByteWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "9f6bbe0e46da80685fab8fa51617b776f2a4a5bd",
            "filename": "src/java/org/apache/hadoop/io/BytesWritable.java",
            "status": "added",
            "additions": 216,
            "deletions": 0,
            "changes": 216,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FBytesWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FBytesWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FBytesWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "a0cf8a69441a7718fcf1ab032b582d3a420d9892",
            "filename": "src/java/org/apache/hadoop/io/Closeable.java",
            "status": "added",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FCloseable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FCloseable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FCloseable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "17aca07c4d83f9153dd776f9dcd4418837ccbd36",
            "filename": "src/java/org/apache/hadoop/io/CompressedWritable.java",
            "status": "added",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FCompressedWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FCompressedWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FCompressedWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "71b98f81a39080ac7c3124f827ec4e36e7d82fc6",
            "filename": "src/java/org/apache/hadoop/io/DataInputBuffer.java",
            "status": "added",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDataInputBuffer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDataInputBuffer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDataInputBuffer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "a7ad89839ea99e31e655527298c7326236e740c2",
            "filename": "src/java/org/apache/hadoop/io/DataOutputBuffer.java",
            "status": "added",
            "additions": 108,
            "deletions": 0,
            "changes": 108,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDataOutputBuffer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDataOutputBuffer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDataOutputBuffer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "124a550942d960df38b19ec33334d374b79561eb",
            "filename": "src/java/org/apache/hadoop/io/DefaultStringifier.java",
            "status": "added",
            "additions": 199,
            "deletions": 0,
            "changes": 199,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDefaultStringifier.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDefaultStringifier.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDefaultStringifier.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b27973c180e41119a817fec8b82e11bb4749b116",
            "filename": "src/java/org/apache/hadoop/io/DeprecatedUTF8.java",
            "status": "added",
            "additions": 60,
            "deletions": 0,
            "changes": 60,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDeprecatedUTF8.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDeprecatedUTF8.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDeprecatedUTF8.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "fa6f3843bf39a8398f52d824e9c2a1cb81cf3f31",
            "filename": "src/java/org/apache/hadoop/io/DoubleWritable.java",
            "status": "added",
            "additions": 95,
            "deletions": 0,
            "changes": 95,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDoubleWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDoubleWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FDoubleWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "7549dca2b6e22cdd440ea41e157880d2183f500d",
            "filename": "src/java/org/apache/hadoop/io/EnumSetWritable.java",
            "status": "added",
            "additions": 202,
            "deletions": 0,
            "changes": 202,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FEnumSetWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FEnumSetWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FEnumSetWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "484423f0b4576cd501f0766e73633a2d691e119f",
            "filename": "src/java/org/apache/hadoop/io/FloatWritable.java",
            "status": "added",
            "additions": 87,
            "deletions": 0,
            "changes": 87,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FFloatWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FFloatWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FFloatWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "41df13635a5b93892b3ef7a23231e965dd91961f",
            "filename": "src/java/org/apache/hadoop/io/GenericWritable.java",
            "status": "added",
            "additions": 152,
            "deletions": 0,
            "changes": 152,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FGenericWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FGenericWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FGenericWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "44723f4c32500f63cd7f6c138dc287ff565e1358",
            "filename": "src/java/org/apache/hadoop/io/IOUtils.java",
            "status": "added",
            "additions": 177,
            "deletions": 0,
            "changes": 177,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FIOUtils.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FIOUtils.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FIOUtils.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "272a707738b701a6c80298ca917a8f2dd4c22e2b",
            "filename": "src/java/org/apache/hadoop/io/InputBuffer.java",
            "status": "added",
            "additions": 89,
            "deletions": 0,
            "changes": 89,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FInputBuffer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FInputBuffer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FInputBuffer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "99875030a634158906c9154506e810b78d2a53df",
            "filename": "src/java/org/apache/hadoop/io/IntWritable.java",
            "status": "added",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FIntWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FIntWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FIntWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "7f2c610e6b940f73ccc753687c3bf56aa10764c3",
            "filename": "src/java/org/apache/hadoop/io/LongWritable.java",
            "status": "added",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FLongWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FLongWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FLongWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "a28c3ae20a2856f986f98a50ca59b1c8417cff8a",
            "filename": "src/java/org/apache/hadoop/io/MD5Hash.java",
            "status": "added",
            "additions": 221,
            "deletions": 0,
            "changes": 221,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FMD5Hash.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FMD5Hash.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FMD5Hash.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "10598f0a42a6bb5ffec864e8d8c4643f144ded64",
            "filename": "src/java/org/apache/hadoop/io/MapFile.java",
            "status": "added",
            "additions": 713,
            "deletions": 0,
            "changes": 713,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FMapFile.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FMapFile.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FMapFile.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "66c493be2c3851370d18c301b56300fe999542bf",
            "filename": "src/java/org/apache/hadoop/io/MapWritable.java",
            "status": "added",
            "additions": 169,
            "deletions": 0,
            "changes": 169,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FMapWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FMapWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FMapWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "eea6b556d7451281a26cddeb364a959210e642ed",
            "filename": "src/java/org/apache/hadoop/io/MultipleIOException.java",
            "status": "added",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FMultipleIOException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FMultipleIOException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FMultipleIOException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "1df85c84fa94b482f976311fa232544213a38fea",
            "filename": "src/java/org/apache/hadoop/io/NullWritable.java",
            "status": "added",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FNullWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FNullWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FNullWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "df1c44bb2acd6d0fe2ecb467308f6d4ed55c57d8",
            "filename": "src/java/org/apache/hadoop/io/ObjectWritable.java",
            "status": "added",
            "additions": 273,
            "deletions": 0,
            "changes": 273,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FObjectWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FObjectWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FObjectWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "943cb52dbce07774f2c3a28a114833cd4a212c4a",
            "filename": "src/java/org/apache/hadoop/io/OutputBuffer.java",
            "status": "added",
            "additions": 92,
            "deletions": 0,
            "changes": 92,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FOutputBuffer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FOutputBuffer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FOutputBuffer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "4efbb7acfc087146d848f21147bd1a39624dd26c",
            "filename": "src/java/org/apache/hadoop/io/RawComparator.java",
            "status": "added",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FRawComparator.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FRawComparator.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FRawComparator.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "c2ee954097822d2be06231cb4917883c1c824cef",
            "filename": "src/java/org/apache/hadoop/io/SequenceFile.java",
            "status": "added",
            "additions": 3244,
            "deletions": 0,
            "changes": 3244,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FSequenceFile.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FSequenceFile.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FSequenceFile.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "a0cb84922aa77a2909cc12accdf2807a4923182e",
            "filename": "src/java/org/apache/hadoop/io/SetFile.java",
            "status": "added",
            "additions": 105,
            "deletions": 0,
            "changes": 105,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FSetFile.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FSetFile.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FSetFile.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "53a28dddd3a7fceace649608f0c9a2d260ca2d67",
            "filename": "src/java/org/apache/hadoop/io/SortedMapWritable.java",
            "status": "added",
            "additions": 204,
            "deletions": 0,
            "changes": 204,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FSortedMapWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FSortedMapWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FSortedMapWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "e8dba8e05ecf1768c2952071fa9472a99028b731",
            "filename": "src/java/org/apache/hadoop/io/Stringifier.java",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FStringifier.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FStringifier.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FStringifier.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "19faa8768d638dc21e1351c833bb2e6dcdb2aaeb",
            "filename": "src/java/org/apache/hadoop/io/Text.java",
            "status": "added",
            "additions": 594,
            "deletions": 0,
            "changes": 594,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FText.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FText.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FText.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "23463a2a1248a9ec79d53b18230a1ec00700fbff",
            "filename": "src/java/org/apache/hadoop/io/TwoDArrayWritable.java",
            "status": "added",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FTwoDArrayWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FTwoDArrayWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FTwoDArrayWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "d9f45f7e6b43f6ad153ce68b9921d06d01845ee4",
            "filename": "src/java/org/apache/hadoop/io/UTF8.java",
            "status": "added",
            "additions": 286,
            "deletions": 0,
            "changes": 286,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FUTF8.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FUTF8.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FUTF8.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "a8af11bcfff8ca5499c30f565b7f55bebbfa2728",
            "filename": "src/java/org/apache/hadoop/io/VIntWritable.java",
            "status": "added",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FVIntWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FVIntWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FVIntWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "14d8602275fda31bca491601fb3e42b17aa44772",
            "filename": "src/java/org/apache/hadoop/io/VLongWritable.java",
            "status": "added",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FVLongWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FVLongWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FVLongWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "5f57908fd5a2ac122e495f234fc2d9f286a66d35",
            "filename": "src/java/org/apache/hadoop/io/VersionMismatchException.java",
            "status": "added",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FVersionMismatchException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FVersionMismatchException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FVersionMismatchException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "3ca4fe919ab4e7521421ab5a8679f4543c37bc19",
            "filename": "src/java/org/apache/hadoop/io/VersionedWritable.java",
            "status": "added",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FVersionedWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FVersionedWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FVersionedWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b61e5b5c34a7a1f55cd5b66a654b1e84f42f1b3a",
            "filename": "src/java/org/apache/hadoop/io/Writable.java",
            "status": "added",
            "additions": 80,
            "deletions": 0,
            "changes": 80,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b8aaf731cc52458def418452017cfeb552b6d10a",
            "filename": "src/java/org/apache/hadoop/io/WritableComparable.java",
            "status": "added",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableComparable.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableComparable.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableComparable.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b0b08b4126c1b315324eaddc5e73844e9a644704",
            "filename": "src/java/org/apache/hadoop/io/WritableComparator.java",
            "status": "added",
            "additions": 216,
            "deletions": 0,
            "changes": 216,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableComparator.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableComparator.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableComparator.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "92569bd7bae623a9c95cefb5f193351382eac51e",
            "filename": "src/java/org/apache/hadoop/io/WritableFactories.java",
            "status": "added",
            "additions": 63,
            "deletions": 0,
            "changes": 63,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableFactories.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableFactories.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableFactories.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "736485eef57af44313a9f10da14db5903db4bb13",
            "filename": "src/java/org/apache/hadoop/io/WritableFactory.java",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableFactory.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableFactory.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableFactory.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "6b6c1480b554c1beb58e2510f534ad5cdabb2279",
            "filename": "src/java/org/apache/hadoop/io/WritableName.java",
            "status": "added",
            "additions": 79,
            "deletions": 0,
            "changes": 79,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableName.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableName.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableName.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "e49ea9240c2cdd86f95a35ff8d59e85472b5693e",
            "filename": "src/java/org/apache/hadoop/io/WritableUtils.java",
            "status": "added",
            "additions": 418,
            "deletions": 0,
            "changes": 418,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableUtils.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableUtils.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2FWritableUtils.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "84a51410b837ffe195a57f7f655f798c64ddbc45",
            "filename": "src/java/org/apache/hadoop/io/compress/BZip2Codec.java",
            "status": "added",
            "additions": 301,
            "deletions": 0,
            "changes": 301,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FBZip2Codec.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FBZip2Codec.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FBZip2Codec.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b1fb21f0eaf9e97f6ac5a51a3dcc020af4ddf0a2",
            "filename": "src/java/org/apache/hadoop/io/compress/BlockCompressorStream.java",
            "status": "added",
            "additions": 156,
            "deletions": 0,
            "changes": 156,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FBlockCompressorStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FBlockCompressorStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FBlockCompressorStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "96636e7a4ffcefece5141f3b1ea129a8b25741ff",
            "filename": "src/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
            "status": "added",
            "additions": 128,
            "deletions": 0,
            "changes": 128,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FBlockDecompressorStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FBlockDecompressorStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FBlockDecompressorStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "8960b41ef3d66bc76d8ea10174a07d003f94d9bf",
            "filename": "src/java/org/apache/hadoop/io/compress/CodecPool.java",
            "status": "added",
            "additions": 154,
            "deletions": 0,
            "changes": 154,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCodecPool.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCodecPool.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCodecPool.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "9d9ccd4e632397e3c493bd6eb3e2e082d28519cf",
            "filename": "src/java/org/apache/hadoop/io/compress/CompressionCodec.java",
            "status": "added",
            "additions": 110,
            "deletions": 0,
            "changes": 110,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressionCodec.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressionCodec.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressionCodec.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "dae2e68e1c38e1313406422c3e474e04d8dad1aa",
            "filename": "src/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java",
            "status": "added",
            "additions": 230,
            "deletions": 0,
            "changes": 230,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressionCodecFactory.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressionCodecFactory.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressionCodecFactory.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "aabdd2b5e4d44e142f692643b71340019e31e82c",
            "filename": "src/java/org/apache/hadoop/io/compress/CompressionInputStream.java",
            "status": "added",
            "additions": 63,
            "deletions": 0,
            "changes": 63,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressionInputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressionInputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressionInputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "3b0420f11b57faf0bed22f79151da8d715adeb27",
            "filename": "src/java/org/apache/hadoop/io/compress/CompressionOutputStream.java",
            "status": "added",
            "additions": 69,
            "deletions": 0,
            "changes": 69,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressionOutputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressionOutputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressionOutputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "66bc4bfeeda074cd02074eaf3a13ae8203dcbc08",
            "filename": "src/java/org/apache/hadoop/io/compress/Compressor.java",
            "status": "added",
            "additions": 106,
            "deletions": 0,
            "changes": 106,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressor.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressor.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressor.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "6917ebfd051f170dc2113dfa2c23caf5c9bb47c2",
            "filename": "src/java/org/apache/hadoop/io/compress/CompressorStream.java",
            "status": "added",
            "additions": 109,
            "deletions": 0,
            "changes": 109,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressorStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressorStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FCompressorStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "5832a4a741c44c69d3690abd0bcd431ae4cbd090",
            "filename": "src/java/org/apache/hadoop/io/compress/Decompressor.java",
            "status": "added",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FDecompressor.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FDecompressor.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FDecompressor.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "a84bea443e4614813cc31a7d0655658f42157646",
            "filename": "src/java/org/apache/hadoop/io/compress/DecompressorStream.java",
            "status": "added",
            "additions": 159,
            "deletions": 0,
            "changes": 159,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FDecompressorStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FDecompressorStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FDecompressorStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "29dc140c00c9060f4a76070b6c8bd947bfd41100",
            "filename": "src/java/org/apache/hadoop/io/compress/DefaultCodec.java",
            "status": "added",
            "additions": 87,
            "deletions": 0,
            "changes": 87,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FDefaultCodec.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FDefaultCodec.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FDefaultCodec.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "674dce280fe65873225583f18c7b66062a2b4e19",
            "filename": "src/java/org/apache/hadoop/io/compress/GzipCodec.java",
            "status": "added",
            "additions": 216,
            "deletions": 0,
            "changes": 216,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FGzipCodec.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FGzipCodec.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2FGzipCodec.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "99dc28146d31d7e7210a7186c8f15b49a982871d",
            "filename": "src/java/org/apache/hadoop/io/compress/bzip2/BZip2Constants.java",
            "status": "added",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FBZip2Constants.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FBZip2Constants.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FBZip2Constants.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "2594717113c571c5fd88ca180cf68bd63ac6ed93",
            "filename": "src/java/org/apache/hadoop/io/compress/bzip2/BZip2DummyCompressor.java",
            "status": "added",
            "additions": 62,
            "deletions": 0,
            "changes": 62,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FBZip2DummyCompressor.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FBZip2DummyCompressor.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FBZip2DummyCompressor.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "15308fbd038dd2d6b4990fd17ae460e6cce3e815",
            "filename": "src/java/org/apache/hadoop/io/compress/bzip2/BZip2DummyDecompressor.java",
            "status": "added",
            "additions": 52,
            "deletions": 0,
            "changes": 52,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FBZip2DummyDecompressor.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FBZip2DummyDecompressor.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FBZip2DummyDecompressor.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "567cb5efd3fc9657f42ecbeb38e54455d3ac53bf",
            "filename": "src/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java",
            "status": "added",
            "additions": 969,
            "deletions": 0,
            "changes": 969,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FCBZip2InputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FCBZip2InputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FCBZip2InputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "392cf2c521dd444493b3645871cbea624455747a",
            "filename": "src/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java",
            "status": "added",
            "additions": 2081,
            "deletions": 0,
            "changes": 2081,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FCBZip2OutputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FCBZip2OutputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FCBZip2OutputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "a9eaf205804904f6ce0f1acc7d822c5b0a78d505",
            "filename": "src/java/org/apache/hadoop/io/compress/bzip2/CRC.java",
            "status": "added",
            "additions": 125,
            "deletions": 0,
            "changes": 125,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FCRC.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FCRC.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fbzip2%2FCRC.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "f27e831a58b2e415e9a1cfead449cd633c720b0d",
            "filename": "src/java/org/apache/hadoop/io/compress/zlib/BuiltInZlibDeflater.java",
            "status": "added",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FBuiltInZlibDeflater.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FBuiltInZlibDeflater.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FBuiltInZlibDeflater.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "0223587ad01b11dfe4424fd9c930f161d3f9e7df",
            "filename": "src/java/org/apache/hadoop/io/compress/zlib/BuiltInZlibInflater.java",
            "status": "added",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FBuiltInZlibInflater.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FBuiltInZlibInflater.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FBuiltInZlibInflater.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "754af216ad2373571c075671b75d5c7e3937862a",
            "filename": "src/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java",
            "status": "added",
            "additions": 378,
            "deletions": 0,
            "changes": 378,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FZlibCompressor.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FZlibCompressor.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FZlibCompressor.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "56738252284e50164bd85180971201e215f4b8d4",
            "filename": "src/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java",
            "status": "added",
            "additions": 287,
            "deletions": 0,
            "changes": 287,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FZlibDecompressor.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FZlibDecompressor.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FZlibDecompressor.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "e3ce3ec1afe7ff8dc606a63818a66cac9da1ce0a",
            "filename": "src/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java",
            "status": "added",
            "additions": 110,
            "deletions": 0,
            "changes": 110,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FZlibFactory.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FZlibFactory.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fcompress%2Fzlib%2FZlibFactory.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "ce4ca1f352a4a64ba93e5c3eac70247062b4f976",
            "filename": "src/java/org/apache/hadoop/io/package.html",
            "status": "added",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "19b68538539a333cd9d3ce04301284e5bf3a0896",
            "filename": "src/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java",
            "status": "added",
            "additions": 88,
            "deletions": 0,
            "changes": 88,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2FRetryInvocationHandler.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2FRetryInvocationHandler.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2FRetryInvocationHandler.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "30a78885da83e8d1580dcb4432f1bc7f44aebbd9",
            "filename": "src/java/org/apache/hadoop/io/retry/RetryPolicies.java",
            "status": "added",
            "additions": 258,
            "deletions": 0,
            "changes": 258,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2FRetryPolicies.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2FRetryPolicies.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2FRetryPolicies.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "26d3267bc2ac5d0c7e5fbeef0bf2e29b5c4d51c4",
            "filename": "src/java/org/apache/hadoop/io/retry/RetryPolicy.java",
            "status": "added",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2FRetryPolicy.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2FRetryPolicy.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2FRetryPolicy.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "937f832213ce7ee570cbee4c6cb519656ea3013e",
            "filename": "src/java/org/apache/hadoop/io/retry/RetryProxy.java",
            "status": "added",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2FRetryProxy.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2FRetryProxy.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2FRetryProxy.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "ae553fc7a62dad7c6f566b83fa591b1ec43ece2f",
            "filename": "src/java/org/apache/hadoop/io/retry/package.html",
            "status": "added",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fretry%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "1234a57b2b4ba80b82c6049416ce8d578536ccf0",
            "filename": "src/java/org/apache/hadoop/io/serializer/Deserializer.java",
            "status": "added",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FDeserializer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FDeserializer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FDeserializer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "70e8b689e9c2da4ca585e1f29754b40c84c308fe",
            "filename": "src/java/org/apache/hadoop/io/serializer/DeserializerComparator.java",
            "status": "added",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FDeserializerComparator.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FDeserializerComparator.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FDeserializerComparator.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b44b4b1db76e5d4f29fc30f3b9695185ff7a6cf6",
            "filename": "src/java/org/apache/hadoop/io/serializer/JavaSerialization.java",
            "status": "added",
            "additions": 101,
            "deletions": 0,
            "changes": 101,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FJavaSerialization.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FJavaSerialization.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FJavaSerialization.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "f3de2b10c32542750625b38fe8f4813fba84ecbf",
            "filename": "src/java/org/apache/hadoop/io/serializer/JavaSerializationComparator.java",
            "status": "added",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FJavaSerializationComparator.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FJavaSerializationComparator.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FJavaSerializationComparator.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "6e724bd78b12f05fb05da99e6ddcb37664cac04e",
            "filename": "src/java/org/apache/hadoop/io/serializer/Serialization.java",
            "status": "added",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FSerialization.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FSerialization.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FSerialization.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "f4ba54b4e49bcff111960e8c4264d0e1c20349e7",
            "filename": "src/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
            "status": "added",
            "additions": 89,
            "deletions": 0,
            "changes": 89,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FSerializationFactory.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FSerializationFactory.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FSerializationFactory.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b3243f5b6b8c69a1aed12aeeed6f910a5a6416c1",
            "filename": "src/java/org/apache/hadoop/io/serializer/Serializer.java",
            "status": "added",
            "additions": 52,
            "deletions": 0,
            "changes": 52,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FSerializer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FSerializer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FSerializer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "47586e8c2dd84aa3eb79fe0ced3b49da2581785b",
            "filename": "src/java/org/apache/hadoop/io/serializer/WritableSerialization.java",
            "status": "added",
            "additions": 111,
            "deletions": 0,
            "changes": 111,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FWritableSerialization.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FWritableSerialization.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2FWritableSerialization.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "58c8a3a5c3bb531284b8c37df05186ae29cd63a3",
            "filename": "src/java/org/apache/hadoop/io/serializer/package.html",
            "status": "added",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fio%2Fserializer%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "1d01faf673f6694374569dd1cc8c5c0640e75de8",
            "filename": "src/java/org/apache/hadoop/ipc/Client.java",
            "status": "added",
            "additions": 914,
            "deletions": 0,
            "changes": 914,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FClient.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FClient.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FClient.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "44b113b7edc26a93f74c659414443d8cba6841d0",
            "filename": "src/java/org/apache/hadoop/ipc/ConnectionHeader.java",
            "status": "added",
            "additions": 93,
            "deletions": 0,
            "changes": 93,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FConnectionHeader.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FConnectionHeader.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FConnectionHeader.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "94b0ec82e2e858e7bc8069c33d6eef8fc2ec19b6",
            "filename": "src/java/org/apache/hadoop/ipc/RPC.java",
            "status": "added",
            "additions": 575,
            "deletions": 0,
            "changes": 575,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FRPC.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FRPC.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FRPC.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "214b2f66b612f033db74fb7d9e11f3b7b70beba0",
            "filename": "src/java/org/apache/hadoop/ipc/RemoteException.java",
            "status": "added",
            "additions": 120,
            "deletions": 0,
            "changes": 120,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FRemoteException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FRemoteException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FRemoteException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "890569897b44bef06b8207d93dca6040b07e1760",
            "filename": "src/java/org/apache/hadoop/ipc/Server.java",
            "status": "added",
            "additions": 1255,
            "deletions": 0,
            "changes": 1255,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FServer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FServer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FServer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "16fd871ffa630c3b59b90f21ab3de60d9addb050",
            "filename": "src/java/org/apache/hadoop/ipc/Status.java",
            "status": "added",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FStatus.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FStatus.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FStatus.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "ef5187522f7ac79090d7f269bc2e70cb0b67eee6",
            "filename": "src/java/org/apache/hadoop/ipc/VersionedProtocol.java",
            "status": "added",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FVersionedProtocol.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FVersionedProtocol.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2FVersionedProtocol.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "e2b33b7874385344b07353fb224838f30b01b76d",
            "filename": "src/java/org/apache/hadoop/ipc/metrics/RpcActivityMBean.java",
            "status": "added",
            "additions": 80,
            "deletions": 0,
            "changes": 80,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fmetrics%2FRpcActivityMBean.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fmetrics%2FRpcActivityMBean.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fmetrics%2FRpcActivityMBean.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "a1fbccd06d45b6f423a231a0ae68c5ee834223bf",
            "filename": "src/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java",
            "status": "added",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fmetrics%2FRpcMetrics.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fmetrics%2FRpcMetrics.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fmetrics%2FRpcMetrics.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "443c1947fe2b32bb51dbe6282c7ad59d9953ca9e",
            "filename": "src/java/org/apache/hadoop/ipc/metrics/RpcMgt.java",
            "status": "added",
            "additions": 119,
            "deletions": 0,
            "changes": 119,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fmetrics%2FRpcMgt.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fmetrics%2FRpcMgt.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fmetrics%2FRpcMgt.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "c92bbac574a0eeb492612b3db0e0da0ac6fbd459",
            "filename": "src/java/org/apache/hadoop/ipc/metrics/RpcMgtMBean.java",
            "status": "added",
            "additions": 105,
            "deletions": 0,
            "changes": 105,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fmetrics%2FRpcMgtMBean.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fmetrics%2FRpcMgtMBean.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fmetrics%2FRpcMgtMBean.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "3efd81a2978a63c2e62b38a5fd4cf818ddf68bc9",
            "filename": "src/java/org/apache/hadoop/ipc/package.html",
            "status": "added",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fipc%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "99fd3d0e7e7bea574be7ec56914e57efe2d03ff4",
            "filename": "src/java/org/apache/hadoop/log/LogLevel.java",
            "status": "added",
            "additions": 151,
            "deletions": 0,
            "changes": 151,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Flog%2FLogLevel.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Flog%2FLogLevel.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Flog%2FLogLevel.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "67bd9f95006a5820184bf9f30465d1c59d4bed2e",
            "filename": "src/java/org/apache/hadoop/metrics/ContextFactory.java",
            "status": "added",
            "additions": 204,
            "deletions": 0,
            "changes": 204,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FContextFactory.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FContextFactory.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FContextFactory.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "588a572015354a0a6160dcba6004b7e131a1ba01",
            "filename": "src/java/org/apache/hadoop/metrics/MetricsContext.java",
            "status": "added",
            "additions": 118,
            "deletions": 0,
            "changes": 118,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsContext.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsContext.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsContext.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "8e4f7a0497d00d7186678b29d8251e3d3090fccb",
            "filename": "src/java/org/apache/hadoop/metrics/MetricsException.java",
            "status": "added",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "cec80f225a22070f8d3a6798054601721c3363c5",
            "filename": "src/java/org/apache/hadoop/metrics/MetricsRecord.java",
            "status": "added",
            "additions": 246,
            "deletions": 0,
            "changes": 246,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsRecord.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsRecord.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsRecord.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "44c0bd3965411248e8c3ec887f9c44daaf57e447",
            "filename": "src/java/org/apache/hadoop/metrics/MetricsServlet.java",
            "status": "added",
            "additions": 160,
            "deletions": 0,
            "changes": 160,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsServlet.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsServlet.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsServlet.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "09b9de62ea599badb00cd8eb4c9071c599400582",
            "filename": "src/java/org/apache/hadoop/metrics/MetricsUtil.java",
            "status": "added",
            "additions": 100,
            "deletions": 0,
            "changes": 100,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsUtil.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsUtil.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FMetricsUtil.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "e418ec09c29395bb17a7344a6c3ac73dc55e5c31",
            "filename": "src/java/org/apache/hadoop/metrics/Updater.java",
            "status": "added",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FUpdater.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FUpdater.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2FUpdater.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "16193276974d7347e63ff98230d3909175b496ba",
            "filename": "src/java/org/apache/hadoop/metrics/file/FileContext.java",
            "status": "added",
            "additions": 139,
            "deletions": 0,
            "changes": 139,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Ffile%2FFileContext.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Ffile%2FFileContext.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Ffile%2FFileContext.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "73584864e273a2f0c5cf6942f6bc3998b252c2c4",
            "filename": "src/java/org/apache/hadoop/metrics/file/package.html",
            "status": "added",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Ffile%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Ffile%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Ffile%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "1affb02f727d51ccf856dd5c3b8ebe5c5f4c5661",
            "filename": "src/java/org/apache/hadoop/metrics/ganglia/GangliaContext.java",
            "status": "added",
            "additions": 231,
            "deletions": 0,
            "changes": 231,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fganglia%2FGangliaContext.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fganglia%2FGangliaContext.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fganglia%2FGangliaContext.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "87598e503325ccb7fcaf6bc657359c84f1bd7d33",
            "filename": "src/java/org/apache/hadoop/metrics/ganglia/package.html",
            "status": "added",
            "additions": 74,
            "deletions": 0,
            "changes": 74,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fganglia%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fganglia%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fganglia%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "deb476eed69d373d9348184190dabb222dd8292e",
            "filename": "src/java/org/apache/hadoop/metrics/jvm/EventCounter.java",
            "status": "added",
            "additions": 94,
            "deletions": 0,
            "changes": 94,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fjvm%2FEventCounter.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fjvm%2FEventCounter.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fjvm%2FEventCounter.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "c51916875bcae5d37af021233ee19f384de1ed45",
            "filename": "src/java/org/apache/hadoop/metrics/jvm/JvmMetrics.java",
            "status": "added",
            "additions": 191,
            "deletions": 0,
            "changes": 191,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fjvm%2FJvmMetrics.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fjvm%2FJvmMetrics.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fjvm%2FJvmMetrics.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "dd16e382dace46bc132b0fc6cda2f1c83cc8ce2a",
            "filename": "src/java/org/apache/hadoop/metrics/package.html",
            "status": "added",
            "additions": 159,
            "deletions": 0,
            "changes": 159,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "e6f85ae37812dac9b4f55f32548ea27357830895",
            "filename": "src/java/org/apache/hadoop/metrics/spi/AbstractMetricsContext.java",
            "status": "added",
            "additions": 475,
            "deletions": 0,
            "changes": 475,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FAbstractMetricsContext.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FAbstractMetricsContext.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FAbstractMetricsContext.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "782fb30485c4c95bc93f9f6df745c1dab81f7bea",
            "filename": "src/java/org/apache/hadoop/metrics/spi/CompositeContext.java",
            "status": "added",
            "additions": 186,
            "deletions": 0,
            "changes": 186,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FCompositeContext.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FCompositeContext.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FCompositeContext.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "4a6929b850764e56a3d0123997d4ff54a7b9b573",
            "filename": "src/java/org/apache/hadoop/metrics/spi/MetricValue.java",
            "status": "added",
            "additions": 52,
            "deletions": 0,
            "changes": 52,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FMetricValue.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FMetricValue.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FMetricValue.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "e3bac5641172dfb6618216f2db0fbb8a0bd3aa49",
            "filename": "src/java/org/apache/hadoop/metrics/spi/MetricsRecordImpl.java",
            "status": "added",
            "additions": 275,
            "deletions": 0,
            "changes": 275,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FMetricsRecordImpl.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FMetricsRecordImpl.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FMetricsRecordImpl.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "9e9893426a5e1928465fce5f45133c173ac27b66",
            "filename": "src/java/org/apache/hadoop/metrics/spi/NoEmitMetricsContext.java",
            "status": "added",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FNoEmitMetricsContext.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FNoEmitMetricsContext.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FNoEmitMetricsContext.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "11cccb5b0af309e6bb0268df4126d9bdc81f33b4",
            "filename": "src/java/org/apache/hadoop/metrics/spi/NullContext.java",
            "status": "added",
            "additions": 58,
            "deletions": 0,
            "changes": 58,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FNullContext.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FNullContext.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FNullContext.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "5efe5f0fb770b17f097acf1054b5cee27429e9d1",
            "filename": "src/java/org/apache/hadoop/metrics/spi/NullContextWithUpdateThread.java",
            "status": "added",
            "additions": 69,
            "deletions": 0,
            "changes": 69,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FNullContextWithUpdateThread.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FNullContextWithUpdateThread.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FNullContextWithUpdateThread.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "4fa5415895651e3757e11ab531ad8661a5d64ebb",
            "filename": "src/java/org/apache/hadoop/metrics/spi/OutputRecord.java",
            "status": "added",
            "additions": 90,
            "deletions": 0,
            "changes": 90,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FOutputRecord.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FOutputRecord.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FOutputRecord.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "d7c1912976f7b1a6f4c93d4c28d643452168c497",
            "filename": "src/java/org/apache/hadoop/metrics/spi/Util.java",
            "status": "added",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FUtil.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FUtil.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2FUtil.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b72552f761f298e4d56c9f51f2165bbdbf7a9a51",
            "filename": "src/java/org/apache/hadoop/metrics/spi/package.html",
            "status": "added",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Fspi%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "ded1a5a1958c72799f34532ee0ec98cc54c51a9e",
            "filename": "src/java/org/apache/hadoop/metrics/util/MBeanUtil.java",
            "status": "added",
            "additions": 87,
            "deletions": 0,
            "changes": 87,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMBeanUtil.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMBeanUtil.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMBeanUtil.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "1cbcf3212a68d293c5c512ffbac426fad16b352d",
            "filename": "src/java/org/apache/hadoop/metrics/util/MetricsBase.java",
            "status": "added",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsBase.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsBase.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsBase.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "d65cce0597e6db42302c117cc76ce517b7ff7980",
            "filename": "src/java/org/apache/hadoop/metrics/util/MetricsDynamicMBeanBase.java",
            "status": "added",
            "additions": 226,
            "deletions": 0,
            "changes": 226,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsDynamicMBeanBase.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsDynamicMBeanBase.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsDynamicMBeanBase.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "d467677aaa1ad234b42a57212404912eefc95416",
            "filename": "src/java/org/apache/hadoop/metrics/util/MetricsIntValue.java",
            "status": "added",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsIntValue.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsIntValue.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsIntValue.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "639b6a7bd54e4c717e3cecfb7de53cb01db0bd21",
            "filename": "src/java/org/apache/hadoop/metrics/util/MetricsLongValue.java",
            "status": "added",
            "additions": 88,
            "deletions": 0,
            "changes": 88,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsLongValue.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsLongValue.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsLongValue.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "faf4b63524bb53f274fff38f0cc3fece1dfb5e8c",
            "filename": "src/java/org/apache/hadoop/metrics/util/MetricsRegistry.java",
            "status": "added",
            "additions": 85,
            "deletions": 0,
            "changes": 85,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsRegistry.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsRegistry.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsRegistry.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "96b4fe14880e9860b901abb0bf495dfe8194abcb",
            "filename": "src/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingInt.java",
            "status": "added",
            "additions": 128,
            "deletions": 0,
            "changes": 128,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsTimeVaryingInt.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsTimeVaryingInt.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsTimeVaryingInt.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "929303c832fe9f882f9d758140abc95d22307443",
            "filename": "src/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingLong.java",
            "status": "added",
            "additions": 124,
            "deletions": 0,
            "changes": 124,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsTimeVaryingLong.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsTimeVaryingLong.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsTimeVaryingLong.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "7d05af325da71c4a0f77641664bdfc95f5cb2928",
            "filename": "src/java/org/apache/hadoop/metrics/util/MetricsTimeVaryingRate.java",
            "status": "added",
            "additions": 196,
            "deletions": 0,
            "changes": 196,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsTimeVaryingRate.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsTimeVaryingRate.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fmetrics%2Futil%2FMetricsTimeVaryingRate.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "0490e3cabf44c753774b4e6e13da460fa0681880",
            "filename": "src/java/org/apache/hadoop/net/CachedDNSToSwitchMapping.java",
            "status": "added",
            "additions": 80,
            "deletions": 0,
            "changes": 80,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FCachedDNSToSwitchMapping.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FCachedDNSToSwitchMapping.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FCachedDNSToSwitchMapping.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "97fec6e3052d2342fe8cb807d9349da9d3692d02",
            "filename": "src/java/org/apache/hadoop/net/DNS.java",
            "status": "added",
            "additions": 279,
            "deletions": 0,
            "changes": 279,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FDNS.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FDNS.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FDNS.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "f71b95025d17588a6a75c9e1117cc9b49c76874e",
            "filename": "src/java/org/apache/hadoop/net/DNSToSwitchMapping.java",
            "status": "added",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FDNSToSwitchMapping.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FDNSToSwitchMapping.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FDNSToSwitchMapping.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "ce07fab858e1ae80c3b0eb41dbe4ea1e875ef2be",
            "filename": "src/java/org/apache/hadoop/net/NetUtils.java",
            "status": "added",
            "additions": 440,
            "deletions": 0,
            "changes": 440,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FNetUtils.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FNetUtils.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FNetUtils.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "1de588bd43f9eb602777c4da85f6298fe23de568",
            "filename": "src/java/org/apache/hadoop/net/NetworkTopology.java",
            "status": "added",
            "additions": 655,
            "deletions": 0,
            "changes": 655,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FNetworkTopology.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FNetworkTopology.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FNetworkTopology.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "2448a9436028f8eef602df529a71badc66af4e20",
            "filename": "src/java/org/apache/hadoop/net/Node.java",
            "status": "added",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FNode.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FNode.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FNode.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "83696c813ace8676a2a67c81273778abd8474094",
            "filename": "src/java/org/apache/hadoop/net/NodeBase.java",
            "status": "added",
            "additions": 134,
            "deletions": 0,
            "changes": 134,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FNodeBase.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FNodeBase.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FNodeBase.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "bad5499f89b3dc42c868249c26ef6829453cad53",
            "filename": "src/java/org/apache/hadoop/net/ScriptBasedMapping.java",
            "status": "added",
            "additions": 159,
            "deletions": 0,
            "changes": 159,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FScriptBasedMapping.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FScriptBasedMapping.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FScriptBasedMapping.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "f48d2d2db397d1e533068a3a5f3b9c3bcba146b2",
            "filename": "src/java/org/apache/hadoop/net/SocketIOWithTimeout.java",
            "status": "added",
            "additions": 455,
            "deletions": 0,
            "changes": 455,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FSocketIOWithTimeout.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FSocketIOWithTimeout.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FSocketIOWithTimeout.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "2568ba9c2bc9b8ba781ce3ac52147b37c022b0d2",
            "filename": "src/java/org/apache/hadoop/net/SocketInputStream.java",
            "status": "added",
            "additions": 170,
            "deletions": 0,
            "changes": 170,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FSocketInputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FSocketInputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FSocketInputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "fa4e8500b5a9c4106c0939a8181a49f94ad5b144",
            "filename": "src/java/org/apache/hadoop/net/SocketOutputStream.java",
            "status": "added",
            "additions": 219,
            "deletions": 0,
            "changes": 219,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FSocketOutputStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FSocketOutputStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FSocketOutputStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "19c89210da98148f34e809804ce4324cefe1155e",
            "filename": "src/java/org/apache/hadoop/net/SocksSocketFactory.java",
            "status": "added",
            "additions": 161,
            "deletions": 0,
            "changes": 161,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FSocksSocketFactory.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FSocksSocketFactory.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FSocksSocketFactory.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b95258557e9d1321fa14408496cee396a947b331",
            "filename": "src/java/org/apache/hadoop/net/StandardSocketFactory.java",
            "status": "added",
            "additions": 122,
            "deletions": 0,
            "changes": 122,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FStandardSocketFactory.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FStandardSocketFactory.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2FStandardSocketFactory.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b4e5b5dbdc97c939cc736f1de2f200beebd17e7e",
            "filename": "src/java/org/apache/hadoop/net/package.html",
            "status": "added",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fnet%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b750df9aed5d6691104e10c951a8e04337c607e4",
            "filename": "src/java/org/apache/hadoop/record/BinaryRecordInput.java",
            "status": "added",
            "additions": 136,
            "deletions": 0,
            "changes": 136,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FBinaryRecordInput.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FBinaryRecordInput.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FBinaryRecordInput.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "a0586534590b7abf34dec1de68c95e7eecc4cbc9",
            "filename": "src/java/org/apache/hadoop/record/BinaryRecordOutput.java",
            "status": "added",
            "additions": 120,
            "deletions": 0,
            "changes": 120,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FBinaryRecordOutput.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FBinaryRecordOutput.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FBinaryRecordOutput.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "d0fa95d0b48fb4e1c0647ba5a8775fafb0883cba",
            "filename": "src/java/org/apache/hadoop/record/Buffer.java",
            "status": "added",
            "additions": 246,
            "deletions": 0,
            "changes": 246,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FBuffer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FBuffer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FBuffer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "e9c538d28c6b6a40cac8735f8caa6a49a311b185",
            "filename": "src/java/org/apache/hadoop/record/CsvRecordInput.java",
            "status": "added",
            "additions": 200,
            "deletions": 0,
            "changes": 200,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FCsvRecordInput.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FCsvRecordInput.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FCsvRecordInput.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "f2c6be6f4f9658cce237c59274946a64b40e50ec",
            "filename": "src/java/org/apache/hadoop/record/CsvRecordOutput.java",
            "status": "added",
            "additions": 140,
            "deletions": 0,
            "changes": 140,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FCsvRecordOutput.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FCsvRecordOutput.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FCsvRecordOutput.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "82daecec03f82c161f076fe538148f937141be98",
            "filename": "src/java/org/apache/hadoop/record/Index.java",
            "status": "added",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FIndex.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FIndex.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FIndex.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "794b259759905429b65af8af35b4e1400cb5b976",
            "filename": "src/java/org/apache/hadoop/record/Record.java",
            "status": "added",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FRecord.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FRecord.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FRecord.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b2c2ea3e1014f65a62c1a0d99fb84d96995b14c1",
            "filename": "src/java/org/apache/hadoop/record/RecordComparator.java",
            "status": "added",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FRecordComparator.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FRecordComparator.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FRecordComparator.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "f41f12eba11399ecd083d6a0e9c8ab73119dd472",
            "filename": "src/java/org/apache/hadoop/record/RecordInput.java",
            "status": "added",
            "additions": 120,
            "deletions": 0,
            "changes": 120,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FRecordInput.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FRecordInput.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FRecordInput.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "a8aba2f3d615ac7abc34d7e196818769dbaa689b",
            "filename": "src/java/org/apache/hadoop/record/RecordOutput.java",
            "status": "added",
            "additions": 141,
            "deletions": 0,
            "changes": 141,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FRecordOutput.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FRecordOutput.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FRecordOutput.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "1e8d8277a98889d362a601a0dafa0f9fe76b60e1",
            "filename": "src/java/org/apache/hadoop/record/Utils.java",
            "status": "added",
            "additions": 490,
            "deletions": 0,
            "changes": 490,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FUtils.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FUtils.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FUtils.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "5272c68727cb635b6ff545bf54927b97bc437662",
            "filename": "src/java/org/apache/hadoop/record/XmlRecordInput.java",
            "status": "added",
            "additions": 243,
            "deletions": 0,
            "changes": 243,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FXmlRecordInput.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FXmlRecordInput.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FXmlRecordInput.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "643ee1f225cf2befd9c313e23758e5b308ab59f2",
            "filename": "src/java/org/apache/hadoop/record/XmlRecordOutput.java",
            "status": "added",
            "additions": 248,
            "deletions": 0,
            "changes": 248,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FXmlRecordOutput.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FXmlRecordOutput.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2FXmlRecordOutput.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "b62b62924bf039415cfeb965efdfd9e24bb4ba83",
            "filename": "src/java/org/apache/hadoop/record/compiler/CGenerator.java",
            "status": "added",
            "additions": 71,
            "deletions": 0,
            "changes": 71,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FCGenerator.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FCGenerator.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FCGenerator.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "5ba8c9fa62c70f98576f3d013490e345cdbe5182",
            "filename": "src/java/org/apache/hadoop/record/compiler/CodeBuffer.java",
            "status": "added",
            "additions": 96,
            "deletions": 0,
            "changes": 96,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FCodeBuffer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FCodeBuffer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FCodeBuffer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "6224eaf39277e8a2e0c7d03bd1e95af36555fd04",
            "filename": "src/java/org/apache/hadoop/record/compiler/CodeGenerator.java",
            "status": "added",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FCodeGenerator.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FCodeGenerator.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FCodeGenerator.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "6bfd5360d5cada7614779ce1ab3586849b401657",
            "filename": "src/java/org/apache/hadoop/record/compiler/Consts.java",
            "status": "added",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FConsts.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FConsts.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FConsts.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "e1fb599c049cbc4c6b94fdc66d1a36c73fb46de6",
            "filename": "src/java/org/apache/hadoop/record/compiler/CppGenerator.java",
            "status": "added",
            "additions": 74,
            "deletions": 0,
            "changes": 74,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FCppGenerator.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FCppGenerator.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FCppGenerator.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "28ddff09e33d744177101787aa361158643d4b7c",
            "filename": "src/java/org/apache/hadoop/record/compiler/JBoolean.java",
            "status": "added",
            "additions": 92,
            "deletions": 0,
            "changes": 92,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJBoolean.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJBoolean.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJBoolean.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "9cafe334d0c97f9e356d1561d9cf642b312e7ea3",
            "filename": "src/java/org/apache/hadoop/record/compiler/JBuffer.java",
            "status": "added",
            "additions": 103,
            "deletions": 0,
            "changes": 103,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJBuffer.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJBuffer.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJBuffer.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "ab75db5383d45ce4bede03dfcfe1c0f2e55b3ee2",
            "filename": "src/java/org/apache/hadoop/record/compiler/JByte.java",
            "status": "added",
            "additions": 80,
            "deletions": 0,
            "changes": 80,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJByte.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJByte.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJByte.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "119a575efb62fd3935f8932f989bdaaed71f2703",
            "filename": "src/java/org/apache/hadoop/record/compiler/JCompType.java",
            "status": "added",
            "additions": 72,
            "deletions": 0,
            "changes": 72,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJCompType.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJCompType.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJCompType.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "d228841636263cde5bc5023fca7bfb2027aab32f",
            "filename": "src/java/org/apache/hadoop/record/compiler/JDouble.java",
            "status": "added",
            "additions": 89,
            "deletions": 0,
            "changes": 89,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJDouble.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJDouble.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJDouble.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "f6ff6f0832b7536065fe7849756ec833bce0b0c9",
            "filename": "src/java/org/apache/hadoop/record/compiler/JField.java",
            "status": "added",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJField.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJField.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJField.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "5bff60e1b9a2d4e3856ebf54ce84446a9e69de4e",
            "filename": "src/java/org/apache/hadoop/record/compiler/JFile.java",
            "status": "added",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJFile.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJFile.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJFile.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "08d772dd41df128f06f22b19e3c5efb53e19e6fd",
            "filename": "src/java/org/apache/hadoop/record/compiler/JFloat.java",
            "status": "added",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJFloat.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJFloat.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJFloat.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "ecf735b0e41a6f22e601b897b77e7273d7be1dd1",
            "filename": "src/java/org/apache/hadoop/record/compiler/JInt.java",
            "status": "added",
            "additions": 80,
            "deletions": 0,
            "changes": 80,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJInt.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJInt.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJInt.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "38df1e87b3cb44eb686e977288319b018438702c",
            "filename": "src/java/org/apache/hadoop/record/compiler/JLong.java",
            "status": "added",
            "additions": 84,
            "deletions": 0,
            "changes": 84,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJLong.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJLong.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJLong.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "fb42dc496f599f8400438d6b23f0eaeb680fe9a9",
            "filename": "src/java/org/apache/hadoop/record/compiler/JMap.java",
            "status": "added",
            "additions": 229,
            "deletions": 0,
            "changes": 229,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJMap.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJMap.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJMap.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "96955f365f05d1b845caa9ba7c8ce47b5ea361b3",
            "filename": "src/java/org/apache/hadoop/record/compiler/JRecord.java",
            "status": "added",
            "additions": 806,
            "deletions": 0,
            "changes": 806,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJRecord.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJRecord.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJRecord.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "931359b993c995bdcf273fc190b2a8fe5517280a",
            "filename": "src/java/org/apache/hadoop/record/compiler/JString.java",
            "status": "added",
            "additions": 83,
            "deletions": 0,
            "changes": 83,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJString.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJString.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJString.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "6f1ff67ccb452be2695421e9e8db09dfdfadf8bf",
            "filename": "src/java/org/apache/hadoop/record/compiler/JType.java",
            "status": "added",
            "additions": 222,
            "deletions": 0,
            "changes": 222,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJType.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJType.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJType.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "f87442ad7163ef77c5b773bbd28eb3c738478365",
            "filename": "src/java/org/apache/hadoop/record/compiler/JVector.java",
            "status": "added",
            "additions": 197,
            "deletions": 0,
            "changes": 197,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJVector.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJVector.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJVector.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "04c4bd84733392e4a73f15bfc85a4b6d63a27cdd",
            "filename": "src/java/org/apache/hadoop/record/compiler/JavaGenerator.java",
            "status": "added",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJavaGenerator.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJavaGenerator.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2FJavaGenerator.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "ce1bc2cbf9d540e812a4732d21c1341dae99f441",
            "filename": "src/java/org/apache/hadoop/record/compiler/ant/RccTask.java",
            "status": "added",
            "additions": 136,
            "deletions": 0,
            "changes": 136,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fant%2FRccTask.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fant%2FRccTask.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fant%2FRccTask.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "59d2e4676231aec4901f80d6e675acceeb08de64",
            "filename": "src/java/org/apache/hadoop/record/compiler/generated/ParseException.java",
            "status": "added",
            "additions": 210,
            "deletions": 0,
            "changes": 210,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FParseException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FParseException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FParseException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "933710a683b2240ac13c0f7d0a70a75f315bf410",
            "filename": "src/java/org/apache/hadoop/record/compiler/generated/Rcc.java",
            "status": "added",
            "additions": 535,
            "deletions": 0,
            "changes": 535,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FRcc.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FRcc.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FRcc.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "f50d2f9a1785d1917d5fb944525e58bf5a50778e",
            "filename": "src/java/org/apache/hadoop/record/compiler/generated/RccConstants.java",
            "status": "added",
            "additions": 88,
            "deletions": 0,
            "changes": 88,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FRccConstants.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FRccConstants.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FRccConstants.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "42b04733ebfc1ad3eecdc4b619de0b806559e25e",
            "filename": "src/java/org/apache/hadoop/record/compiler/generated/RccTokenManager.java",
            "status": "added",
            "additions": 833,
            "deletions": 0,
            "changes": 833,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FRccTokenManager.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FRccTokenManager.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FRccTokenManager.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "364d708e462d49b18a0af1aad33484ae4f2f1519",
            "filename": "src/java/org/apache/hadoop/record/compiler/generated/SimpleCharStream.java",
            "status": "added",
            "additions": 439,
            "deletions": 0,
            "changes": 439,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FSimpleCharStream.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FSimpleCharStream.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FSimpleCharStream.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "29f36ab1e079dc85ac615ad3105b3fe90a872287",
            "filename": "src/java/org/apache/hadoop/record/compiler/generated/Token.java",
            "status": "added",
            "additions": 99,
            "deletions": 0,
            "changes": 99,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FToken.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FToken.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FToken.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "14f3ae34805b1cc757e0f61893c53d8a9571ed34",
            "filename": "src/java/org/apache/hadoop/record/compiler/generated/TokenMgrError.java",
            "status": "added",
            "additions": 151,
            "deletions": 0,
            "changes": 151,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FTokenMgrError.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FTokenMgrError.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2FTokenMgrError.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "2fd0f68967d50f0941f1864462d2ea171146fb32",
            "filename": "src/java/org/apache/hadoop/record/compiler/generated/package.html",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "4eeae3e47dba9a954bac97a45f8d3c682178c66f",
            "filename": "src/java/org/apache/hadoop/record/compiler/generated/rcc.jj",
            "status": "added",
            "additions": 384,
            "deletions": 0,
            "changes": 384,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2Frcc.jj",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2Frcc.jj",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fgenerated%2Frcc.jj?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "23cac15ffb41c92de97a155f38b998e3e3a211d9",
            "filename": "src/java/org/apache/hadoop/record/compiler/package.html",
            "status": "added",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fcompiler%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "82d4c8affc4d677f09271213b903fa06778f721d",
            "filename": "src/java/org/apache/hadoop/record/meta/FieldTypeInfo.java",
            "status": "added",
            "additions": 98,
            "deletions": 0,
            "changes": 98,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FFieldTypeInfo.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FFieldTypeInfo.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FFieldTypeInfo.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "2180d94adc1497a3d8a9a3fdc303d3e233abd009",
            "filename": "src/java/org/apache/hadoop/record/meta/MapTypeID.java",
            "status": "added",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FMapTypeID.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FMapTypeID.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FMapTypeID.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "2e24d7861c76cec9f56f720a641cd8dc87436635",
            "filename": "src/java/org/apache/hadoop/record/meta/RecordTypeInfo.java",
            "status": "added",
            "additions": 151,
            "deletions": 0,
            "changes": 151,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FRecordTypeInfo.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FRecordTypeInfo.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FRecordTypeInfo.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "e18ed27a3ace46656c2c931d54666763cfb18c23",
            "filename": "src/java/org/apache/hadoop/record/meta/StructTypeID.java",
            "status": "added",
            "additions": 156,
            "deletions": 0,
            "changes": 156,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FStructTypeID.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FStructTypeID.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FStructTypeID.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "78aa5487acabbe0126d9be07fc78fd1e2a47cf32",
            "filename": "src/java/org/apache/hadoop/record/meta/TypeID.java",
            "status": "added",
            "additions": 107,
            "deletions": 0,
            "changes": 107,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FTypeID.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FTypeID.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FTypeID.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "99f39f9c9451a9c06127242b02c79e07b251c5d5",
            "filename": "src/java/org/apache/hadoop/record/meta/Utils.java",
            "status": "added",
            "additions": 96,
            "deletions": 0,
            "changes": 96,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FUtils.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FUtils.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FUtils.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "e4a2b3f0bd795c9e6a812ea8a46897fa11b7ad95",
            "filename": "src/java/org/apache/hadoop/record/meta/VectorTypeID.java",
            "status": "added",
            "additions": 65,
            "deletions": 0,
            "changes": 65,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FVectorTypeID.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FVectorTypeID.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fmeta%2FVectorTypeID.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "d736f4a38f7f3fd80af8958477704e1a22e766cf",
            "filename": "src/java/org/apache/hadoop/record/package.html",
            "status": "added",
            "additions": 800,
            "deletions": 0,
            "changes": 800,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fpackage.html",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fpackage.html",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Frecord%2Fpackage.html?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "d04c52948c86c006cf54514f0c799167a1c7669b",
            "filename": "src/java/org/apache/hadoop/security/AccessControlException.java",
            "status": "added",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FAccessControlException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FAccessControlException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FAccessControlException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "81b6383381e220801f2deeab9d3e256f3ce460db",
            "filename": "src/java/org/apache/hadoop/security/AccessKey.java",
            "status": "added",
            "additions": 110,
            "deletions": 0,
            "changes": 110,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FAccessKey.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FAccessKey.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FAccessKey.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "5a5d9a72f46cb0aea9f6e0156edaa427e3125083",
            "filename": "src/java/org/apache/hadoop/security/AccessToken.java",
            "status": "added",
            "additions": 89,
            "deletions": 0,
            "changes": 89,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FAccessToken.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FAccessToken.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FAccessToken.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "8ede2bb3104c446d3c9d57b4c95b4c8632a2e081",
            "filename": "src/java/org/apache/hadoop/security/AccessTokenHandler.java",
            "status": "added",
            "additions": 289,
            "deletions": 0,
            "changes": 289,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FAccessTokenHandler.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FAccessTokenHandler.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FAccessTokenHandler.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "e5ab2934b4bc83fa82620ae1ee8c406defbdf869",
            "filename": "src/java/org/apache/hadoop/security/ExportedAccessKeys.java",
            "status": "added",
            "additions": 138,
            "deletions": 0,
            "changes": 138,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FExportedAccessKeys.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FExportedAccessKeys.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FExportedAccessKeys.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "2bb8caad8f733f6910090306c92db3f81fcdbc18",
            "filename": "src/java/org/apache/hadoop/security/Group.java",
            "status": "added",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FGroup.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FGroup.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FGroup.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "eabce15ea3b1310a3c47db45eaf676e240d63832",
            "filename": "src/java/org/apache/hadoop/security/InvalidAccessTokenException.java",
            "status": "added",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FInvalidAccessTokenException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FInvalidAccessTokenException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FInvalidAccessTokenException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "ea8246f5132730e19f2a65b7ebc529f45a6f5d2b",
            "filename": "src/java/org/apache/hadoop/security/PermissionChecker.java",
            "status": "added",
            "additions": 80,
            "deletions": 0,
            "changes": 80,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FPermissionChecker.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FPermissionChecker.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FPermissionChecker.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "94b68254c71bee6465d963aafff4117faad117e9",
            "filename": "src/java/org/apache/hadoop/security/SecurityUtil.java",
            "status": "added",
            "additions": 159,
            "deletions": 0,
            "changes": 159,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FSecurityUtil.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FSecurityUtil.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FSecurityUtil.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "62cbb659869ba6cb0407188d0fdf74ada9b017d5",
            "filename": "src/java/org/apache/hadoop/security/UnixUserGroupInformation.java",
            "status": "added",
            "additions": 432,
            "deletions": 0,
            "changes": 432,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FUnixUserGroupInformation.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FUnixUserGroupInformation.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FUnixUserGroupInformation.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "dd62debcf8d24fd1d12d6455ff154118fff1f027",
            "filename": "src/java/org/apache/hadoop/security/User.java",
            "status": "added",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FUser.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FUser.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FUser.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "ada9dcf2958495bd871359b22044b8c92b651837",
            "filename": "src/java/org/apache/hadoop/security/UserGroupInformation.java",
            "status": "added",
            "additions": 129,
            "deletions": 0,
            "changes": 129,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FUserGroupInformation.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FUserGroupInformation.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2FUserGroupInformation.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "c001a2dd6f51b96310e0266309b75f8498d3483e",
            "filename": "src/java/org/apache/hadoop/security/authorize/AuthorizationException.java",
            "status": "added",
            "additions": 76,
            "deletions": 0,
            "changes": 76,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2Fauthorize%2FAuthorizationException.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2Fauthorize%2FAuthorizationException.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2Fauthorize%2FAuthorizationException.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "6b90829aa854b1fc87f9a3541b8b5b76af01bd41",
            "filename": "src/java/org/apache/hadoop/security/authorize/ConfiguredPolicy.java",
            "status": "added",
            "additions": 156,
            "deletions": 0,
            "changes": 156,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2Fauthorize%2FConfiguredPolicy.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2Fauthorize%2FConfiguredPolicy.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2Fauthorize%2FConfiguredPolicy.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        },
        {
            "sha": "7099f0ee2c382e5f733e356ca702fbe290234412",
            "filename": "src/java/org/apache/hadoop/security/authorize/ConnectionPermission.java",
            "status": "added",
            "additions": 74,
            "deletions": 0,
            "changes": 74,
            "blob_url": "https://github.com/apache/hadoop/blob/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2Fauthorize%2FConnectionPermission.java",
            "raw_url": "https://github.com/apache/hadoop/raw/5128a9a453d64bfe1ed978cf9ffed27985eeef36/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2Fauthorize%2FConnectionPermission.java",
            "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src%2Fjava%2Forg%2Fapache%2Fhadoop%2Fsecurity%2Fauthorize%2FConnectionPermission.java?ref=5128a9a453d64bfe1ed978cf9ffed27985eeef36"
        }
    ]
}